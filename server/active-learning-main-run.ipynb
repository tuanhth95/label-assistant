{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2025-06-13T15:16:39.918905Z",
     "iopub.status.busy": "2025-06-13T15:16:39.918347Z",
     "iopub.status.idle": "2025-06-13T15:18:51.806273Z",
     "shell.execute_reply": "2025-06-13T15:18:51.805306Z",
     "shell.execute_reply.started": "2025-06-13T15:16:39.918885Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== PWD ====================\n",
      "total 728\n",
      "drwxrwxr-x  8 npl npl   8192 Jul  6 17:51 .\n",
      "drwxrwxr-x 16 npl npl   8192 Jul  5 05:35 ..\n",
      "drwxrwxr-x  3 npl npl    152 Jul  4 06:32 .working\n",
      "drwxrwxr-x  2 npl npl   8192 Jul  4 06:32 __pycache__\n",
      "-rw-rw-r--  1 npl npl 222355 Jul  5 04:52 active-learning-main-run.ipynb\n",
      "-rw-rw-r--  1 npl npl 272758 Jul  6 12:35 app.log\n",
      "-rw-rw-r--  1 npl npl      0 Jul  6 05:10 app_critical.log\n",
      "drwxrwxr-x  2 npl npl    152 Jul  4 06:32 bertscore\n",
      "drwxrwxr-x  2 npl npl   8192 Jul  4 06:32 bleu\n",
      "-rw-rw-r--  1 npl npl   9909 Jul  4 06:32 drive_func.py\n",
      "-rw-rw-r--  1 npl npl  13861 Jul  4 06:32 mainrun.py\n",
      "-rw-rw-r--  1 npl npl   6197 Jul  6 05:09 requirements.txt\n",
      "-rw-rw-r--  1 npl npl  47720 Jul  6 12:26 result.csv\n",
      "drwxrwxr-x  2 npl npl    152 Jul  4 06:32 rouge\n",
      "-rw-rw-r--  1 npl npl   1139 Jul  4 06:32 run.slurm\n",
      "-rw-rw-r--  1 npl npl   6938 Jul  4 06:32 run_vit5_random.out\n",
      "-rw-rw-r--  1 npl npl   5798 Jul  4 06:32 select_algorithm.py\n",
      "-rw-rw-r--  1 npl npl   2391 Jul  4 08:01 service-account-key-2.json\n",
      "-rw-rw-r--  1 npl npl   2391 Jul  6 17:51 service-account-key-4.json\n",
      "-rw-rw-r--  1 npl npl   2391 Jul  6 17:51 service-account-key-5.json\n",
      "-rw-rw-r--  1 npl npl   2387 Jul  4 06:32 service-account-key.json\n",
      "-rw-rw-r--  1 npl npl   1048 Jul  4 06:32 setup_logger.py\n",
      "-rw-rw-r--  1 npl npl    916 Jul  6 12:26 state_config.json\n",
      "-rw-rw-r--  1 npl npl  15520 Jul  4 06:32 train_model.py\n",
      "-rw-rw-r--  1 npl npl   7895 Jul  4 06:32 utility.py\n",
      "drwxrwxr-x  6 npl npl    152 Jul  5 06:26 working\n",
      "\n",
      "==================== IMPORT AND DOWNLOAD ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/npl/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== GPU ====================\n",
      "Sun Jul  6 18:03:29 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   38C    P0             245W / 400W |  51409MiB / 81920MiB |     99%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   39C    P0             221W / 400W |  15767MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          On  | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   22C    P0              60W / 400W |      3MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          On  | 00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   46C    P0             299W / 400W |  68430MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-80GB          On  | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   60C    P0             323W / 400W |  73100MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-80GB          On  | 00000000:90:00.0 Off |                    0 |\n",
      "| N/A   64C    P0             373W / 400W |  79700MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-80GB          On  | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              63W / 400W |      3MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-80GB          On  | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              66W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    276560      C   python                                      984MiB |\n",
      "|    0   N/A  N/A   2648684      C   python                                     1370MiB |\n",
      "|    0   N/A  N/A   2654405      C   python                                     1370MiB |\n",
      "|    0   N/A  N/A   4025352      C   python                                    47658MiB |\n",
      "|    1   N/A  N/A    689182      C   python                                    10246MiB |\n",
      "|    1   N/A  N/A   2652996      C   python                                     1370MiB |\n",
      "|    1   N/A  N/A   3758784      C   python                                     1370MiB |\n",
      "|    1   N/A  N/A   3760240      C   python                                     1370MiB |\n",
      "|    1   N/A  N/A   3761006      C   python                                     1370MiB |\n",
      "|    3   N/A  N/A    200223      C   python                                    49534MiB |\n",
      "|    3   N/A  N/A   3941696      C   /data/inseclab/DVy/bin/ollama             18874MiB |\n",
      "|    4   N/A  N/A   1724268      C   python3                                    9132MiB |\n",
      "|    4   N/A  N/A   4099812      C   python                                    63942MiB |\n",
      "|    5   N/A  N/A    666384      C   ...veLearning/conda/envs/al/bin/python    79692MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "\n",
      "__Python VERSION: 3.9.23 | packaged by conda-forge | (main, Jun  4 2025, 17:57:12) \n",
      "[GCC 13.3.0]\n",
      "__pyTorch VERSION: 2.7.1+cu126\n",
      "__CUDA VERSION\n",
      "/bin/bash: line 1: nvcc: command not found\n",
      "__CUDNN VERSION: 90501\n",
      "__Number CUDA Devices: 8\n",
      "__Devices\n",
      "index, name, driver_version, memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "0, NVIDIA A100-SXM4-80GB, 535.161.08, 81920 MiB, 51409 MiB, 29641 MiB\n",
      "1, NVIDIA A100-SXM4-80GB, 535.161.08, 81920 MiB, 15767 MiB, 65283 MiB\n",
      "2, NVIDIA A100-SXM4-80GB, 535.161.08, 81920 MiB, 3 MiB, 81047 MiB\n",
      "3, NVIDIA A100-SXM4-80GB, 535.161.08, 81920 MiB, 68430 MiB, 12620 MiB\n",
      "4, NVIDIA A100-SXM4-80GB, 535.161.08, 81920 MiB, 73100 MiB, 7950 MiB\n",
      "5, NVIDIA A100-SXM4-80GB, 535.161.08, 81920 MiB, 79744 MiB, 1305 MiB\n",
      "6, NVIDIA A100-SXM4-80GB, 535.161.08, 81920 MiB, 3 MiB, 81047 MiB\n",
      "7, NVIDIA A100-SXM4-80GB, 535.161.08, 81920 MiB, 5 MiB, 81045 MiB\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  8\n",
      "Current cuda device  0\n",
      "\n",
      "==================== OS ====================\n",
      "Cluster Manager v10.0\n",
      "slave\n",
      "DGX_NAME=\"DGX Server\"\n",
      "DGX_PRETTY_NAME=\"NVIDIA DGX Server\"\n",
      "DGX_SWBUILD_DATE=\"2024-04-22-11-45-46\"\n",
      "DGX_SWBUILD_VERSION=\"6.2.1\"\n",
      "DGX_COMMIT_ID=\"41beb69\"\n",
      "DGX_PLATFORM=\"DGX Server for A100\"\n",
      "DGX_SERIAL_NUMBER=\"Not Specified\"\n",
      "DISTRIB_ID=Ubuntu\n",
      "DISTRIB_RELEASE=22.04\n",
      "DISTRIB_CODENAME=jammy\n",
      "DISTRIB_DESCRIPTION=\"Ubuntu 22.04.4 LTS\"\n",
      "PRETTY_NAME=\"Ubuntu 22.04.4 LTS\"\n",
      "NAME=\"Ubuntu\"\n",
      "VERSION_ID=\"22.04\"\n",
      "VERSION=\"22.04.4 LTS (Jammy Jellyfish)\"\n",
      "VERSION_CODENAME=jammy\n",
      "ID=ubuntu\n",
      "ID_LIKE=debian\n",
      "HOME_URL=\"https://www.ubuntu.com/\"\n",
      "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
      "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
      "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
      "UBUNTU_CODENAME=jammy\n",
      "\n",
      "==================== PYTHON ====================\n",
      "Python 3.9.23\n",
      "Check output folder for requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# install and import package\n",
    "import subprocess\n",
    "def run_cmd(cmd):\n",
    "    return subprocess.getoutput(cmd)\n",
    "# !pip install -q transformers==4.50 sentencepiece \n",
    "# !pip install -q accelerate datasets==3.5 rouge_score bert-score nltk\n",
    "# !pip install -U -q sentence-transformers evaluate faiss-cpu\n",
    "# !pip install -q --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib google-auth\n",
    "# !pip install -q --upgrade cffi\n",
    "#!pip install --upgrade ipywidgets\n",
    "\n",
    "print('\\n' + \"=\"*20 + \" PWD \" + \"=\"*20)\n",
    "!ls -la\n",
    "\n",
    "print('\\n' + \"=\"*20 + \" IMPORT AND DOWNLOAD \" + \"=\"*20)\n",
    "import faiss, logging, json, mimetypes, transformers, io\n",
    "from email.message import EmailMessage\n",
    "from datasets import load_dataset, Dataset\n",
    "from bert_score import score as bertscores\n",
    "from transformers import MBartForConditionalGeneration, BartForConditionalGeneration, AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import DataCollatorForSeq2Seq, get_scheduler, MBartForConditionalGeneration, Seq2SeqTrainer, T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import get_scheduler, DataCollatorWithPadding, TrainingArguments, Seq2SeqTrainingArguments\n",
    "from subprocess import call\n",
    "from IPython.display import display\n",
    "import shutil, os, sys, torch, os.path, ssl, smtplib, re, random, time, torch\n",
    "import pprint as pp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk, evaluate\n",
    "from evaluate import load\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload\n",
    "from googleapiclient.errors import HttpError\n",
    "#nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "pd.options.mode.copy_on_write = False\n",
    "from matplotlib import pyplot as plt\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"=\"*20 + \" GPU \" + \"=\"*20)\n",
    "if 'not found' not in run_cmd('nvidia-smi'):\n",
    "    !nvidia-smi\n",
    "    print()\n",
    "    print('__Python VERSION:', sys.version)\n",
    "    print('__pyTorch VERSION:', torch.__version__)\n",
    "    print('__CUDA VERSION')\n",
    "    ! nvcc --version\n",
    "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "    print('__Devices')\n",
    "    call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "    print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "    print ('Available devices ', torch.cuda.device_count())\n",
    "    print ('Current cuda device ', torch.cuda.current_device())\n",
    "else:\n",
    "    print('NON-GPU VERSION')\n",
    "\n",
    "print('\\n' + \"=\"*20 + \" OS \" + \"=\"*20)\n",
    "!cat /etc/*release\n",
    "\n",
    "print('\\n' + \"=\"*20 + \" PYTHON \" + \"=\"*20)\n",
    "!python --version\n",
    "!pip freeze > requirements.txt \n",
    "print(\"Check output folder for requirements.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:51.814764Z",
     "iopub.status.busy": "2025-06-13T15:18:51.814468Z",
     "iopub.status.idle": "2025-06-13T15:18:51.832015Z",
     "shell.execute_reply": "2025-06-13T15:18:51.831436Z",
     "shell.execute_reply.started": "2025-06-13T15:18:51.814733Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# setup logging\n",
    "def show_only_critical(record):\n",
    "    return record.levelname == \"CRITICAL\"\n",
    "def initialize_logging(name): \n",
    "    logger = logging.getLogger(name)\n",
    "    console_handler = logging.StreamHandler()\n",
    "    file_handler = logging.FileHandler(\"app.log\", mode = \"a\", encoding=\"utf-16\")\n",
    "    file_critical_handler = logging.FileHandler(\"app_critical.log\", mode = \"a\", encoding=\"utf-16\")\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(file_critical_handler)\n",
    "    formatter = logging.Formatter(\n",
    "        \"[{asctime}.{msecs:0<3.0f} - {levelname}]  {message}\",\n",
    "        style = \"{\",\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    console_handler.setFormatter(formatter)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    file_critical_handler.setFormatter(formatter)\n",
    "    logger.setLevel(\"DEBUG\")\n",
    "    console_handler.setLevel(\"DEBUG\")\n",
    "    file_handler.setLevel(\"INFO\")\n",
    "    file_critical_handler.addFilter(show_only_critical)\n",
    "    logger.debug(\"initialized\")\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:51.834026Z",
     "iopub.status.busy": "2025-06-13T15:18:51.833807Z",
     "iopub.status.idle": "2025-06-13T15:18:51.857400Z",
     "shell.execute_reply": "2025-06-13T15:18:51.856695Z",
     "shell.execute_reply.started": "2025-06-13T15:18:51.834011Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#get number of word in string\n",
    "def len_word(str):\n",
    "    return len(re.split('[-\\s_]', str))\n",
    "\n",
    "#get index of nth-occur char or -1\n",
    "def nth_occur_char(st, char=' ', nth=1, begin = 0):\n",
    "    n = 0\n",
    "    i = begin\n",
    "    while i < len(st):\n",
    "        i = st.find(' ', i,)\n",
    "        if i == -1:\n",
    "            return -1\n",
    "        else:\n",
    "            n += 1\n",
    "            if n == nth:\n",
    "                return i\n",
    "        i+=1\n",
    "    return -1\n",
    "\n",
    "# clean string list - remove too short length string item, remove hyphense,...\n",
    "def clean_list(lst):\n",
    "    l, i = len(lst), 0\n",
    "    while i < l:\n",
    "        if len(lst[i]) <= 2:\n",
    "            lst.remove(lst[i])\n",
    "            l -= 1\n",
    "        else:\n",
    "            lst[i] = clean_text(lst[i])\n",
    "            i += 1    \n",
    "    return lst\n",
    "\n",
    "#clean text\n",
    "def clean_text(text):\n",
    "    return re.sub(r'^\\n|^\\s|^-\\s', '', text)\n",
    "\n",
    "#preprocess data\n",
    "def preprocess_dataset(data):\n",
    "    data.set_format(type = \"pandas\")\n",
    "    df = data[:]\n",
    "    df_detail = pd.DataFrame(columns=[\"cluster_id\", \"title\", \"anchor_text\", \"raw_text\"])\n",
    "    df_cluster_info = pd.DataFrame(columns=[\"cluster_id\", \"summary\", \"category\"])\n",
    "    for i in range(len(df[\"single_documents\"])): \n",
    "        temp = pd.DataFrame.from_dict(df[\"single_documents\"][i].tolist(), orient='columns')\n",
    "        temp[\"cluster_id\"] = i\n",
    "        df_detail = pd.concat([df_detail, temp], axis=0, )\n",
    "        temp2 = df.iloc[[i]].copy()\n",
    "        temp2[\"cluster_id\"] = i\n",
    "        temp2 = temp2.drop([\"single_documents\"], axis=\"columns\")\n",
    "        df_cluster_info = pd.concat([df_cluster_info, temp2], axis=0, )\n",
    "    df_detail = df_detail.reset_index()\n",
    "    df_detail = df_detail.drop(\"index\", axis=\"columns\")\n",
    "    return df_detail, df_cluster_info\n",
    "\n",
    "def preprocess_sentence_dataset(document_data):\n",
    "    input_type = \"\"\n",
    "    if isinstance(document_data, Dataset):\n",
    "        input_type = \"dataset\"\n",
    "        document_data.set_format('pandas')\n",
    "        document_data = document_data[:]\n",
    "    else:\n",
    "        input_type = \"pandas\"\n",
    "    document_data = document_data.copy()\n",
    "    document_data.loc[:,'raw_text'] = document_data.loc[:,'raw_text'].str.split('\\.(?!\\d)|\\u2026|\\n|(?<!Ảnh)[:;] (?=[A-Z])|: (?=[\"“])')\n",
    "    document_data.loc[:,'raw_text'].apply(lambda x: clean_list(x))\n",
    "    document_data['raw_index'] = pd.Series( [[i for i in range(len(e))] for e in document_data['raw_text']],index = document_data.index)\n",
    "    document_data = document_data.explode(['raw_text','raw_index']).reset_index(drop=True)\n",
    "\n",
    "    if input_type == \"dataset\":\n",
    "        document_data = Dataset.from_pandas(document_data)\n",
    "    return document_data\n",
    "    \n",
    "def dataset_sentence_split(data, length_split):\n",
    "    input_type = \"\"\n",
    "    if isinstance(data, Dataset):\n",
    "        input_type = \"dataset\"\n",
    "        data.set_format('pandas')\n",
    "        data = data[:]\n",
    "    else:\n",
    "        input_type = \"pandas\"\n",
    "\n",
    "    data.loc[:,'raw_text'] = data.loc[:,'raw_text'].map(lambda x: x if nth_occur_char(x, ' ', length_split) == -1 else [x[:nth_occur_char(x, ' ', length_split)], x[nth_occur_char(x, ' ', length_split)+1:]])\n",
    "    data = data.explode(['raw_text']).reset_index(drop=True)\n",
    "    \n",
    "    if input_type == \"dataset\":\n",
    "        data = Dataset.from_pandas(data)\n",
    "    return data \n",
    "def get_embeddings(\n",
    "    data,\n",
    "    embed_col,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    encoder_seq_length,\n",
    "    device,\n",
    "    return_device,\n",
    "    graph_split = None, # truncate - None / sentence split to sentences\n",
    "    graph_merge = None, # mean/max/None pooling\n",
    "    sentence_split = None, # truncate - None / split to 2 sentence, default truncate\n",
    "    sentence_merge = None, # mean/max/None pooling MOI\n",
    "    sentence_vec = None, # cls - None/mean/max pooling MOI\n",
    "    return_type = 'datasets',\n",
    "    batch_size = 256\n",
    "):\n",
    "    \n",
    "    if isinstance(data, pd.DataFrame) == True:\n",
    "      data = Dataset.from_pandas(data)\n",
    "        \n",
    "    if \"embeddings\" in data.column_names:\n",
    "        if return_type == 'pandas':\n",
    "            data.set_format('pandas')\n",
    "            return data[:]\n",
    "        else: #if return_type == 'datasets':\n",
    "            return data\n",
    "            \n",
    "    if graph_split == \"sentence\":\n",
    "        data = preprocess_sentence_dataset(data)\n",
    "        if sentence_split == 'split':\n",
    "            data = dataset_sentence_split(data, 20)\n",
    "            \n",
    "    \n",
    "    encoded_input = data.map(lambda x: tokenizer(x[embed_col], truncation=True, padding=True, max_length=encoder_seq_length, return_tensors=\"pt\"), batched=True, remove_columns=data.column_names)\n",
    "    dataloader_or_data = DataLoader(\n",
    "        encoded_input,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=DataCollatorWithPadding(\n",
    "            tokenizer=tokenizer\n",
    "        ),\n",
    "        pin_memory=(str(device)),\n",
    "    )\n",
    "    if model.device != device:\n",
    "        model2 = model.to(device)\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        model = model2\n",
    "    \n",
    "    num_obs = len(dataloader_or_data.dataset)\n",
    "    dim = list(model.state_dict().items())[-1][1].shape[0]\n",
    "    embeddings = torch.empty((num_obs, dim), dtype=torch.float, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        start = 0\n",
    "        for batch in tqdm(dataloader_or_data, desc=\"Embeddings created\"):\n",
    "            batch = {\n",
    "                k: v.to(device) for k, v in batch.items()\n",
    "            }\n",
    "            predictions = model(**batch)\n",
    "    \n",
    "            if sentence_vec == 'mean':\n",
    "                batch_embeddings = torch.mean(predictions.last_hidden_state[:,1:,:], 1)\n",
    "            elif sentence_vec == 'max':\n",
    "                batch_embeddings = torch.max(predictions.last_hidden_state[:,1:,:], 1).values\n",
    "            else: #cls\n",
    "                batch_embeddings = predictions.last_hidden_state[:,0,:]\n",
    "    \n",
    "            end = start + len(batch[\"input_ids\"])  # len(batch[list(batch.keys())[0]])\n",
    "            embeddings[start:end].copy_(batch_embeddings, non_blocking=True)\n",
    "            start = end\n",
    "            del predictions, batch\n",
    "        temp = embeddings.to(return_device)\n",
    "        del embeddings\n",
    "        torch.cuda.empty_cache()\n",
    "        embeddings = temp\n",
    "        data = data.add_column('embeddings',embeddings.tolist())\n",
    "        data.set_format('pandas')\n",
    "        data = data[:]\n",
    "        if graph_split != None:\n",
    "            if sentence_split == 'split': \n",
    "                if sentence_merge == \"mean\":\n",
    "                    data = data.groupby(['cluster_id', 'title', 'anchor_text', 'raw_index']).agg({\n",
    "                        'raw_text': lambda x : ' '.join(x),\n",
    "                        'embeddings': 'mean'\n",
    "                    }).reset_index()\n",
    "                elif sentence_merge == \"max\":\n",
    "                    data = data.groupby(['cluster_id', 'title', 'anchor_text', 'raw_index']).agg({\n",
    "                        'raw_text': lambda x : ' '.join(x),\n",
    "                        'embeddings': lambda x : np.max(list(x), axis=0)\n",
    "                    }).reset_index()\n",
    "            if graph_merge == \"mean\":\n",
    "                data = data.drop('raw_index', axis=1)\n",
    "                data = data.groupby(['cluster_id', 'title', 'anchor_text']).agg({\n",
    "                    'raw_text': lambda x : '. '.join(x),\n",
    "                    'embeddings': 'mean'\n",
    "                }).reset_index()\n",
    "            elif graph_merge == \"max\":\n",
    "                data = data.drop('raw_index', axis=1)\n",
    "                data = data.groupby(['cluster_id', 'title', 'anchor_text']).agg({\n",
    "                    'raw_text': lambda x : '. '.join(x),\n",
    "                    'embeddings': lambda x : np.max(list(x), axis=0)\n",
    "                }).reset_index()\n",
    "        if return_type == 'pandas':\n",
    "            return data\n",
    "        else: #if return_type == 'datasets':\n",
    "            return Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:51.858571Z",
     "iopub.status.busy": "2025-06-13T15:18:51.858344Z",
     "iopub.status.idle": "2025-06-13T15:18:51.878101Z",
     "shell.execute_reply": "2025-06-13T15:18:51.877485Z",
     "shell.execute_reply.started": "2025-06-13T15:18:51.858554Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# select algorithm\n",
    "def average_similarity(embed, data_embeddings):\n",
    "    if data_embeddings.size(0) == 0:  # Kiểm tra nếu data_embeddings trống\n",
    "        return 0\n",
    "    # Giả định sử dụng cosine similarity trên GPU\n",
    "    cos = torch.nn.functional.cosine_similarity(embed.unsqueeze(0), data_embeddings)\n",
    "    return cos.mean().item()  # Trả về giá trị trung bình của độ tương đồng\n",
    "def select_idds_samples(unlabel_train_data, labeled_train_data, lamb, k, device='cuda'):\n",
    "    \n",
    "    # Kiểm tra và in kích thước embedding của từng DataFrame trước khi chuyển đổi sang tensor\n",
    "    assert \"embeddings\" in unlabel_train_data.columns, \"Column 'embeddings' missing in unlabel_train_data\"\n",
    "    assert \"embeddings\" in labeled_train_data.columns, \"Column 'embeddings' missing in labeled_train_data\"\n",
    "    \n",
    "    # Chuyển embeddings của unlabel và labeled thành tensor trên GPU\n",
    "    unlabel_embeddings = torch.tensor(np.array(list(unlabel_train_data[\"embeddings\"].values), dtype='float32'), device=device)\n",
    "    labeled_embeddings = torch.tensor(np.array(list(labeled_train_data[\"embeddings\"].values), dtype='float32'), device=device)\n",
    "    \n",
    "    # Tính toán độ tương đồng trên GPU cho toàn bộ unlabel_train_data\n",
    "    similarities_unlabel = lamb * torch.tensor(\n",
    "        [average_similarity(embed, unlabel_embeddings) for embed in unlabel_embeddings],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Tính toán độ tương đồng giữa unlabel_train_data và labeled_train_data trên GPU\n",
    "    similarities_label = (1 - lamb) * torch.tensor(\n",
    "        [average_similarity(embed, labeled_embeddings) for embed in unlabel_embeddings],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Tính điểm cho từng hàng\n",
    "    scores = similarities_unlabel - similarities_label\n",
    "\n",
    "    # Tìm chỉ số của điểm cao nhất\n",
    "    if len(scores) < k:\n",
    "        k = len(scores)\n",
    "    k_max_idx = torch.topk(scores, k=k).indices.tolist()\n",
    "\n",
    "    # Trả về hàng có điểm cao nhất (chuyển về CPU để lấy dữ liệu từ DataFrame)\n",
    "    return unlabel_train_data.iloc[k_max_idx].copy()\n",
    "    \n",
    "\n",
    "def select_random_samples(unlabel_train_data, k):\n",
    "    return unlabel_train_data.sample(n=k, random_state=random.randint(1, 100000), axis=0) \n",
    "    \n",
    "def data_select( \n",
    "        unlabel_train_data, budget = -1,\n",
    "        init = 100, k_init = 10, init_type = \"random\",\n",
    "        iter = 10,  k_iter = -1, iter_type = \"random\",\n",
    "        lamb = 0.67, device=\"cuda\"\n",
    "    ): \n",
    "    # k<= 0 boc het, 0<k <1, boc theo ti le, k>=1, boc theo so luong \n",
    "    # co the xet thaam so moi laan boc k\n",
    "    # k=1 moi laan chi boóc mau: 1200 maau maat 160s\n",
    "    # k=iter moi laan boóc iter: 1200 maau maat 4s\n",
    "    if \"iter\" in unlabel_train_data.columns:\n",
    "        return unlabel_train_data\n",
    "    data_len = len(unlabel_train_data)\n",
    "    if budget <= 0 or budget >= data_len:\n",
    "        budget = data_len\n",
    "    elif budget > 0 and budget < 1:\n",
    "        budget = round(budget*data_len)\n",
    "    elif budget >=1:\n",
    "        budget = round(budget)\n",
    "        \n",
    "    if init > budget:\n",
    "        init = budget\n",
    "    if iter > budget:\n",
    "        iter = budget\n",
    "        \n",
    "    if k_init <= 0 or k_init >= init:\n",
    "        k_init = init\n",
    "    elif k_init > 0 and k_init < 1:\n",
    "        k_init = round(k_init*init)\n",
    "    elif k_init >=1:\n",
    "        k_init = round(k_init)\n",
    "\n",
    "    if k_iter <= 0 or k_iter >= iter:\n",
    "        k_iter = iter\n",
    "    elif k_iter > 0 and k_iter < 1:\n",
    "        k_iter = round(k_iter*iter)\n",
    "    elif k_iter >=1:\n",
    "        k_iter = round(k_iter)\n",
    "        \n",
    "    labeled_train_data = pd.DataFrame(columns=[\"cluster_id\", \"title\", \"anchor_text\", \"raw_text\", \"embeddings\", \"iter\"])\n",
    "    if len(unlabel_train_data) < init: \n",
    "        assert \"Not enough data\" == False\n",
    "        return\n",
    "    \n",
    "    # Tạo một mask để đánh dấu các hàng đã chọn\n",
    "    mask = pd.Series([False] * len(unlabel_train_data))\n",
    "    progress_bar = tqdm(range(budget), desc=f\"{iter_type} Select\")\n",
    "\n",
    "    \n",
    "    if init_type == \"random\":\n",
    "        available_indices = unlabel_train_data[~mask].index\n",
    "        samples_index = np.random.choice(available_indices, size=init, replace=False)\n",
    "        sample = unlabel_train_data.loc[samples_index].copy()\n",
    "        mask[samples_index] = True\n",
    "        labeled_train_data = pd.concat([labeled_train_data, sample], ignore_index=True)\n",
    "    else:\n",
    "        l = init\n",
    "        while l > 0:\n",
    "            if l < k_init:\n",
    "                k_init = l\n",
    "            sample = select_idds_samples(unlabel_train_data[~mask], labeled_train_data, lamb, k=k_init, device=device)            \n",
    "            mask[sample.index.tolist()] = True\n",
    "            labeled_train_data = pd.concat([labeled_train_data, sample], ignore_index=True)\n",
    "            l = l - k_init\n",
    "    labeled_train_data[\"iter\"] = 0\n",
    "    progress_bar.update(init)\n",
    "    \n",
    "    # Vòng lặp chính để chọn mẫu tiếp theo\n",
    "    th_iter = 0\n",
    "    while mask.sum() < budget:\n",
    "        if budget - mask.sum() >= iter:\n",
    "            i = iter\n",
    "        else:\n",
    "            i = budget - mask.sum()\n",
    "        k = k_iter\n",
    "        th_iter += 1\n",
    "        while i > 0:\n",
    "            if i < k:\n",
    "                k = i\n",
    "            if iter_type == \"IDDS\":\n",
    "                sample = select_idds_samples(unlabel_train_data[~mask], labeled_train_data, lamb, k=k, device=device)\n",
    "            elif iter_type == \"random\":\n",
    "                sample = select_random_samples(unlabel_train_data[~mask], k)\n",
    "            sample[\"iter\"] = th_iter\n",
    "            mask[sample.index.tolist()] = True\n",
    "            labeled_train_data = pd.concat([labeled_train_data, sample], ignore_index=True)\n",
    "            progress_bar.update(k)\n",
    "            i = i - k\n",
    "    return labeled_train_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:51.912265Z",
     "iopub.status.busy": "2025-06-13T15:18:51.911687Z",
     "iopub.status.idle": "2025-06-13T15:18:51.931745Z",
     "shell.execute_reply": "2025-06-13T15:18:51.931043Z",
     "shell.execute_reply.started": "2025-06-13T15:18:51.912247Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# drive function\n",
    "\n",
    "def authenticate_service_account(path, scope):\n",
    "    \"\"\"Xác thực bằng service account và trả về đối tượng service.\"\"\"\n",
    "    try:\n",
    "        creds = service_account.Credentials.from_service_account_file(\n",
    "            path, scopes=scope)\n",
    "        service = build('drive', 'v3', credentials=creds)\n",
    "        print(\"Xác thực bằng Service Account thành công!\")\n",
    "        return service\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi xác thực: {e}\")\n",
    "        return None\n",
    "\n",
    "def ls(drive, folder_id):\n",
    "    if not drive:\n",
    "        return 'drive is none'\n",
    "    try: \n",
    "        results = drive.files().list(\n",
    "            q=f'\"{folder_id}\" in parents',\n",
    "        ).execute()\n",
    "        file_list = results.get('files', [])\n",
    "        minimum = []\n",
    "        for file in file_list:\n",
    "            minimum.append(f\"{'+' if file['mimeType'].split('.')[-1] == 'folder' else '-'} {file['name']} ({file['id']}) {file['mimeType'].split('.')[-1]}\")\n",
    "        return minimum\n",
    "    except:\n",
    "        return 'error'\n",
    "\n",
    "def download_file(drive, file_id, save_as):\n",
    "    if not drive:\n",
    "        return 'drive is none'\n",
    "    try: \n",
    "        request = drive.files().get_media(fileId=file_id)\n",
    "    \n",
    "        with open(save_as, \"wb\") as f: #mo file o che do ghi bin\n",
    "            downloader = MediaIoBaseDownload(f, request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                status, done = downloader.next_chunk()\n",
    "                if status:\n",
    "                    print(f\"Da tai xuong {int(status.progress()*100)}%\")\n",
    "        print(f\"Tai tep thanh cong, luu tai: {save_as}\")\n",
    "        return True\n",
    "    except HttpError as error:\n",
    "        print(f\"Loi API: {error}\")\n",
    "        if error.resp.status == 404:\n",
    "            print(\"Lỗi 404: tệp không tồn tại hoặc không có quyền truy cập\")\n",
    "        return False\n",
    "    except IOError as e:\n",
    "        print(f\"Lỗi ghi tệp: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi không xác định: {e}\")\n",
    "        return False\n",
    "# WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"storageQuotaExceeded\"\n",
    "\n",
    "# Lỗi API: <HttpError 403 when requesting None returned \"The user's Drive storage quota has been exceeded.\". Details: \"[{'message': \"The user's Drive storage quota has been exceeded.\", 'domain': 'usageLimits', 'reason': 'storageQuotaExceeded'}]\">\n",
    "def upload_file(drive, path, file_name, folder_id=None, overwrite=True):\n",
    "    full_local_path = os.path.join(path, file_name)\n",
    "    if not os.path.isfile(full_local_path):\n",
    "        print(f\"Lỗi tệp không tồn tại\")\n",
    "        return None\n",
    "    ex = file_exists(drive, file_name, folder_id)\n",
    "    mime_type, _ = mimetypes.guess_type(full_local_path)\n",
    "    if mime_type is None:\n",
    "        mime_type = 'application/octet-stream'\n",
    "    metadata = {\n",
    "        \"name\": file_name,\n",
    "        \"parents\": [folder_id if folder_id else 'root']\n",
    "    }\n",
    "    media = MediaFileUpload(full_local_path, mimetype=mime_type, resumable=True)\n",
    "\n",
    "    try:\n",
    "        if ex == None or overwrite == False:\n",
    "            file = drive.files().create(\n",
    "                body = metadata,\n",
    "                media_body=media,\n",
    "                fields='id, name' # yeu cau tra ve id va name\n",
    "            ).execute()\n",
    "            print(f\"Tải lên thành công, Id tệp: '{file.get('id')}'\")\n",
    "            return file.get('id')\n",
    "        else:\n",
    "            file = drive.files().update(\n",
    "                fileId=ex['id'],\n",
    "                media_body=media,\n",
    "                fields='id, name'\n",
    "            ).execute()\n",
    "            print(f\"Cập nhật thành công, Id tệp: '{file.get('id')}'\")\n",
    "            return file.get('id')\n",
    "    except HttpError as e:\n",
    "        print(f\"Lỗi API: {e}\")\n",
    "        decoded_string = e.content.decode('utf-8')\n",
    "        reason = json.loads(decoded_string)['error']['errors'][0]['reason']\n",
    "        if reason == 'storageQuotaExceeded':\n",
    "            drive = authenticate_service_account('service-account-key-2.json', SCOPES)\n",
    "            try:\n",
    "                if ex == None or overwrite == False:\n",
    "                    file = drive.files().create(\n",
    "                        body = metadata,\n",
    "                        media_body=media,\n",
    "                        fields='id, name' # yeu cau tra ve id va name\n",
    "                    ).execute()\n",
    "                    print(f\"Tải lên thành công, Id tệp: '{file.get('id')}'\")\n",
    "                    return file.get('id')\n",
    "                else:\n",
    "                    file = drive.files().update(\n",
    "                        fileId=ex['id'],\n",
    "                        media_body=media,\n",
    "                        fields='id, name'\n",
    "                    ).execute()\n",
    "                    print(f\"Cập nhật thành công, Id tệp: '{file.get('id')}'\")\n",
    "                    return file.get('id')\n",
    "            except Exception as e:\n",
    "                print(f\"Lỗi không xác định: {e}\")\n",
    "                return None\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi không xác định: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_id_of_title(drive, title, parent_directory_id='root'):\n",
    "    try:\n",
    "        res = drive.files().list(\n",
    "            q = f\"name='{title}' and '{parent_directory_id}' in parents and trashed=false\",\n",
    "            spaces='drive',\n",
    "            fields='files(id,name)',\n",
    "            pageSize=1\n",
    "        ).execute()\n",
    "        res = res.get('files', [])\n",
    "        if not res:\n",
    "            return None\n",
    "        else:\n",
    "            return res[0]['id']\n",
    "    except HttpError as error:\n",
    "        print(f\"Lỗi API: {error}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi không xác định: {e}\")\n",
    "        return None\n",
    "\n",
    "def file_exists(drive, file_name, parent_id):\n",
    "    try:\n",
    "        res = drive.files().list(\n",
    "            q = f\"name='{file_name}' and '{parent_id}' in parents and trashed=false\",\n",
    "            spaces='drive',\n",
    "            fields='files(id,name,mimeType)',\n",
    "            pageSize=1\n",
    "        ).execute()\n",
    "        res = res.get('files', [])\n",
    "        if not res:\n",
    "            return None\n",
    "        else:\n",
    "            return res[0]\n",
    "    except HttpError as error:\n",
    "        print(f\"Lỗi API: {error}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi không xác định: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_folder(drive, name, parent_id=None):\n",
    "    if not drive:\n",
    "        return None\n",
    "    if parent_id == None:\n",
    "        parent_id = 'root'\n",
    "    try:\n",
    "        res = drive.files().list(\n",
    "            q=f\"name='{name}' and '{parent_id}' in parents and trashed =false\",\n",
    "            fields='files(id)'\n",
    "        ).execute()\n",
    "        res = res.get('files', [])\n",
    "        if res:\n",
    "            print(\"Thư mục đã tồn tại\");\n",
    "            return None\n",
    "        else:\n",
    "            metadata = {\n",
    "                'name': name,\n",
    "                'mimeType': 'application/vnd.google-apps.folder',\n",
    "                'parents': [parent_id]\n",
    "            }\n",
    "            folder = drive.files().create(\n",
    "                body=metadata,\n",
    "                fields='id, name'\n",
    "            ).execute()\n",
    "            print(f'Tạo thư mục thành công: {folder.get(\"id\")}')\n",
    "            return folder.get('id')\n",
    "    except HttpError as error:\n",
    "        print(f\"Lỗi API: {error}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi không xác định: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def delete_file(drive, file_name, parent_id=None):\n",
    "    if not drive:\n",
    "        return None\n",
    "    try:\n",
    "        file = drive.files().list(\n",
    "            q = f\"name='{file_name}' and '{parent_id}' in parents and trashed=false\",\n",
    "            pageSize=1,\n",
    "        ).execute()\n",
    "        file = file.get('files',[])\n",
    "    except HttpError as error:\n",
    "        print(f\"Lỗi API khong get duoc id: {error}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi không xác định: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        if file == []:\n",
    "            print(\"File không tồn tại\")\n",
    "            return None\n",
    "        file = drive.files().delete(\n",
    "            fileId=file[0]['id'],\n",
    "            fields='id'\n",
    "        ).execute()\n",
    "        print(\"Đã xóa file\")\n",
    "        return True\n",
    "    except HttpError as error:\n",
    "        print(f\"Lỗi API khong xoa duoc file: {error}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi không xác định: {e}\")\n",
    "        return None\n",
    "\n",
    "def download_folder(drive, folder_id, save_as):\n",
    "    \"\"\"\n",
    "    Tải toàn bộ một thư mục và nội dung của nó từ Google Drive.\n",
    "\n",
    "    Args:\n",
    "        drive: Đối tượng service đã được xác thực của Google Drive API.\n",
    "        folder_id (str): ID của thư mục trên Drive cần tải về.\n",
    "        save_as (str): Đường dẫn đến thư mục trên máy tính\n",
    "                                      nơi sẽ chứa thư mục tải về.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Lấy tên của thư mục gốc để tạo thư mục tương ứng ở local\n",
    "        folder_info = drive.files().get(fileId=folder_id, fields='name').execute()\n",
    "        folder_name = folder_info.get('name')\n",
    "        \n",
    "        # Tạo đường dẫn đầy đủ cho thư mục sẽ được tải về\n",
    "        local_folder_path = os.path.join(save_as, folder_name)\n",
    "        if os.path.exists(local_folder_path):\n",
    "            shutil.rmtree(local_folder_path)\n",
    "        else:\n",
    "            print(f\"📁 Tạo thư mục cục bộ: '{local_folder_path}'\")\n",
    "        os.makedirs(local_folder_path, exist_ok=True)\n",
    "\n",
    "        # Liệt kê tất cả các mục trong thư mục trên Drive\n",
    "        query = f\"'{folder_id}' in parents and trashed = false\"\n",
    "        results = drive.files().list(q=query,\n",
    "                                       pageSize=1000, # Lấy tối đa 1000 mục mỗi lần\n",
    "                                       fields=\"nextPageToken, files(id, name, mimeType)\").execute()\n",
    "        items = results.get('files', [])\n",
    "\n",
    "        if not items:\n",
    "            print(f\"   Thư mục '{folder_name}' rỗng.\")\n",
    "            return\n",
    "\n",
    "        # Dùng tqdm để hiển thị thanh tiến trình\n",
    "        with tqdm(total=len(items), desc=f'📥 Tải xuống {folder_name}', unit='item') as pbar:\n",
    "            for item in items:\n",
    "                item_name = item.get('name')\n",
    "                item_id = item.get('id')\n",
    "                item_mime_type = item.get('mimeType')\n",
    "                \n",
    "                if item_mime_type == 'application/vnd.google-apps.folder':\n",
    "                    # Nếu là thư mục, gọi đệ quy để tải thư mục con\n",
    "                    pbar.set_description(f\"📂 Đệ quy vào {item_name}\")\n",
    "                    download_folder(drive, item_id, local_folder_path)\n",
    "                else:\n",
    "                    # Nếu là tệp, gọi hàm download_file\n",
    "                    pbar.set_description(f\"📄 Tải tệp {item_name}\")\n",
    "                    local_file_path = os.path.join(local_folder_path, item_name)\n",
    "                    download_file(drive, item_id, local_file_path)\n",
    "                \n",
    "                pbar.update(1)\n",
    "\n",
    "    except HttpError as error:\n",
    "        print(f\"Lỗi API khi xử lý thư mục ID '{folder_id}': {error}\")\n",
    "\n",
    "# def upload_folder(drive, local_path, parent_folder_id=None, overwrite=False):  \n",
    "# def delete_folder_recursive(drive, folder_identifier, parent_id=None, include_folder = True):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:51.879271Z",
     "iopub.status.busy": "2025-06-13T15:18:51.879005Z",
     "iopub.status.idle": "2025-06-13T15:18:51.910915Z",
     "shell.execute_reply": "2025-06-13T15:18:51.910339Z",
     "shell.execute_reply.started": "2025-06-13T15:18:51.879244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#train model\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [[pre.strip() for pre in pred] for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # ROUGE kì vọng dòng mới sau mỗi câu\n",
    "    preds = [[\"\\n\".join(nltk.sent_tokenize(pre)) for pre in pred] for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "    \n",
    "def train_method( **kwargs):\n",
    "    total_train_data = kwargs.get('total_train_data', None)\n",
    "    #print(len(total_train_data))\n",
    "    assert total_train_data is not None  \n",
    "    result_log = kwargs['df_log']\n",
    "    log_write = kwargs.get('log_write', None)\n",
    "    model_name = kwargs.get('model_name',None)\n",
    "    select_strategy = kwargs.get('select_strategy', None)\n",
    "    encoder = kwargs.get('encoder', None)\n",
    "    state_config = kwargs.get('state_config', {})\n",
    "    alpha = kwargs.get('alpha', 0.0005*100)\n",
    "    last_test_score = -1\n",
    "    test_score = 0\n",
    "    ind = state_config['iter']\n",
    "    #train model\n",
    "    bertscore = bertscores\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    while abs(test_score - last_test_score)*100 > alpha :\n",
    "        upload_file(drive, WORKING_DIR, 'app.log', folder_id=BACKUP_DIR_ID)\n",
    "        upload_file(drive, WORKING_DIR, 'app_critical.log', folder_id=BACKUP_DIR_ID)\n",
    "        last_test_score = test_score\n",
    "        if ind > max(total_train_data[\"iter\"].unique().tolist()):\n",
    "            state_config['iter'] = 0\n",
    "            with open('state_config.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(state_config, f, ensure_ascii=False, indent=4)\n",
    "            upload_file(drive, WORKING_DIR, 'state_config.json', folder_id=BACKUP_DIR_ID)\n",
    "            break\n",
    "        \n",
    "        print(f\"\\n\\n================ITER:{ind}=======================\")\n",
    "        log_write.info(f\"ITER:{ind}\")\n",
    "        state_config['iter'] = ind\n",
    "        with open('state_config.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(state_config, f, ensure_ascii=False, indent=4)\n",
    "        upload_file(drive, WORKING_DIR, 'state_config.json', folder_id=BACKUP_DIR_ID)\n",
    "        #print(\"check 3\")\n",
    "        #print(Dataset.from_pandas(total_train_data[ind]))\n",
    "        kwargs['df_log'] = result_log\n",
    "        result_log = train_model(\n",
    "            train_detail_ds = Dataset.from_pandas(total_train_data[total_train_data[\"iter\"] <= ind]),\n",
    "            iter = ind,\n",
    "            num_of_samples = len(total_train_data[total_train_data[\"iter\"] <= ind]),\n",
    "            bert_score = bertscore,\n",
    "            #meteor_score = meteor,\n",
    "            bleu_score = bleu,\n",
    "            **kwargs\n",
    "        )\n",
    "        result_log.to_csv(\"result.csv\",index=False)\n",
    "        upload_file(drive, WORKING_DIR, 'result.csv', folder_id=BACKUP_DIR_ID)\n",
    "        log_write.info(f\"ITER:{ind} Done\")\n",
    "        ind += 1\n",
    "        \n",
    "        test_score = result_log.loc[len(result_log)-1, \"rouge_avg_score\"]\n",
    "        print(f\"different: {test_score} : {last_test_score} : {test_score - last_test_score}\")\n",
    "    if abs(test_score - last_test_score)*100 <= alpha:\n",
    "        state_config['iter'] = 0\n",
    "        with open('state_config.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(state_config, f, ensure_ascii=False, indent=4)\n",
    "        upload_file(drive, WORKING_DIR, 'state_config.json', folder_id=BACKUP_DIR_ID)\n",
    "        upload_file(drive, WORKING_DIR, 'app.log', folder_id=BACKUP_DIR_ID)\n",
    "        upload_file(drive, WORKING_DIR, 'app_critical.log', folder_id=BACKUP_DIR_ID)\n",
    "    return result_log\n",
    "\n",
    "def train_model( **kwargs ):\n",
    "    train_detail_ds = kwargs.get('train_detail_ds', None)\n",
    "    val_detail = kwargs.get('val_detail', None)\n",
    "    test_detail = kwargs.get('test_detail', None)\n",
    "    model_path = kwargs.get('model_path', None)\n",
    "    model_name = kwargs.get('model_name',None)\n",
    "    output_dir = kwargs.get('output_dir', None)\n",
    "    batch_size = kwargs.get('batch_size', 8)\n",
    "    num_train_epochs = kwargs.get('num_train_epochs', 3)\n",
    "    limit_new_tokens = kwargs.get('limit_new_tokens', 512)\n",
    "    select_strategy = kwargs.get('select_strategy', None)\n",
    "    encoder = kwargs.get('encoder', None)\n",
    "    loop = kwargs.get('loop', None)\n",
    "    device = kwargs.get('device', torch.device(\"cpu\"))\n",
    "    df_log = kwargs.get('df_log', pd.DataFrame(columns=result_log_template_col))\n",
    "    iter = kwargs.get('iter', None)\n",
    "    num_of_samples = kwargs.get('num_of_samples', None)\n",
    "    log_write = kwargs.get('log_write', None)\n",
    "    bert_score = kwargs.get('bert_score',bertscores)\n",
    "    #meteor_score = kwargs.get('meteor_score', evaluate.load('meteor'))\n",
    "    bleu_score = kwargs.get('bleu_score', evaluate.load('bleu'))\n",
    "    num_var = kwargs.get('num_var', 1)\n",
    "    var_certainty = kwargs.get('var_certainty', 'bleu')\n",
    "    test_score = 0\n",
    "    last_test_score = test_score\n",
    "    train_detail_ds.reset_format()\n",
    "    val_detail.reset_format()\n",
    "    test_detail.reset_format()\n",
    "\n",
    "    if \"T5\" in model_name:\n",
    "        model_iter = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "        model_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        print('Use model T5')\n",
    "    elif re.search(\"bart\", model_name, re.I) != None:  \n",
    "        model_iter = MBartForConditionalGeneration.from_pretrained(model_path)\n",
    "        if re.search(\"mbart\", model_name, re.I) != None:\n",
    "            model_tokenizer = AutoTokenizer.from_pretrained(model_path, src_lang=\"vi_VN\", tgt_lang=\"vi_VN\")\n",
    "            print('Use model mbart')\n",
    "        else:\n",
    "            model_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            print('Use model bartpho')\n",
    "    model_iter.to(device)\n",
    "    \n",
    "    tokenized_train_datasets = train_detail_ds.map(\n",
    "        lambda x:  model_tokenizer(x[\"raw_text\"], text_target=x[\"anchor_text\"] , max_length=max_input_length, truncation=True), \n",
    "        remove_columns=['cluster_id', 'title', 'anchor_text', 'raw_text', 'embeddings', 'iter']\n",
    "    )\n",
    "    tokenized_val_datasets = val_detail.map(\n",
    "        lambda x: model_tokenizer(x[\"raw_text\"], text_target=x[\"anchor_text\"] , max_length=max_input_length, truncation=True),\n",
    "        remove_columns=['cluster_id', 'title', 'anchor_text', 'raw_text'] \n",
    "    )\n",
    "    #tokenized_test_datasets = test_detail.map(lambda x: preprocess_function(x,model_tokenizer), remove_columns=['cluster_id', 'title', 'anchor_text', 'raw_text'] )\n",
    "    tokenized_train_datasets.set_format(\"torch\")\n",
    "    tokenized_val_datasets.set_format(\"torch\")\n",
    "    #tokenized_test_datasets.set_format(\"torch\") \n",
    "    \n",
    "    #model_iter = MBartForConditionalGeneration.from_pretrained(\"/kaggle/working/origin_backup/bartpho\")\n",
    "    data_collator = DataCollatorForSeq2Seq(model_tokenizer, model=model_iter)\n",
    "    print(f\"train_detail_ds : {len(train_detail_ds)}\")\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_train_datasets, shuffle=True, collate_fn=data_collator, batch_size=batch_size,\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_val_datasets, shuffle=True, collate_fn=data_collator, batch_size=batch_size\n",
    "    )\n",
    "    # test_dataloader = DataLoader(\n",
    "    #     tokenized_test_datasets, shuffle=True, collate_fn=data_collator, batch_size=batch_size,\n",
    "    # )\n",
    "    optimizer = AdamW(model_iter.parameters(), lr=2e-5)\n",
    "    accelerator = Accelerator(cpu = 'cpu' in device.type)\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model_iter, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "    rouge_score = evaluate.load(\"rouge\")\n",
    "    log_list = []\n",
    "    log_dict = {\n",
    "        \"Dataset\" : \"abmusu\",\n",
    "        \"Encoder\" : encoder,\n",
    "        \"Model\" : model_name,\n",
    "        \"Select strategy\" : select_strategy,\n",
    "        \"Loop\": loop,\n",
    "        \"Iter\" : iter,\n",
    "        \"Num of Samples\": num_of_samples,\n",
    "        \"Epoch\" : 0,\n",
    "        \"Type score\" : \"\",\n",
    "        \"rouge_avg_score\" : 0.0, \n",
    "        \"rouge1\" : 0.0, \n",
    "        \"rouge2\" : 0.0, \n",
    "        \"rougeL\" : 0.0, \n",
    "        \"rougeLsum\" : 0.0, \n",
    "        \"bleu\" : 0.0, \n",
    "        \"bleu_std\" : 0.0, \n",
    "        \"bleu_1\" : 0.0, \n",
    "        \"bleu_2\" : 0.0,\n",
    "        \"bleu_3\" : 0.0, \n",
    "        \"bleu_4\" : 0.0,\n",
    "        \"bleu_brevity_penalty\" : 0.0, \n",
    "        \"bleu_length_ratio\" : 0.0, \n",
    "        \"bleu_translation_length\" : 0, \n",
    "        \"bert_precision\" : 0.0, \n",
    "        \"bert_recall\" : 0.0, \n",
    "        \"bert_f1\" : 0.0,\n",
    "        \"bert_std\" : 0.0,\n",
    "        \"Time\" : 0,\n",
    "    }\n",
    "    \n",
    "    progress_bar = tqdm(range(num_training_steps), desc=\"Train\")\n",
    "    model.train()\n",
    "    for epoch in range(num_train_epochs):\n",
    "        time_start = time.time()\n",
    "        # Huấn luyện\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batchs = {\n",
    "                'input_ids' : batch['input_ids'],#.to(cudadevice), \n",
    "                'attention_mask' : batch['attention_mask'],#.to(cudadevice), \n",
    "                'labels' : batch['labels'],#.to(cudadevice)\n",
    "            }\n",
    "            outputs = model(**batchs)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        # Đánh giá epoch\n",
    "        log_write.info(f\"train epoch {epoch} done\")\n",
    "        #print(\"check1\")\n",
    "    model.eval()\n",
    "    predictions_list = []\n",
    "    certain_list = []\n",
    "    labels_list = []\n",
    "    bert_std_list = []\n",
    "    bleu_std_list = []\n",
    "    for module in model.modules():\n",
    "        if module.__class__.__name__.startswith('Dropout'):\n",
    "            module.train()\n",
    "    eval_progress_bar = tqdm(range(len(eval_dataloader)), desc=\"Eval\")\n",
    "    if num_var < 1:\n",
    "        num_var = 1\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            if num_var == 1:\n",
    "                generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                    batch[\"input_ids\"],#.to(cudadevice),\n",
    "                    attention_mask=batch[\"attention_mask\"],#.to(cudadevice),\n",
    "                    max_new_tokens = limit_new_tokens,\n",
    "                )\n",
    "            else:\n",
    "                num_var = int(num_var)\n",
    "                generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                    batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    max_new_tokens = limit_new_tokens,\n",
    "                    num_return_sequences=num_var,\n",
    "                    do_sample=True,\n",
    "                    top_k=50,\n",
    "                    top_p=0.95\n",
    "                )\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=model_tokenizer.pad_token_id\n",
    "            )\n",
    "            labels = batch[\"labels\"]\n",
    "            labels = accelerator.pad_across_processes(\n",
    "                batch[\"labels\"], dim=1, pad_index=model_tokenizer.pad_token_id\n",
    "            )\n",
    "            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "            generated_tokens = generated_tokens.reshape(len(batch[\"input_ids\"]), num_var, -1)\n",
    "            labels = accelerator.gather(labels).cpu().numpy()\n",
    "            labels = np.where(labels != -100, labels, model_tokenizer.pad_token_id)\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "            decoded_preds = [model_tokenizer.batch_decode(generated_token, skip_special_tokens=True) for generated_token in generated_tokens]\n",
    "            decoded_labels = model_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "            predictions_list.extend(decoded_preds)\n",
    "            labels_list.extend(decoded_labels)\n",
    "            eval_progress_bar.update(1)\n",
    "            del generated_tokens\n",
    "            del labels\n",
    "    if num_var > 1:\n",
    "        for l in range(len(predictions_list)):\n",
    "            f = 0\n",
    "            while f < len(predictions_list[l]):\n",
    "                if predictions_list[l][f] == '':\n",
    "                    #decoded_preds[l].pop(f)\n",
    "                    predictions_list[l][f] = 'a'\n",
    "                else:\n",
    "                    f += 1\n",
    "        for l in range(len(predictions_list)):\n",
    "            bleu_matrix = [[bleu_score.compute(predictions=[i], references=[j])['bleu'] for j in predictions_list[l]] for i in predictions_list[l]]\n",
    "            bleu_std_list.append(np.std([bleu_matrix[i][j] for i in range(0, len(predictions_list[l])-1) for j in range(i+1, len(predictions_list[l]))]))\n",
    "            if var_certainty == 'bleu':\n",
    "                certain_list.append(predictions_list[l][np.argmax(np.sum(bleu_matrix, axis = 1))])\n",
    "        for l in range(len(predictions_list)):\n",
    "            pred_for_bert = [predictions_list[l][i] for i in range(0, len(predictions_list[l])-1) for j in range(i+1, len(predictions_list[l]))]\n",
    "            ref_for_bert = [predictions_list[l][j] for i in range(0, len(predictions_list[l])-1) for j in range(i+1, len(predictions_list[l]))]\n",
    "            bert_f1 = bert_score(pred_for_bert, ref_for_bert, model_type=\"bert-base-multilingual-cased\")[2]\n",
    "            bert_matrix = [[0.0] * len(predictions_list[l])] * len(predictions_list[l])\n",
    "            for i in range(0, len(predictions_list[l])):\n",
    "                for j in range(0, len(predictions_list[l])):\n",
    "                    if i < j:\n",
    "                        bert_matrix[i][j] = bert_f1[i*len(predictions_list[l]) + j - 1 - (i+3)*i//2]\n",
    "                    elif i > j:\n",
    "                        bert_matrix[i][j] = bert_f1[j*len(predictions_list[l]) + i - 1 - (j+3)*j//2]\n",
    "                    else:\n",
    "                        bert_matrix[i][j] = torch.tensor(1.0)\n",
    "            bert_std_list.append(np.std(bert_f1.tolist()))\n",
    "            #last list to cal rouge, bleu, bert_f1\n",
    "            if var_certainty == 'bert':\n",
    "                certain_list.append(predictions_list[l][np.argmax(np.sum(bert_matrix, axis = 1))])\n",
    "            # certain_list.append(decoded_preds[l][0])\n",
    "    else:\n",
    "        for l in range(len(predictions_list)):\n",
    "            if predictions_list[l][0] == '':\n",
    "                predictions_list[l][0] = 'a'\n",
    "            bleu_std_list.append(1)\n",
    "            bert_std_list.append(1)\n",
    "            certain_list.append(predictions_list[l][0])\n",
    "    # Tính toán các chỉ số\n",
    "\n",
    "    result_rouge_score = rouge_score.compute(predictions=certain_list, references=labels_list)\n",
    "    result_rouge_score = {key: round(value * 100,4) for key, value in result_rouge_score.items()}\n",
    "    #result_meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    try:\n",
    "        result_bleu_score = bleu_score.compute(predictions=certain_list, references=labels_list)\n",
    "    except:\n",
    "        result_bleu_score = -1\n",
    "    bert_precision, bert_recall, bert_f1 = bert_score(certain_list, labels_list, model_type=\"working/origin_backup/bert-base-multilingual-cased\", num_layers=9,  lang=\"vi\", verbose=True)\n",
    "    bert_precision = round(float(bert_precision.mean())*100,4)\n",
    "    bert_recall = round(float(bert_recall.mean())*100,4)\n",
    "    bert_f1 = round(float(bert_f1.mean())*100,4)\n",
    "    bert_std = round(np.average(bert_std_list)*100,4)\n",
    "    bleu_std = np.average(bleu_std_list)\n",
    "    test_score = sum(result_rouge_score.values()) / 4\n",
    "    te = log_dict.copy()\n",
    "    te['Type score'] = 'Val'\n",
    "    te['Time'] = time.time() - time_start\n",
    "    te['rouge_avg_score'] = test_score\n",
    "    te['Epoch'] = epoch\n",
    "    te['rouge1'] = result_rouge_score['rouge1']\n",
    "    te['rouge2'] = result_rouge_score['rouge2']\n",
    "    te['rougeL'] = result_rouge_score['rougeL']\n",
    "    te['rougeLsum'] = result_rouge_score['rougeLsum']\n",
    "    # te['meteor'] = round(result_meteor_score['meteor'] * 100,4)\n",
    "    te['bleu'] = result_bleu_score['bleu']\n",
    "    te['bleu_std'] = bleu_std\n",
    "    te['bleu_1'] = result_bleu_score['precisions'][0]\n",
    "    te['bleu_2'] = result_bleu_score['precisions'][1]\n",
    "    te['bleu_3'] = result_bleu_score['precisions'][2]\n",
    "    te['bleu_4'] = result_bleu_score['precisions'][3]\n",
    "    te['bleu_brevity_penalty'] = result_bleu_score['brevity_penalty']\n",
    "    te['bleu_length_ratio'] = result_bleu_score['length_ratio']\n",
    "    te['bleu_translation_length'] = result_bleu_score['translation_length']\n",
    "    te['bert_precision'] = bert_precision\n",
    "    te['bert_recall'] = bert_recall\n",
    "    te['bert_f1'] = bert_f1\n",
    "    te['bert_std'] = bert_std\n",
    "    \n",
    "    log_list.append(te)\n",
    "\n",
    "    \n",
    "    # print(f\"Epoch {epoch}:\", result_rouge_score)\n",
    "    # print(f\"different: {test_score} : {last_test_score} : {test_score - last_test_score}\")\n",
    "    # last_test_score = test_score\n",
    "    log_write.info(f\"test done\")\n",
    "    # Lưu và tải\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        model_tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    #test iter\n",
    "    # print(f\"last_test_score: {last_test_score}\")\n",
    "    print(f\"test_score: {test_score}\")\n",
    "\n",
    "    new_log_df = pd.DataFrame(log_list)\n",
    "    del model_iter\n",
    "    del model_tokenizer\n",
    "    del train_dataloader\n",
    "    del eval_dataloader\n",
    "    # del test_dataloader\n",
    "    torch.cuda.empty_cache()\n",
    "    if len(df_log) == 0:\n",
    "        return new_log_df\n",
    "    else:\n",
    "        return pd.concat([df_log, new_log_df], axis=0,ignore_index=True)\n",
    "    #Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:51.932550Z",
     "iopub.status.busy": "2025-06-13T15:18:51.932361Z",
     "iopub.status.idle": "2025-06-13T15:18:51.947710Z",
     "shell.execute_reply": "2025-06-13T15:18:51.946971Z",
     "shell.execute_reply.started": "2025-06-13T15:18:51.932536Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# GPU function\n",
    "def memory_stats():\n",
    "    print(torch.cuda.memory_allocated()/1024**2)\n",
    "    print(torch.cuda.memory_reserved()/1024**2)\n",
    "def memory_cache_clear():\n",
    "    torch.cuda.empty_cache()\n",
    "    memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:51.809347Z",
     "iopub.status.busy": "2025-06-13T15:18:51.808133Z",
     "iopub.status.idle": "2025-06-13T15:18:51.813643Z",
     "shell.execute_reply": "2025-06-13T15:18:51.812880Z",
     "shell.execute_reply.started": "2025-06-13T15:18:51.809324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 1024\n",
    "\n",
    "MODE = \"RUN\"\n",
    "\n",
    "WORKING_DIR = '.'\n",
    "if MODE == \"TEST\":\n",
    "    BACKUP_DIR_ID = '1rhuFAQnmGBoVuARyhpcX8f_LDjcEHCGB'\n",
    "else:\n",
    "    BACKUP_DIR_ID = '15i-as_VUYJBRXXie01FgUZo9AqDiq-Cn'\n",
    "\n",
    "SERVICE_ACCOUNT_FILE = 'service-account-key-4.json' # Đường dẫn tới tệp key của service account\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive'] # Phạm vi truy cập. Vẫn cần thiết để định nghĩa quyền hạn.\n",
    "\n",
    "CUDA_INDEX = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup template state config\n",
    "template_state_config = {}\n",
    "if MODE == \"TEST\":\n",
    "    #just for test\n",
    "    template_state_config['loop'] = 1 # measure stability\n",
    "    template_state_config['iter_init'] = 8\n",
    "    template_state_config['iter_num'] = 5\n",
    "    template_state_config['select_lamb'] = 0.67\n",
    "    template_state_config['num_train_epochs'] = 2\n",
    "    template_state_config['alpha'] = 0.005*100\n",
    "    template_state_config['limit_new_tokens'] =512\n",
    "    template_state_config['batch_size'] = 2 #bartpho dung 16, mbart chi duoc 2, viT5:4 ok với 6iter, mT5:ko xai duoc\n",
    "    template_state_config['num_var'] = 2\n",
    "    template_state_config['var_certainty'] = 'bert' # hoặc 'bleu'\n",
    "  \n",
    "else:\n",
    "    #for real\n",
    "    template_state_config['loop'] = 3\n",
    "    template_state_config['iter_init'] = 100\n",
    "    template_state_config['iter_num'] = 50\n",
    "    template_state_config['select_lamb'] = 0.67\n",
    "    template_state_config['num_train_epochs'] = 10 # 10\n",
    "    template_state_config['alpha'] = 0.005*100\n",
    "    template_state_config['limit_new_tokens'] = 512\n",
    "    template_state_config['batch_size'] = 8\n",
    "    template_state_config['num_var'] = 1\n",
    "    template_state_config['var_certainty'] = 'bert' # hoặc 'bleu'\n",
    "    # vit5 24 tầm 68.33GB it6 -> 78.54GB+ it8(hẹo), \n",
    "    # vit5 20 tầm 61GB\n",
    "    # vit5 16 tầm 46GB -> 47.44 ở iter 5, \n",
    "    # vit5 12 tầm 37.73GB(it8)->41.69(it13)\n",
    "    # vit5 8  tầm 27.8GB, \n",
    "    # vit5 4  tầm \n",
    "\n",
    "    # bartpho 24 tầm 21.15GB\n",
    "    # bartpho 32 tầm 31.18GB -> 36.23GB(it12)\n",
    "    # bartpho 48 tầm 50.15GB(i6L0) -> 52.36GB(i7L1)\n",
    "\n",
    "    # mT5 16 crash\n",
    "    # mT5 12 67.62GB(i0) -> 77.67GB(i8) -> crash(it13L1) ~ 83GB\n",
    "    # mT5 10 74.15GB(i13) -> 78GB(it16)\n",
    "\n",
    "    # mbart 36 71.99GB(i0L0) -> 76.30(i4L0) -> 78.31GB(i6L0) -> crash(it10) ~ 85GB\n",
    "    # mbart 32 74.86GB(i10L0) -> 78.15GB(i0L1) -> 83.15GB(i14L1)\n",
    "    # mbart 24 76.66GB(i14L1)\n",
    "template_state_config['name'] = 'train model mt5 budget=full loop=3 numvar=1 certainty=bert'\n",
    "template_state_config['description'] = 'train model với quy trình cập nhật có lấy cả certainty'\n",
    "template_state_config['budget'] = 1200\n",
    "template_state_config['current_loop'] = 0\n",
    "template_state_config['iter'] = 0\n",
    "template_state_config['output_dir'] = WORKING_DIR\n",
    "template_state_config['encoders'] = ['phobert', 'bert-base-multilingual-cased']\n",
    "template_state_config['current_encoder'] = template_state_config['encoders'][0]\n",
    "template_state_config['models'] = ['mT5']\n",
    "template_state_config['current_models'] = template_state_config['models'][0]\n",
    "template_state_config['select_strategies'] = [\"IDDS\", \"random\"]\n",
    "template_state_config['current_select_strategies'] = template_state_config['select_strategies'][0]\n",
    "#config for embedding\n",
    "template_state_config['graph_split'] = None               # truncate - None / sentence split to sentences\n",
    "template_state_config['graph_merge'] = None              # mean/max/None pooling\n",
    "template_state_config['sentence_split'] = None            # truncate - None / split to 2 sentence, default truncate\n",
    "template_state_config['sentence_merge'] = None            # mean/max/None pooling\n",
    "template_state_config['sentence_vec'] = None             # cls - None/mean/max pooling\n",
    "\n",
    "\n",
    "result_log_template_col = [\"Dataset\", \"Encoder\", \"Model\", \"Select strategy\", \"Loop\", \"Iter\", \"Num of Samples\", \"Epoch\", \"Type score\", \n",
    "                            \"rouge_avg_score\", \"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\", \n",
    "                            \"bleu\", \"bleu_std\", \"bleu_1\", \"bleu_2\", \"bleu_3\", \"bleu_4\", \"bleu_brevity_penalty\", \"bleu_length_ratio\", \"bleu_translation_length\", \n",
    "                            \"bert_precision\", \"bert_recall\", \"bert_f1\", \"bert_std\", \"Time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:51.948814Z",
     "iopub.status.busy": "2025-06-13T15:18:51.948492Z",
     "iopub.status.idle": "2025-06-13T15:18:52.019692Z",
     "shell.execute_reply": "2025-06-13T15:18:52.019138Z",
     "shell.execute_reply.started": "2025-06-13T15:18:51.948792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xác thực bằng Service Account thành công!\n"
     ]
    }
   ],
   "source": [
    "drive = authenticate_service_account(SERVICE_ACCOUNT_FILE, SCOPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:52.022318Z",
     "iopub.status.busy": "2025-06-13T15:18:52.021814Z",
     "iopub.status.idle": "2025-06-13T15:18:54.372780Z",
     "shell.execute_reply": "2025-06-13T15:18:54.371992Z",
     "shell.execute_reply.started": "2025-06-13T15:18:52.022303Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Da tai xuong 100%\n",
      "Tai tep thanh cong, luu tai: ./state_config.json\n",
      "Da tai xuong 100%\n",
      "Tai tep thanh cong, luu tai: ./result.csv\n",
      "Da tai xuong 100%\n",
      "Tai tep thanh cong, luu tai: ./working/data/data_backup_model-mT5_strategies-IDDS_encoder-bert-base-multilingual-cased_loop-1.jsonl\n",
      "Da tai xuong 100%\n",
      "Tai tep thanh cong, luu tai: ./app.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-06 18:03:47.995 - DEBUG]  initialized\n",
      "[2025-07-06 18:03:47.996 - INFO]  authenticate service account successful\n",
      "[2025-07-06 18:03:47.998 - INFO]  RUN mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Da tai xuong 0%\n",
      "Tai tep thanh cong, luu tai: ./app_critical.log\n"
     ]
    }
   ],
   "source": [
    "state_exist = file_exists(drive, 'state_config.json', BACKUP_DIR_ID)\n",
    "if state_exist != None:\n",
    "    download_file(drive, state_exist['id'], WORKING_DIR+'/state_config.json')\n",
    "    with open(WORKING_DIR+'/state_config.json', 'r') as f:\n",
    "        state_config = json.load(f)\n",
    "else: \n",
    "    state_config = template_state_config\n",
    "\n",
    "result_log_exist = file_exists(drive, 'result.csv', BACKUP_DIR_ID)\n",
    "if result_log_exist != None:\n",
    "    download_file(drive, result_log_exist['id'], WORKING_DIR+'/result.csv')\n",
    "    result_log = pd.read_csv(WORKING_DIR+'/result.csv')\n",
    "else:\n",
    "    result_log = pd.DataFrame(columns=result_log_template_col)\n",
    "\n",
    "data_backup_exist = file_exists(drive, f'data_backup_model-{state_config[\"current_models\"]}_strategies-{state_config[\"current_select_strategies\"]}_encoder-{state_config[\"current_encoder\"]}_loop-{state_config[\"current_loop\"]}.jsonl', BACKUP_DIR_ID)\n",
    "if data_backup_exist != None:\n",
    "    download_file(drive, data_backup_exist['id'], WORKING_DIR+f'/working/data/data_backup_model-{state_config[\"current_models\"]}_strategies-{state_config[\"current_select_strategies\"]}_encoder-{state_config[\"current_encoder\"]}_loop-{state_config[\"current_loop\"]}.jsonl')\n",
    "\n",
    "app_exist = file_exists(drive, 'app.log', BACKUP_DIR_ID)\n",
    "if app_exist != None:\n",
    "    download_file(drive, app_exist['id'], WORKING_DIR+'/app.log')\n",
    "\n",
    "app_critical_exist = file_exists(drive, 'app_critical.log', BACKUP_DIR_ID)\n",
    "if app_critical_exist != None:\n",
    "    download_file(drive, app_critical_exist['id'], WORKING_DIR+'/app_critical.log')\n",
    "    \n",
    "logger = initialize_logging(__name__)\n",
    "logger.info('authenticate service account successful')\n",
    "logger.info(f\"{MODE} mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:54.373711Z",
     "iopub.status.busy": "2025-06-13T15:18:54.373506Z",
     "iopub.status.idle": "2025-06-13T15:19:58.237040Z",
     "shell.execute_reply": "2025-06-13T15:19:58.236360Z",
     "shell.execute_reply.started": "2025-06-13T15:18:54.373695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#config device\n",
    "torch.cuda.set_device(CUDA_INDEX)\n",
    "cudadevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "cpudevice = torch.device(\"cpu\")\n",
    "\n",
    "# if not os.path.exists('./working/origin_backup/phobert'):\n",
    "#     download_folder(drive, '1YXkURCQp7aUUe0pskKzMufidofCnwV-Z', './working/origin_backup/phobert')\n",
    "# if not os.path.exists('./working/origin_backup/viT5'):\n",
    "#     download_folder(drive, '19o4wxqTTeP8owtV6Kthbxw69k20FPEWq', './working/origin_backup/viT5')\n",
    "#Load model encoder\n",
    "# checkpoint_encoder = \"google-bert/bert-base-multilingual-cased\"\n",
    "# checkpoint_encoder = 'keepitreal/vietnamese-sbert'\n",
    "# checkpoint_encoder = 'vinai/phobert-base-v2'\n",
    "# local_checkpoint_encoder = f'./working/origin_backup/{state_config[\"current_encoder\"]}'\n",
    "# tokenizer_encoder = AutoTokenizer.from_pretrained(local_checkpoint_encoder, local_files_only=True)\n",
    "# model_encoder = AutoModel.from_pretrained(local_checkpoint_encoder, local_files_only=True)\n",
    "# encoder_seq_length = 512\n",
    "# model_encoder.to(cudadevice)\n",
    "# logger.info(f\"{state_config['current_encoder']} cloned\")\n",
    "\n",
    "#Load model main\n",
    "# checkpoint_bartpho = \"vinai/bartpho-word-base\"\n",
    "# bartpho_tokenizer = AutoTokenizer.from_pretrained(checkpoint_bartpho)\n",
    "# bartpho_model = BartForConditionalGeneration.from_pretrained(checkpoint_bartpho)\n",
    "# bartpho_model.save_pretrained(\"/kaggle/working/origin_backup/bartpho\")\n",
    "# bartpho_tokenizer.save_pretrained(\"/kaggle/working/origin_backup/bartpho\")\n",
    "# #bartpho_model.to(cudadevice);\n",
    "# logger.info(\"bartpho-word-base cloned\")\n",
    "\n",
    "# checkpoint_mbart = \"facebook/mbart-large-50\" # 680M params\n",
    "# mbart_tokenizer = AutoTokenizer.from_pretrained(checkpoint_mbart)\n",
    "# mbart_model = MBartForConditionalGeneration.from_pretrained(checkpoint_mbart)\n",
    "# mbart_model.save_pretrained(\"./working/origin_backup/mbart\")\n",
    "# mbart_tokenizer.save_pretrained(\"./working/origin_backup/mbart\")\n",
    "# #mbart_model.to(cudadevice);\n",
    "# logger.info(\"mbart-large-50 cloned\")\n",
    "\n",
    "# checkpoint_viT5 = \"VietAI/vit5-base\" # 310M params and 866M params for large version\n",
    "# viT5_tokenizer = AutoTokenizer.from_pretrained(checkpoint_viT5, force_download=True)\n",
    "# viT5_model = T5ForConditionalGeneration.from_pretrained(checkpoint_viT5, force_download=True)\n",
    "# viT5_model.save_pretrained(\"./working/origin_backup/viT5\")\n",
    "# viT5_tokenizer.save_pretrained(\"./working/origin_backup/viT5\")\n",
    "# #mbart_model.to(cudadevice);\n",
    "# logger.info(\"viT5-base cloned\")\n",
    "\n",
    "\n",
    "#mbart_model.to(cudadevice);\n",
    "# logger.info(\"viT5-base cloned\")\n",
    "\n",
    "# checkpoint_mT5 = \"google/mt5-base\" # 580 million parameters\n",
    "# mT5_tokenizer = AutoTokenizer.from_pretrained(checkpoint_mT5)\n",
    "# mT5_model = T5ForConditionalGeneration.from_pretrained(checkpoint_mT5)\n",
    "# mT5_model.save_pretrained(\"./working/origin_backup/mT5\")\n",
    "# mT5_tokenizer.save_pretrained(\"./working/origin_backup/mT5\")\n",
    "# #mbart_model.to(cudadevice);\n",
    "# logger.info(\"mT5-base cloned\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:19:58.243117Z",
     "iopub.status.busy": "2025-06-13T15:19:58.242892Z",
     "iopub.status.idle": "2025-06-13T15:20:03.466390Z",
     "shell.execute_reply": "2025-06-13T15:20:03.464593Z",
     "shell.execute_reply.started": "2025-06-13T15:19:58.243096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#preprocess abmusu\n",
    "#limit data train to test\n",
    "if MODE == \"TEST\":\n",
    "    num_sample_train = 16\n",
    "    num_sample_val = 16\n",
    "    num_sample_test = 8\n",
    "else:\n",
    "    num_sample_train = -1\n",
    "    num_sample_val = -1\n",
    "    num_sample_test = -1\n",
    "\n",
    "\n",
    "#Load data abmusu\n",
    "# data_path = {\"train\":\"abmusu/vlsp_2022_abmusu_train_data_new.jsonl\",\n",
    "#              \"val\":\"abmusu/vlsp_2022_abmusu_validation_data_new.jsonl\",\n",
    "#              \"test\":\"abmusu/vlsp_abmusu_test_data.jsonl\"\n",
    "#             }\n",
    "# data = load_dataset(\"json\", data_files=data_path)\n",
    "# train_df_detail_ex = pd.read_csv('abmusu/abmusu_1200.csv')\n",
    "# display(data)\n",
    "\n",
    "# if MODE == \"TEST\":\n",
    "#     #just for test\n",
    "#     train_df_detail, train_df_cluster_info = preprocess_dataset(data[\"train\"].select(range(num_sample_train)))\n",
    "#     train_df_detail = train_df_detail[:num_sample_train]\n",
    "#     val_df_detail, val_df_cluster_info = preprocess_dataset(data[\"val\"].select(range(num_sample_val)))\n",
    "#     val_df_detail = val_df_detail[:num_sample_val]\n",
    "#     test_df_detail, test_df_cluster_info = preprocess_dataset(data[\"test\"].select(range(num_sample_val)))\n",
    "#     test_df_detail = test_df_detail[:num_sample_val]\n",
    "\n",
    "#     train_df_detail_ex = train_df_detail_ex[:num_sample_train]\n",
    "# else:\n",
    "#     #for real\n",
    "#     #split to paragraph\n",
    "#     train_df_detail, train_df_cluster_info = preprocess_dataset(data[\"train\"])\n",
    "#     val_df_detail, val_df_cluster_info = preprocess_dataset(data[\"val\"])\n",
    "#     test_df_detail, test_df_cluster_info = preprocess_dataset(data[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:20:03.794997Z",
     "iopub.status.busy": "2025-06-13T15:20:03.794795Z",
     "iopub.status.idle": "2025-06-13T15:20:20.404318Z",
     "shell.execute_reply": "2025-06-13T15:20:20.403541Z",
     "shell.execute_reply.started": "2025-06-13T15:20:03.794981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_detail = Dataset.from_pandas(train_df_detail) # train_df_detail_ex\n",
    "# val_detail = Dataset.from_pandas(val_df_detail)\n",
    "# test_detail = Dataset.from_pandas(test_df_detail)\n",
    "\n",
    "# logger.info('abmusu processed')\n",
    "# # tach theo cau, theo doan\n",
    "# # khongg xu ly(mac dinh truncate), truncate\n",
    "# # sau embedding max,mean pooling, cls theo cau, doan\n",
    "# train_detail = get_embeddings(\n",
    "#     train_detail,\n",
    "#     'raw_text',\n",
    "#     tokenizer_encoder,\n",
    "#     model_encoder,\n",
    "#     encoder_seq_length,\n",
    "#     device = cudadevice,\n",
    "#     return_device = cpudevice,\n",
    "#     graph_split = state_config['graph_split'],                  # truncate - None / sentence split to sentences\n",
    "#     graph_merge = state_config['graph_merge'],                  # mean/max/None pooling\n",
    "#     sentence_split = state_config['sentence_split'],            # truncate - None / split to 2 sentence, default truncate\n",
    "#     sentence_merge = state_config['sentence_merge'],            # mean/max/None pooling\n",
    "#     sentence_vec = state_config['sentence_vec'],                # cls - None/mean/max pooling\n",
    "#     return_type = 'datasets',\n",
    "#     batch_size = 256\n",
    "# )\n",
    "\n",
    "# train_detail.add_faiss_index(column=\"embeddings\")\n",
    "# t = model_encoder.to(cpudevice)\n",
    "# del model_encoder\n",
    "# model_encoder = t\n",
    "# torch.cuda.empty_cache()\n",
    "# train_detail.features\n",
    "# logger.info(\"abmusu embedded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-13T15:23:17.669Z",
     "iopub.execute_input": "2025-06-13T15:20:28.913361Z",
     "iopub.status.busy": "2025-06-13T15:20:28.913121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoders = state_config['encoders'][state_config['encoders'].index(state_config['current_encoder']):]\n",
    "for encoder in encoders:\n",
    "    models = state_config['models'][state_config['models'].index(state_config['current_models']):]\n",
    "    for j in models:\n",
    "        strategies = state_config['select_strategies'][state_config['select_strategies'].index(state_config['current_select_strategies']):]\n",
    "        for i in strategies:\n",
    "            current_loop = state_config['current_loop']\n",
    "            for l in range(current_loop, state_config['loop']):\n",
    "                logger.info(f\"Train {j} - {i} - {encoder} - with th-loop: {l}\")\n",
    "                state_config['current_loop'] = l\n",
    "                state_config['current_models'] = j\n",
    "                state_config['current_select_strategies'] = i\n",
    "                state_config['current_encoder'] = encoder\n",
    "                with open('state_config.json', 'w', encoding='utf-8') as f:\n",
    "                    json.dump(state_config, f, ensure_ascii=False, indent=4)\n",
    "                upload_file(drive, WORKING_DIR, 'state_config.json', folder_id= BACKUP_DIR_ID)\n",
    "                if os.path.exists(WORKING_DIR+f'/working/data/data_backup_model-{j}_strategies-{i}_encoder-{encoder}_loop-{l}.jsonl') == False or state_config['iter'] == 0:\n",
    "                    # process data\n",
    "                    local_checkpoint_encoder = f'./working/origin_backup/{state_config[\"current_encoder\"]}'\n",
    "                    tokenizer_encoder = AutoTokenizer.from_pretrained(local_checkpoint_encoder, local_files_only=True)\n",
    "                    model_encoder = AutoModel.from_pretrained(local_checkpoint_encoder, local_files_only=True)\n",
    "                    encoder_seq_length = int(model_encoder.config.max_position_embeddings / 256) * 256\n",
    "                    model_encoder.to(cudadevice)\n",
    "                    logger.info(f\"{state_config['current_encoder']} loaded\")\n",
    "                    if MODE == \"TEST\":\n",
    "                        train_df_detail = pd.read_csv('../abmusu/abmusu_1200.csv').loc[:num_sample_train,]\n",
    "                        val_df_detail = pd.read_csv('../abmusu/abmusu_val.csv').loc[:num_sample_val, ['cluster_id', 'title', 'anchor_text', 'raw_text']]\n",
    "                        test_df_detail = pd.read_csv('../abmusu/abmusu_test.csv').loc[:num_sample_test, ['cluster_id', 'title', 'anchor_text', 'raw_text']]\n",
    "                    else:\n",
    "                        train_df_detail = pd.read_csv('../abmusu/abmusu_1200.csv')\n",
    "                        val_df_detail = pd.read_csv('../abmusu/abmusu_val.csv').loc[:, ['cluster_id', 'title', 'anchor_text', 'raw_text']]\n",
    "                        test_df_detail = pd.read_csv('../abmusu/abmusu_test.csv').loc[:, ['cluster_id', 'title', 'anchor_text', 'raw_text']]\n",
    "\n",
    "                    train_detail = Dataset.from_pandas(train_df_detail) # train_df_detail_ex\n",
    "                    val_detail = Dataset.from_pandas(val_df_detail)\n",
    "                    test_detail = Dataset.from_pandas(test_df_detail)\n",
    "                    logger.info('abmusu processed')\n",
    "                    \n",
    "                    train_detail = get_embeddings(\n",
    "                        train_detail,\n",
    "                        'raw_text',\n",
    "                        tokenizer_encoder,\n",
    "                        model_encoder,\n",
    "                        encoder_seq_length,\n",
    "                        device = cudadevice,\n",
    "                        return_device = cpudevice,\n",
    "                        graph_split = state_config['graph_split'],                  # truncate - None / sentence split to sentences\n",
    "                        graph_merge = state_config['graph_merge'],                  # mean/max/None pooling\n",
    "                        sentence_split = state_config['sentence_split'],            # truncate - None / split to 2 sentence, default truncate\n",
    "                        sentence_merge = state_config['sentence_merge'],            # mean/max/None pooling\n",
    "                        sentence_vec = state_config['sentence_vec'],                # cls - None/mean/max pooling\n",
    "                        return_type = 'datasets',\n",
    "                        batch_size = 256\n",
    "                    )\n",
    "                    t = model_encoder.to(cpudevice)\n",
    "                    del model_encoder\n",
    "                    model_encoder = t\n",
    "                    torch.cuda.empty_cache()\n",
    "                    logger.info(\"abmusu embedded\")\n",
    "                    \n",
    "                    train_detail.set_format(\"pandas\")\n",
    "                    unlabel_train= train_detail[:]\n",
    "\n",
    "                    total_train_data = data_select(\n",
    "                        unlabel_train.copy(), budget = state_config['budget'],\n",
    "                        init = state_config['iter_init'], k_init = -1, init_type = f\"{i}\",\n",
    "                        iter = state_config['iter_num'],  k_iter = -1, iter_type = f\"{i}\",\n",
    "                        lamb = state_config['select_lamb'], device=cudadevice\n",
    "                    )\n",
    "                    data_backup = Dataset.from_pandas(total_train_data)\n",
    "                    data_backup.to_json(f'working/data/data_backup_model-{j}_strategies-{i}_encoder-{encoder}_loop-{l}.jsonl')\n",
    "                    upload_file(drive, WORKING_DIR + f'/working/data/', f'data_backup_model-{j}_strategies-{i}_encoder-{encoder}_loop-{l}.jsonl', folder_id=BACKUP_DIR_ID)\n",
    "                    logger.info(f\"backup data DONE\")\n",
    "                else:\n",
    "                    data = load_dataset(\"json\", data_files=WORKING_DIR+f'/working/data/data_backup_model-{j}_strategies-{i}_encoder-{encoder}_loop-{l}.jsonl')\n",
    "                    data = data['train']\n",
    "                    data.set_format(\"pandas\")\n",
    "                    total_train_data= data[:]\n",
    "                    if MODE == \"TEST\":\n",
    "                        val_df_detail = pd.read_csv('../abmusu/abmusu_val.csv').loc[:num_sample_val, ['cluster_id', 'title', 'anchor_text', 'raw_text']]\n",
    "                        test_df_detail = pd.read_csv('../abmusu/abmusu_test.csv').loc[:num_sample_test, ['cluster_id', 'title', 'anchor_text', 'raw_text']]\n",
    "                    else:\n",
    "                        val_df_detail = pd.read_csv('../abmusu/abmusu_val.csv').loc[:, ['cluster_id', 'title', 'anchor_text', 'raw_text']]\n",
    "                        test_df_detail = pd.read_csv('../abmusu/abmusu_test.csv').loc[:, ['cluster_id', 'title', 'anchor_text', 'raw_text']]\n",
    "\n",
    "                    val_detail = Dataset.from_pandas(val_df_detail)\n",
    "                    test_detail = Dataset.from_pandas(test_df_detail)\n",
    "                result_log = train_method(\n",
    "                    total_train_data = total_train_data,\n",
    "                    val_detail = val_detail,\n",
    "                    test_detail = test_detail,\n",
    "                    encoder = encoder,\n",
    "                    alpha = state_config['alpha'],\n",
    "                    model_path = f'{WORKING_DIR}/working/origin_backup/{j}',\n",
    "                    output_dir = f'{WORKING_DIR}/working/{j}/{i}',\n",
    "                    batch_size = state_config['batch_size'],\n",
    "                    model_name = j,\n",
    "                    num_train_epochs = state_config['num_train_epochs'],\n",
    "                    limit_new_tokens = state_config['limit_new_tokens'],\n",
    "                    select_strategy = i,\n",
    "                    loop = l,\n",
    "                    state_config = state_config,\n",
    "                    df_log = result_log,\n",
    "                    log_write = logger,\n",
    "                    num_var = state_config['num_var'],\n",
    "                    var_certainty = state_config['var_certainty'],\n",
    "                    device = cudadevice\n",
    "                )\n",
    "                result_log.to_csv(\"result.csv\",index=False)\n",
    "                upload_file(drive, WORKING_DIR, 'result.csv', folder_id=BACKUP_DIR_ID)\n",
    "                model_id = create_folder(drive, f'backup-model-{j}_strategies-{i}_encoder-{encoder}_loop-{l}', parent_id=BACKUP_DIR_ID)\n",
    "                if model_id:\n",
    "                    for f in os.listdir(f'{WORKING_DIR}/working/{j}/{i}'):\n",
    "                        upload_file(drive, f'{WORKING_DIR}/working/{j}/{i}', f, folder_id=model_id)\n",
    "                else:\n",
    "                    print('Không thể tạo folder backup model')\n",
    "                    logger.info(f\"Không thể tạo backup-model-{j}_strategies-{i}_encoder-{encoder}_loop-{l}\")\n",
    "            state_config['current_loop'] = 0\n",
    "        state_config['current_select_strategies'] = state_config['select_strategies'][0]\n",
    "    state_config['current_models'] = state_config['models'][0]\n",
    "result_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Encoder</th>\n",
       "      <th>Model</th>\n",
       "      <th>Select strategy</th>\n",
       "      <th>Loop</th>\n",
       "      <th>Iter</th>\n",
       "      <th>Num of Samples</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Type score</th>\n",
       "      <th>rouge_avg_score</th>\n",
       "      <th>...</th>\n",
       "      <th>bleu_3</th>\n",
       "      <th>bleu_4</th>\n",
       "      <th>bleu_brevity_penalty</th>\n",
       "      <th>bleu_length_ratio</th>\n",
       "      <th>bleu_translation_length</th>\n",
       "      <th>bert_precision</th>\n",
       "      <th>bert_recall</th>\n",
       "      <th>bert_f1</th>\n",
       "      <th>bert_std</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>phobert</td>\n",
       "      <td>viT5</td>\n",
       "      <td>IDDS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>22.779075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006269</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.888514</td>\n",
       "      <td>4341</td>\n",
       "      <td>57.2434</td>\n",
       "      <td>65.9305</td>\n",
       "      <td>61.2496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.527842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>phobert</td>\n",
       "      <td>viT5</td>\n",
       "      <td>IDDS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>21.027725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008356</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.429054</td>\n",
       "      <td>4821</td>\n",
       "      <td>58.9836</td>\n",
       "      <td>67.5065</td>\n",
       "      <td>62.9328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.575020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>phobert</td>\n",
       "      <td>viT5</td>\n",
       "      <td>IDDS</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>21.196200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010158</td>\n",
       "      <td>0.005097</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.581081</td>\n",
       "      <td>4956</td>\n",
       "      <td>58.1596</td>\n",
       "      <td>67.4309</td>\n",
       "      <td>62.4315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.347652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>phobert</td>\n",
       "      <td>viT5</td>\n",
       "      <td>random</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>22.960800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011296</td>\n",
       "      <td>0.005323</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.923423</td>\n",
       "      <td>4372</td>\n",
       "      <td>58.8391</td>\n",
       "      <td>67.4589</td>\n",
       "      <td>62.8187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.235364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>phobert</td>\n",
       "      <td>viT5</td>\n",
       "      <td>random</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>22.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006676</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.099099</td>\n",
       "      <td>4528</td>\n",
       "      <td>58.5039</td>\n",
       "      <td>67.1173</td>\n",
       "      <td>62.4848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.989027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "      <td>viT5</td>\n",
       "      <td>IDDS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>23.285150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008155</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.733108</td>\n",
       "      <td>4203</td>\n",
       "      <td>58.2752</td>\n",
       "      <td>67.0827</td>\n",
       "      <td>62.3464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.417964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "      <td>viT5</td>\n",
       "      <td>IDDS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>21.021650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006163</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.702703</td>\n",
       "      <td>5064</td>\n",
       "      <td>57.9655</td>\n",
       "      <td>67.4218</td>\n",
       "      <td>62.3087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.572415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "      <td>viT5</td>\n",
       "      <td>IDDS</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>20.917550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008347</td>\n",
       "      <td>0.003589</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.704955</td>\n",
       "      <td>5066</td>\n",
       "      <td>59.1320</td>\n",
       "      <td>67.2237</td>\n",
       "      <td>62.8820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.054230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "      <td>viT5</td>\n",
       "      <td>random</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>21.718375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003377</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.040541</td>\n",
       "      <td>4476</td>\n",
       "      <td>57.6373</td>\n",
       "      <td>66.1630</td>\n",
       "      <td>61.5894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.067327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "      <td>viT5</td>\n",
       "      <td>random</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>20.065750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>0.004778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.185811</td>\n",
       "      <td>5493</td>\n",
       "      <td>58.7372</td>\n",
       "      <td>68.3151</td>\n",
       "      <td>63.1406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.041931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "      <td>viT5</td>\n",
       "      <td>random</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>20.912925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011178</td>\n",
       "      <td>0.004807</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.680180</td>\n",
       "      <td>5044</td>\n",
       "      <td>59.0050</td>\n",
       "      <td>67.8272</td>\n",
       "      <td>63.0949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.315633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dataset                       Encoder Model Select strategy  Loop  Iter  \\\n",
       "0   abmusu                       phobert  viT5            IDDS     0     0   \n",
       "1   abmusu                       phobert  viT5            IDDS     0     1   \n",
       "2   abmusu                       phobert  viT5            IDDS     0     2   \n",
       "3   abmusu                       phobert  viT5          random     0     0   \n",
       "4   abmusu                       phobert  viT5          random     0     1   \n",
       "5   abmusu  bert-base-multilingual-cased  viT5            IDDS     0     0   \n",
       "6   abmusu  bert-base-multilingual-cased  viT5            IDDS     0     1   \n",
       "7   abmusu  bert-base-multilingual-cased  viT5            IDDS     0     2   \n",
       "8   abmusu  bert-base-multilingual-cased  viT5          random     0     0   \n",
       "9   abmusu  bert-base-multilingual-cased  viT5          random     0     1   \n",
       "10  abmusu  bert-base-multilingual-cased  viT5          random     0     2   \n",
       "\n",
       "    Num of Samples  Epoch Type score  rouge_avg_score  ...    bleu_3  \\\n",
       "0                8      1        Val        22.779075  ...  0.006269   \n",
       "1               13      1        Val        21.027725  ...  0.008356   \n",
       "2               17      1        Val        21.196200  ...  0.010158   \n",
       "3                8      1        Val        22.960800  ...  0.011296   \n",
       "4               13      1        Val        22.957000  ...  0.006676   \n",
       "5                8      1        Val        23.285150  ...  0.008155   \n",
       "6               13      1        Val        21.021650  ...  0.006163   \n",
       "7               17      1        Val        20.917550  ...  0.008347   \n",
       "8                8      1        Val        21.718375  ...  0.003377   \n",
       "9               13      1        Val        20.065750  ...  0.009709   \n",
       "10              17      1        Val        20.912925  ...  0.011178   \n",
       "\n",
       "      bleu_4  bleu_brevity_penalty  bleu_length_ratio  \\\n",
       "0   0.002331                   1.0           4.888514   \n",
       "1   0.003774                   1.0           5.429054   \n",
       "2   0.005097                   1.0           5.581081   \n",
       "3   0.005323                   1.0           4.923423   \n",
       "4   0.002234                   1.0           5.099099   \n",
       "5   0.002649                   1.0           4.733108   \n",
       "6   0.002394                   1.0           5.702703   \n",
       "7   0.003589                   1.0           5.704955   \n",
       "8   0.000904                   1.0           5.040541   \n",
       "9   0.004778                   1.0           6.185811   \n",
       "10  0.004807                   1.0           5.680180   \n",
       "\n",
       "    bleu_translation_length  bert_precision  bert_recall  bert_f1  bert_std  \\\n",
       "0                      4341         57.2434      65.9305  61.2496       0.0   \n",
       "1                      4821         58.9836      67.5065  62.9328       0.0   \n",
       "2                      4956         58.1596      67.4309  62.4315       0.0   \n",
       "3                      4372         58.8391      67.4589  62.8187       0.0   \n",
       "4                      4528         58.5039      67.1173  62.4848       0.0   \n",
       "5                      4203         58.2752      67.0827  62.3464       0.0   \n",
       "6                      5064         57.9655      67.4218  62.3087       0.0   \n",
       "7                      5066         59.1320      67.2237  62.8820       0.0   \n",
       "8                      4476         57.6373      66.1630  61.5894       0.0   \n",
       "9                      5493         58.7372      68.3151  63.1406       0.0   \n",
       "10                     5044         59.0050      67.8272  63.0949       0.0   \n",
       "\n",
       "         Time  \n",
       "0   95.527842  \n",
       "1   88.575020  \n",
       "2   83.347652  \n",
       "3   83.235364  \n",
       "4   82.989027  \n",
       "5   84.417964  \n",
       "6   88.572415  \n",
       "7   88.054230  \n",
       "8   82.067327  \n",
       "9   89.041931  \n",
       "10  84.315633  \n",
       "\n",
       "[11 rows x 28 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=g_j6ILT-X0k\n",
    "\n",
    "email_sender = 'minhtuanhth811@gmail.com'\n",
    "email_password = 'kdho gpnh qsbp gewf'\n",
    "email_receiver = 'minhtuanhth811@gmail.com'\n",
    "\n",
    "subject = 'Check Active Learning runtime'\n",
    "body = f\"\"\"\n",
    "Active Learning đã chạy xong, vui lòng vào check\n",
    "{state_config['name']}\n",
    "\"\"\"\n",
    "\n",
    "em = EmailMessage()\n",
    "em['From'] = email_sender\n",
    "em['To'] = email_receiver\n",
    "em['Subject'] = subject\n",
    "em.set_content(body)\n",
    "\n",
    "context = ssl.create_default_context()\n",
    "with smtplib.SMTP_SSL('smtp.gmail.com', 465, context=context) as smtp:\n",
    "    smtp.login(email_sender, email_password)\n",
    "    smtp.sendmail(email_sender, email_receiver, em.as_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-13T15:23:17.669Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def remove_folder_contents(folder):\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                remove_folder_contents(file_path)\n",
    "                os.rmdir(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "def add_embeddings_old(train_data):\n",
    "    remove_folder_contents('/kaggle/working')\n",
    "    for i in range(len(train_data)):\n",
    "        for j in range(len(train_data[\"single_documents\"][i])):\n",
    "            train_data[\"single_documents\"][i][j][\"embeddings\"] = get_embeddings(train_data[\"single_documents\"][i][j][\"raw_text\"]).detach().cpu().numpy()\n",
    "        train_data.to_csv(f\"train_embedding_{i}\")\n",
    "        if os.path.exists(f\"/kaggle/working/train_embedding_{i-1}\"):\n",
    "            os.remove(f\"/kaggle/working/train_embedding_{i-1}\")\n",
    "        print(f\"added embedding to cluster: {i} - category: {train_data['category'][i]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cần xử lý trùng title\n",
    "# xử lý thay title nếu trống anchor text\n",
    "# có đoạn check duplicate khá hay\n",
    "# train_df_detail_ex = pd.read_csv('abmusu_1200.csv')\n",
    "# for i in range(len(train_df_detail_ex)):\n",
    "#     if pd.isnull(train_df_detail_ex.loc[i,'anchor_text']):\n",
    "#         print(i)\n",
    "#         train_df_detail_ex.loc[i,'anchor_text'] = train_df_detail_ex.loc[i,'title']\n",
    "# c = train_df_detail_ex[train_df_detail_ex['title'].isin(train_df_detail_ex['title'][train_df_detail_ex['title'].duplicated()])].sort_values(\"title\").index.tolist()\n",
    "# i, ind, j, l = 0, 1, 1, len(c)\n",
    "# while j < l:\n",
    "#     if train_df_detail_ex.loc[c[i],'title'] == train_df_detail_ex.loc[c[j],'title']:\n",
    "#         train_df_detail_ex.loc[c[j],'title'] = train_df_detail_ex.loc[c[j],'title'] + f\" ({ind})\"\n",
    "#         ind += 1\n",
    "#     else:\n",
    "#         i = j\n",
    "#         ind = 1\n",
    "#     j += 1\n",
    "# train_df_detail_ex.to_csv('abmusu/abmusu_1200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-13T15:23:17.669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#os.remove('/kaggle/working/state_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-13T15:23:17.669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ls(drive, '1MLkRuZw53k862qGHr9JpPmHIFNd-G0cC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-13T15:23:17.669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# download_file(drive, '12FdkHcqVrdmu5Ae1W5ysPcq0OtulCEpX', '/kaggle/working/result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-13T15:23:17.669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# upload_file(drive, '/kaggle/working', 'result.csv', folder_id='1MLkRuZw53k862qGHr9JpPmHIFNd-G0cC', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3772748,
     "sourceId": 6582058,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4756229,
     "sourceId": 8062685,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7263398,
     "sourceId": 11584366,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7456260,
     "sourceId": 11865699,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "al",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
