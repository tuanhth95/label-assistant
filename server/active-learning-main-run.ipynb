{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": false,
    "execution": {
     "iopub.execute_input": "2025-06-13T15:16:39.918905Z",
     "iopub.status.busy": "2025-06-13T15:16:39.918347Z",
     "iopub.status.idle": "2025-06-13T15:18:51.806273Z",
     "shell.execute_reply": "2025-06-13T15:18:51.805306Z",
     "shell.execute_reply.started": "2025-06-13T15:16:39.918885Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== PWD ====================\n",
      "total 728\n",
      "drwxrwxr-x  8 npl npl   8192 Jul  6 17:51 .\n",
      "drwxrwxr-x 16 npl npl   8192 Jul  5 05:35 ..\n",
      "drwxrwxr-x  3 npl npl    152 Jul  4 06:32 .working\n",
      "drwxrwxr-x  2 npl npl   8192 Jul  4 06:32 __pycache__\n",
      "-rw-rw-r--  1 npl npl 222355 Jul  5 04:52 active-learning-main-run.ipynb\n",
      "-rw-rw-r--  1 npl npl 272758 Jul  6 12:35 app.log\n",
      "-rw-rw-r--  1 npl npl      0 Jul  6 05:10 app_critical.log\n",
      "drwxrwxr-x  2 npl npl    152 Jul  4 06:32 bertscore\n",
      "drwxrwxr-x  2 npl npl   8192 Jul  4 06:32 bleu\n",
      "-rw-rw-r--  1 npl npl   9909 Jul  4 06:32 drive_func.py\n",
      "-rw-rw-r--  1 npl npl  13861 Jul  4 06:32 mainrun.py\n",
      "-rw-rw-r--  1 npl npl   6197 Jul  6 05:09 requirements.txt\n",
      "-rw-rw-r--  1 npl npl  47720 Jul  6 12:26 result.csv\n",
      "drwxrwxr-x  2 npl npl    152 Jul  4 06:32 rouge\n",
      "-rw-rw-r--  1 npl npl   1139 Jul  4 06:32 run.slurm\n",
      "-rw-rw-r--  1 npl npl   6938 Jul  4 06:32 run_vit5_random.out\n",
      "-rw-rw-r--  1 npl npl   5798 Jul  4 06:32 select_algorithm.py\n",
      "-rw-rw-r--  1 npl npl   2391 Jul  4 08:01 service-account-key-2.json\n",
      "-rw-rw-r--  1 npl npl   2391 Jul  6 17:51 service-account-key-4.json\n",
      "-rw-rw-r--  1 npl npl   2391 Jul  6 17:51 service-account-key-5.json\n",
      "-rw-rw-r--  1 npl npl   2387 Jul  4 06:32 service-account-key.json\n",
      "-rw-rw-r--  1 npl npl   1048 Jul  4 06:32 setup_logger.py\n",
      "-rw-rw-r--  1 npl npl    916 Jul  6 12:26 state_config.json\n",
      "-rw-rw-r--  1 npl npl  15520 Jul  4 06:32 train_model.py\n",
      "-rw-rw-r--  1 npl npl   7895 Jul  4 06:32 utility.py\n",
      "drwxrwxr-x  6 npl npl    152 Jul  5 06:26 working\n",
      "\n",
      "==================== IMPORT AND DOWNLOAD ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/npl/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== GPU ====================\n",
      "Sun Jul  6 18:03:29 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          On  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   38C    P0             245W / 400W |  51409MiB / 81920MiB |     99%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-80GB          On  | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   39C    P0             221W / 400W |  15767MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-80GB          On  | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   22C    P0              60W / 400W |      3MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-80GB          On  | 00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   46C    P0             299W / 400W |  68430MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-80GB          On  | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   60C    P0             323W / 400W |  73100MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-80GB          On  | 00000000:90:00.0 Off |                    0 |\n",
      "| N/A   64C    P0             373W / 400W |  79700MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-80GB          On  | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              63W / 400W |      3MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-80GB          On  | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              66W / 400W |      5MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A    276560      C   python                                      984MiB |\n",
      "|    0   N/A  N/A   2648684      C   python                                     1370MiB |\n",
      "|    0   N/A  N/A   2654405      C   python                                     1370MiB |\n",
      "|    0   N/A  N/A   4025352      C   python                                    47658MiB |\n",
      "|    1   N/A  N/A    689182      C   python                                    10246MiB |\n",
      "|    1   N/A  N/A   2652996      C   python                                     1370MiB |\n",
      "|    1   N/A  N/A   3758784      C   python                                     1370MiB |\n",
      "|    1   N/A  N/A   3760240      C   python                                     1370MiB |\n",
      "|    1   N/A  N/A   3761006      C   python                                     1370MiB |\n",
      "|    3   N/A  N/A    200223      C   python                                    49534MiB |\n",
      "|    3   N/A  N/A   3941696      C   /data/inseclab/DVy/bin/ollama             18874MiB |\n",
      "|    4   N/A  N/A   1724268      C   python3                                    9132MiB |\n",
      "|    4   N/A  N/A   4099812      C   python                                    63942MiB |\n",
      "|    5   N/A  N/A    666384      C   ...veLearning/conda/envs/al/bin/python    79692MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "\n",
      "__Python VERSION: 3.9.23 | packaged by conda-forge | (main, Jun  4 2025, 17:57:12) \n",
      "[GCC 13.3.0]\n",
      "__pyTorch VERSION: 2.7.1+cu126\n",
      "__CUDA VERSION\n",
      "/bin/bash: line 1: nvcc: command not found\n",
      "__CUDNN VERSION: 90501\n",
      "__Number CUDA Devices: 8\n",
      "__Devices\n",
      "index, name, driver_version, memory.total [MiB], memory.used [MiB], memory.free [MiB]\n",
      "0, NVIDIA A100-SXM4-80GB, 535.161.08, 81920 MiB, 51409 MiB, 29641 MiB\n",
      "1, NVIDIA A100-SXM4-80GB, 535.161.08, 81920 MiB, 15767 MiB, 65283 MiB\n",
      "2, NVIDIA A100-SXM4-80GB, 535.161.08, 81920 MiB, 3 MiB, 81047 MiB\n",
      "3, NVIDIA A100-SXM4-80GB, 535.161.08, 81920 MiB, 68430 MiB, 12620 MiB\n",
      "4, NVIDIA A100-SXM4-80GB, 535.161.08, 81920 MiB, 73100 MiB, 7950 MiB\n",
      "5, NVIDIA A100-SXM4-80GB, 535.161.08, 81920 MiB, 79744 MiB, 1305 MiB\n",
      "6, NVIDIA A100-SXM4-80GB, 535.161.08, 81920 MiB, 3 MiB, 81047 MiB\n",
      "7, NVIDIA A100-SXM4-80GB, 535.161.08, 81920 MiB, 5 MiB, 81045 MiB\n",
      "Active CUDA Device: GPU 0\n",
      "Available devices  8\n",
      "Current cuda device  0\n",
      "\n",
      "==================== OS ====================\n",
      "Cluster Manager v10.0\n",
      "slave\n",
      "DGX_NAME=\"DGX Server\"\n",
      "DGX_PRETTY_NAME=\"NVIDIA DGX Server\"\n",
      "DGX_SWBUILD_DATE=\"2024-04-22-11-45-46\"\n",
      "DGX_SWBUILD_VERSION=\"6.2.1\"\n",
      "DGX_COMMIT_ID=\"41beb69\"\n",
      "DGX_PLATFORM=\"DGX Server for A100\"\n",
      "DGX_SERIAL_NUMBER=\"Not Specified\"\n",
      "DISTRIB_ID=Ubuntu\n",
      "DISTRIB_RELEASE=22.04\n",
      "DISTRIB_CODENAME=jammy\n",
      "DISTRIB_DESCRIPTION=\"Ubuntu 22.04.4 LTS\"\n",
      "PRETTY_NAME=\"Ubuntu 22.04.4 LTS\"\n",
      "NAME=\"Ubuntu\"\n",
      "VERSION_ID=\"22.04\"\n",
      "VERSION=\"22.04.4 LTS (Jammy Jellyfish)\"\n",
      "VERSION_CODENAME=jammy\n",
      "ID=ubuntu\n",
      "ID_LIKE=debian\n",
      "HOME_URL=\"https://www.ubuntu.com/\"\n",
      "SUPPORT_URL=\"https://help.ubuntu.com/\"\n",
      "BUG_REPORT_URL=\"https://bugs.launchpad.net/ubuntu/\"\n",
      "PRIVACY_POLICY_URL=\"https://www.ubuntu.com/legal/terms-and-policies/privacy-policy\"\n",
      "UBUNTU_CODENAME=jammy\n",
      "\n",
      "==================== PYTHON ====================\n",
      "Python 3.9.23\n",
      "Check output folder for requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# install and import package\n",
    "import subprocess\n",
    "def run_cmd(cmd):\n",
    "    return subprocess.getoutput(cmd)\n",
    "# !pip install -q transformers==4.50 sentencepiece \n",
    "# !pip install -q accelerate datasets==3.5 rouge_score bert-score nltk\n",
    "# !pip install -U -q sentence-transformers evaluate faiss-cpu\n",
    "# !pip install -q --upgrade google-api-python-client google-auth-httplib2 google-auth-oauthlib google-auth\n",
    "# !pip install -q --upgrade cffi\n",
    "#!pip install --upgrade ipywidgets\n",
    "\n",
    "print('\\n' + \"=\"*20 + \" PWD \" + \"=\"*20)\n",
    "!ls -la\n",
    "\n",
    "print('\\n' + \"=\"*20 + \" IMPORT AND DOWNLOAD \" + \"=\"*20)\n",
    "import faiss, logging, json, mimetypes, transformers, io\n",
    "from email.message import EmailMessage\n",
    "from datasets import load_dataset, Dataset\n",
    "from bert_score import score as bertscores\n",
    "from transformers import MBartForConditionalGeneration, BartForConditionalGeneration, AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import DataCollatorForSeq2Seq, get_scheduler, MBartForConditionalGeneration, Seq2SeqTrainer, T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import get_scheduler, DataCollatorWithPadding, TrainingArguments, Seq2SeqTrainingArguments\n",
    "from subprocess import call\n",
    "from IPython.display import display\n",
    "import shutil, os, sys, torch, os.path, ssl, smtplib, re, random, time, torch\n",
    "import pprint as pp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk, evaluate\n",
    "from evaluate import load\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaIoBaseDownload, MediaFileUpload\n",
    "from googleapiclient.errors import HttpError\n",
    "#nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "pd.options.mode.copy_on_write = False\n",
    "from matplotlib import pyplot as plt\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "print(\"=\"*20 + \" GPU \" + \"=\"*20)\n",
    "if 'not found' not in run_cmd('nvidia-smi'):\n",
    "    !nvidia-smi\n",
    "    print()\n",
    "    print('__Python VERSION:', sys.version)\n",
    "    print('__pyTorch VERSION:', torch.__version__)\n",
    "    print('__CUDA VERSION')\n",
    "    ! nvcc --version\n",
    "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "    print('__Devices')\n",
    "    call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
    "    print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
    "    print ('Available devices ', torch.cuda.device_count())\n",
    "    print ('Current cuda device ', torch.cuda.current_device())\n",
    "else:\n",
    "    print('NON-GPU VERSION')\n",
    "\n",
    "print('\\n' + \"=\"*20 + \" OS \" + \"=\"*20)\n",
    "!cat /etc/*release\n",
    "\n",
    "print('\\n' + \"=\"*20 + \" PYTHON \" + \"=\"*20)\n",
    "!python --version\n",
    "!pip freeze > requirements.txt \n",
    "print(\"Check output folder for requirements.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:51.814764Z",
     "iopub.status.busy": "2025-06-13T15:18:51.814468Z",
     "iopub.status.idle": "2025-06-13T15:18:51.832015Z",
     "shell.execute_reply": "2025-06-13T15:18:51.831436Z",
     "shell.execute_reply.started": "2025-06-13T15:18:51.814733Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# setup logging\n",
    "def show_only_critical(record):\n",
    "    return record.levelname == \"CRITICAL\"\n",
    "def initialize_logging(name): \n",
    "    logger = logging.getLogger(name)\n",
    "    console_handler = logging.StreamHandler()\n",
    "    file_handler = logging.FileHandler(\"app.log\", mode = \"a\", encoding=\"utf-16\")\n",
    "    file_critical_handler = logging.FileHandler(\"app_critical.log\", mode = \"a\", encoding=\"utf-16\")\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(file_critical_handler)\n",
    "    formatter = logging.Formatter(\n",
    "        \"[{asctime}.{msecs:0<3.0f} - {levelname}]  {message}\",\n",
    "        style = \"{\",\n",
    "        datefmt='%Y-%m-%d %H:%M:%S'\n",
    "    )\n",
    "    console_handler.setFormatter(formatter)\n",
    "    file_handler.setFormatter(formatter)\n",
    "    file_critical_handler.setFormatter(formatter)\n",
    "    logger.setLevel(\"DEBUG\")\n",
    "    console_handler.setLevel(\"DEBUG\")\n",
    "    file_handler.setLevel(\"INFO\")\n",
    "    file_critical_handler.addFilter(show_only_critical)\n",
    "    logger.debug(\"initialized\")\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:51.834026Z",
     "iopub.status.busy": "2025-06-13T15:18:51.833807Z",
     "iopub.status.idle": "2025-06-13T15:18:51.857400Z",
     "shell.execute_reply": "2025-06-13T15:18:51.856695Z",
     "shell.execute_reply.started": "2025-06-13T15:18:51.834011Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#get number of word in string\n",
    "def len_word(str):\n",
    "    return len(re.split('[-\\s_]', str))\n",
    "\n",
    "#get index of nth-occur char or -1\n",
    "def nth_occur_char(st, char=' ', nth=1, begin = 0):\n",
    "    n = 0\n",
    "    i = begin\n",
    "    while i < len(st):\n",
    "        i = st.find(' ', i,)\n",
    "        if i == -1:\n",
    "            return -1\n",
    "        else:\n",
    "            n += 1\n",
    "            if n == nth:\n",
    "                return i\n",
    "        i+=1\n",
    "    return -1\n",
    "\n",
    "# clean string list - remove too short length string item, remove hyphense,...\n",
    "def clean_list(lst):\n",
    "    l, i = len(lst), 0\n",
    "    while i < l:\n",
    "        if len(lst[i]) <= 2:\n",
    "            lst.remove(lst[i])\n",
    "            l -= 1\n",
    "        else:\n",
    "            lst[i] = clean_text(lst[i])\n",
    "            i += 1    \n",
    "    return lst\n",
    "\n",
    "#clean text\n",
    "def clean_text(text):\n",
    "    return re.sub(r'^\\n|^\\s|^-\\s', '', text)\n",
    "\n",
    "#preprocess data\n",
    "def preprocess_dataset(data):\n",
    "    data.set_format(type = \"pandas\")\n",
    "    df = data[:]\n",
    "    df_detail = pd.DataFrame(columns=[\"cluster_id\", \"title\", \"anchor_text\", \"raw_text\"])\n",
    "    df_cluster_info = pd.DataFrame(columns=[\"cluster_id\", \"summary\", \"category\"])\n",
    "    for i in range(len(df[\"single_documents\"])): \n",
    "        temp = pd.DataFrame.from_dict(df[\"single_documents\"][i].tolist(), orient='columns')\n",
    "        temp[\"cluster_id\"] = i\n",
    "        df_detail = pd.concat([df_detail, temp], axis=0, )\n",
    "        temp2 = df.iloc[[i]].copy()\n",
    "        temp2[\"cluster_id\"] = i\n",
    "        temp2 = temp2.drop([\"single_documents\"], axis=\"columns\")\n",
    "        df_cluster_info = pd.concat([df_cluster_info, temp2], axis=0, )\n",
    "    df_detail = df_detail.reset_index()\n",
    "    df_detail = df_detail.drop(\"index\", axis=\"columns\")\n",
    "    return df_detail, df_cluster_info\n",
    "\n",
    "def preprocess_sentence_dataset(document_data):\n",
    "    input_type = \"\"\n",
    "    if isinstance(document_data, Dataset):\n",
    "        input_type = \"dataset\"\n",
    "        document_data.set_format('pandas')\n",
    "        document_data = document_data[:]\n",
    "    else:\n",
    "        input_type = \"pandas\"\n",
    "    document_data = document_data.copy()\n",
    "    document_data.loc[:,'raw_text'] = document_data.loc[:,'raw_text'].str.split('\\.(?!\\d)|\\u2026|\\n|(?<!·∫¢nh)[:;] (?=[A-Z])|: (?=[\"‚Äú])')\n",
    "    document_data.loc[:,'raw_text'].apply(lambda x: clean_list(x))\n",
    "    document_data['raw_index'] = pd.Series( [[i for i in range(len(e))] for e in document_data['raw_text']],index = document_data.index)\n",
    "    document_data = document_data.explode(['raw_text','raw_index']).reset_index(drop=True)\n",
    "\n",
    "    if input_type == \"dataset\":\n",
    "        document_data = Dataset.from_pandas(document_data)\n",
    "    return document_data\n",
    "    \n",
    "def dataset_sentence_split(data, length_split):\n",
    "    input_type = \"\"\n",
    "    if isinstance(data, Dataset):\n",
    "        input_type = \"dataset\"\n",
    "        data.set_format('pandas')\n",
    "        data = data[:]\n",
    "    else:\n",
    "        input_type = \"pandas\"\n",
    "\n",
    "    data.loc[:,'raw_text'] = data.loc[:,'raw_text'].map(lambda x: x if nth_occur_char(x, ' ', length_split) == -1 else [x[:nth_occur_char(x, ' ', length_split)], x[nth_occur_char(x, ' ', length_split)+1:]])\n",
    "    data = data.explode(['raw_text']).reset_index(drop=True)\n",
    "    \n",
    "    if input_type == \"dataset\":\n",
    "        data = Dataset.from_pandas(data)\n",
    "    return data \n",
    "def get_embeddings(\n",
    "    data,\n",
    "    embed_col,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    encoder_seq_length,\n",
    "    device,\n",
    "    return_device,\n",
    "    graph_split = None, # truncate - None / sentence split to sentences\n",
    "    graph_merge = None, # mean/max/None pooling\n",
    "    sentence_split = None, # truncate - None / split to 2 sentence, default truncate\n",
    "    sentence_merge = None, # mean/max/None pooling MOI\n",
    "    sentence_vec = None, # cls - None/mean/max pooling MOI\n",
    "    return_type = 'datasets',\n",
    "    batch_size = 256\n",
    "):\n",
    "    \n",
    "    if isinstance(data, pd.DataFrame) == True:\n",
    "      data = Dataset.from_pandas(data)\n",
    "        \n",
    "    if \"embeddings\" in data.column_names:\n",
    "        if return_type == 'pandas':\n",
    "            data.set_format('pandas')\n",
    "            return data[:]\n",
    "        else: #if return_type == 'datasets':\n",
    "            return data\n",
    "            \n",
    "    if graph_split == \"sentence\":\n",
    "        data = preprocess_sentence_dataset(data)\n",
    "        if sentence_split == 'split':\n",
    "            data = dataset_sentence_split(data, 20)\n",
    "            \n",
    "    \n",
    "    encoded_input = data.map(lambda x: tokenizer(x[embed_col], truncation=True, padding=True, max_length=encoder_seq_length, return_tensors=\"pt\"), batched=True, remove_columns=data.column_names)\n",
    "    dataloader_or_data = DataLoader(\n",
    "        encoded_input,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=DataCollatorWithPadding(\n",
    "            tokenizer=tokenizer\n",
    "        ),\n",
    "        pin_memory=(str(device)),\n",
    "    )\n",
    "    if model.device != device:\n",
    "        model2 = model.to(device)\n",
    "        del model\n",
    "        torch.cuda.empty_cache()\n",
    "        model = model2\n",
    "    \n",
    "    num_obs = len(dataloader_or_data.dataset)\n",
    "    dim = list(model.state_dict().items())[-1][1].shape[0]\n",
    "    embeddings = torch.empty((num_obs, dim), dtype=torch.float, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        start = 0\n",
    "        for batch in tqdm(dataloader_or_data, desc=\"Embeddings created\"):\n",
    "            batch = {\n",
    "                k: v.to(device) for k, v in batch.items()\n",
    "            }\n",
    "            predictions = model(**batch)\n",
    "    \n",
    "            if sentence_vec == 'mean':\n",
    "                batch_embeddings = torch.mean(predictions.last_hidden_state[:,1:,:], 1)\n",
    "            elif sentence_vec == 'max':\n",
    "                batch_embeddings = torch.max(predictions.last_hidden_state[:,1:,:], 1).values\n",
    "            else: #cls\n",
    "                batch_embeddings = predictions.last_hidden_state[:,0,:]\n",
    "    \n",
    "            end = start + len(batch[\"input_ids\"])  # len(batch[list(batch.keys())[0]])\n",
    "            embeddings[start:end].copy_(batch_embeddings, non_blocking=True)\n",
    "            start = end\n",
    "            del predictions, batch\n",
    "        temp = embeddings.to(return_device)\n",
    "        del embeddings\n",
    "        torch.cuda.empty_cache()\n",
    "        embeddings = temp\n",
    "        data = data.add_column('embeddings',embeddings.tolist())\n",
    "        data.set_format('pandas')\n",
    "        data = data[:]\n",
    "        if graph_split != None:\n",
    "            if sentence_split == 'split': \n",
    "                if sentence_merge == \"mean\":\n",
    "                    data = data.groupby(['cluster_id', 'title', 'anchor_text', 'raw_index']).agg({\n",
    "                        'raw_text': lambda x : ' '.join(x),\n",
    "                        'embeddings': 'mean'\n",
    "                    }).reset_index()\n",
    "                elif sentence_merge == \"max\":\n",
    "                    data = data.groupby(['cluster_id', 'title', 'anchor_text', 'raw_index']).agg({\n",
    "                        'raw_text': lambda x : ' '.join(x),\n",
    "                        'embeddings': lambda x : np.max(list(x), axis=0)\n",
    "                    }).reset_index()\n",
    "            if graph_merge == \"mean\":\n",
    "                data = data.drop('raw_index', axis=1)\n",
    "                data = data.groupby(['cluster_id', 'title', 'anchor_text']).agg({\n",
    "                    'raw_text': lambda x : '. '.join(x),\n",
    "                    'embeddings': 'mean'\n",
    "                }).reset_index()\n",
    "            elif graph_merge == \"max\":\n",
    "                data = data.drop('raw_index', axis=1)\n",
    "                data = data.groupby(['cluster_id', 'title', 'anchor_text']).agg({\n",
    "                    'raw_text': lambda x : '. '.join(x),\n",
    "                    'embeddings': lambda x : np.max(list(x), axis=0)\n",
    "                }).reset_index()\n",
    "        if return_type == 'pandas':\n",
    "            return data\n",
    "        else: #if return_type == 'datasets':\n",
    "            return Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:51.858571Z",
     "iopub.status.busy": "2025-06-13T15:18:51.858344Z",
     "iopub.status.idle": "2025-06-13T15:18:51.878101Z",
     "shell.execute_reply": "2025-06-13T15:18:51.877485Z",
     "shell.execute_reply.started": "2025-06-13T15:18:51.858554Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# select algorithm\n",
    "def average_similarity(embed, data_embeddings):\n",
    "    if data_embeddings.size(0) == 0:  # Ki·ªÉm tra n·∫øu data_embeddings tr·ªëng\n",
    "        return 0\n",
    "    # Gi·∫£ ƒë·ªãnh s·ª≠ d·ª•ng cosine similarity tr√™n GPU\n",
    "    cos = torch.nn.functional.cosine_similarity(embed.unsqueeze(0), data_embeddings)\n",
    "    return cos.mean().item()  # Tr·∫£ v·ªÅ gi√° tr·ªã trung b√¨nh c·ªßa ƒë·ªô t∆∞∆°ng ƒë·ªìng\n",
    "def select_idds_samples(unlabel_train_data, labeled_train_data, lamb, k, device='cuda'):\n",
    "    \n",
    "    # Ki·ªÉm tra v√† in k√≠ch th∆∞·ªõc embedding c·ªßa t·ª´ng DataFrame tr∆∞·ªõc khi chuy·ªÉn ƒë·ªïi sang tensor\n",
    "    assert \"embeddings\" in unlabel_train_data.columns, \"Column 'embeddings' missing in unlabel_train_data\"\n",
    "    assert \"embeddings\" in labeled_train_data.columns, \"Column 'embeddings' missing in labeled_train_data\"\n",
    "    \n",
    "    # Chuy·ªÉn embeddings c·ªßa unlabel v√† labeled th√†nh tensor tr√™n GPU\n",
    "    unlabel_embeddings = torch.tensor(np.array(list(unlabel_train_data[\"embeddings\"].values), dtype='float32'), device=device)\n",
    "    labeled_embeddings = torch.tensor(np.array(list(labeled_train_data[\"embeddings\"].values), dtype='float32'), device=device)\n",
    "    \n",
    "    # T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng tr√™n GPU cho to√†n b·ªô unlabel_train_data\n",
    "    similarities_unlabel = lamb * torch.tensor(\n",
    "        [average_similarity(embed, unlabel_embeddings) for embed in unlabel_embeddings],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa unlabel_train_data v√† labeled_train_data tr√™n GPU\n",
    "    similarities_label = (1 - lamb) * torch.tensor(\n",
    "        [average_similarity(embed, labeled_embeddings) for embed in unlabel_embeddings],\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # T√≠nh ƒëi·ªÉm cho t·ª´ng h√†ng\n",
    "    scores = similarities_unlabel - similarities_label\n",
    "\n",
    "    # T√¨m ch·ªâ s·ªë c·ªßa ƒëi·ªÉm cao nh·∫•t\n",
    "    if len(scores) < k:\n",
    "        k = len(scores)\n",
    "    k_max_idx = torch.topk(scores, k=k).indices.tolist()\n",
    "\n",
    "    # Tr·∫£ v·ªÅ h√†ng c√≥ ƒëi·ªÉm cao nh·∫•t (chuy·ªÉn v·ªÅ CPU ƒë·ªÉ l·∫•y d·ªØ li·ªáu t·ª´ DataFrame)\n",
    "    return unlabel_train_data.iloc[k_max_idx].copy()\n",
    "    \n",
    "\n",
    "def select_random_samples(unlabel_train_data, k):\n",
    "    return unlabel_train_data.sample(n=k, random_state=random.randint(1, 100000), axis=0) \n",
    "    \n",
    "def data_select( \n",
    "        unlabel_train_data, budget = -1,\n",
    "        init = 100, k_init = 10, init_type = \"random\",\n",
    "        iter = 10,  k_iter = -1, iter_type = \"random\",\n",
    "        lamb = 0.67, device=\"cuda\"\n",
    "    ): \n",
    "    # k<= 0 boc het, 0<k <1, boc theo ti le, k>=1, boc theo so luong \n",
    "    # co the xet thaam so moi laan boc k\n",
    "    # k=1 moi laan chi bo√≥c mau: 1200 maau maat 160s\n",
    "    # k=iter moi laan bo√≥c iter: 1200 maau maat 4s\n",
    "    if \"iter\" in unlabel_train_data.columns:\n",
    "        return unlabel_train_data\n",
    "    data_len = len(unlabel_train_data)\n",
    "    if budget <= 0 or budget >= data_len:\n",
    "        budget = data_len\n",
    "    elif budget > 0 and budget < 1:\n",
    "        budget = round(budget*data_len)\n",
    "    elif budget >=1:\n",
    "        budget = round(budget)\n",
    "        \n",
    "    if init > budget:\n",
    "        init = budget\n",
    "    if iter > budget:\n",
    "        iter = budget\n",
    "        \n",
    "    if k_init <= 0 or k_init >= init:\n",
    "        k_init = init\n",
    "    elif k_init > 0 and k_init < 1:\n",
    "        k_init = round(k_init*init)\n",
    "    elif k_init >=1:\n",
    "        k_init = round(k_init)\n",
    "\n",
    "    if k_iter <= 0 or k_iter >= iter:\n",
    "        k_iter = iter\n",
    "    elif k_iter > 0 and k_iter < 1:\n",
    "        k_iter = round(k_iter*iter)\n",
    "    elif k_iter >=1:\n",
    "        k_iter = round(k_iter)\n",
    "        \n",
    "    labeled_train_data = pd.DataFrame(columns=[\"cluster_id\", \"title\", \"anchor_text\", \"raw_text\", \"embeddings\", \"iter\"])\n",
    "    if len(unlabel_train_data) < init: \n",
    "        assert \"Not enough data\" == False\n",
    "        return\n",
    "    \n",
    "    # T·∫°o m·ªôt mask ƒë·ªÉ ƒë√°nh d·∫•u c√°c h√†ng ƒë√£ ch·ªçn\n",
    "    mask = pd.Series([False] * len(unlabel_train_data))\n",
    "    progress_bar = tqdm(range(budget), desc=f\"{iter_type} Select\")\n",
    "\n",
    "    \n",
    "    if init_type == \"random\":\n",
    "        available_indices = unlabel_train_data[~mask].index\n",
    "        samples_index = np.random.choice(available_indices, size=init, replace=False)\n",
    "        sample = unlabel_train_data.loc[samples_index].copy()\n",
    "        mask[samples_index] = True\n",
    "        labeled_train_data = pd.concat([labeled_train_data, sample], ignore_index=True)\n",
    "    else:\n",
    "        l = init\n",
    "        while l > 0:\n",
    "            if l < k_init:\n",
    "                k_init = l\n",
    "            sample = select_idds_samples(unlabel_train_data[~mask], labeled_train_data, lamb, k=k_init, device=device)            \n",
    "            mask[sample.index.tolist()] = True\n",
    "            labeled_train_data = pd.concat([labeled_train_data, sample], ignore_index=True)\n",
    "            l = l - k_init\n",
    "    labeled_train_data[\"iter\"] = 0\n",
    "    progress_bar.update(init)\n",
    "    \n",
    "    # V√≤ng l·∫∑p ch√≠nh ƒë·ªÉ ch·ªçn m·∫´u ti·∫øp theo\n",
    "    th_iter = 0\n",
    "    while mask.sum() < budget:\n",
    "        if budget - mask.sum() >= iter:\n",
    "            i = iter\n",
    "        else:\n",
    "            i = budget - mask.sum()\n",
    "        k = k_iter\n",
    "        th_iter += 1\n",
    "        while i > 0:\n",
    "            if i < k:\n",
    "                k = i\n",
    "            if iter_type == \"IDDS\":\n",
    "                sample = select_idds_samples(unlabel_train_data[~mask], labeled_train_data, lamb, k=k, device=device)\n",
    "            elif iter_type == \"random\":\n",
    "                sample = select_random_samples(unlabel_train_data[~mask], k)\n",
    "            sample[\"iter\"] = th_iter\n",
    "            mask[sample.index.tolist()] = True\n",
    "            labeled_train_data = pd.concat([labeled_train_data, sample], ignore_index=True)\n",
    "            progress_bar.update(k)\n",
    "            i = i - k\n",
    "    return labeled_train_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:51.912265Z",
     "iopub.status.busy": "2025-06-13T15:18:51.911687Z",
     "iopub.status.idle": "2025-06-13T15:18:51.931745Z",
     "shell.execute_reply": "2025-06-13T15:18:51.931043Z",
     "shell.execute_reply.started": "2025-06-13T15:18:51.912247Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# drive function\n",
    "\n",
    "def authenticate_service_account(path, scope):\n",
    "    \"\"\"X√°c th·ª±c b·∫±ng service account v√† tr·∫£ v·ªÅ ƒë·ªëi t∆∞·ª£ng service.\"\"\"\n",
    "    try:\n",
    "        creds = service_account.Credentials.from_service_account_file(\n",
    "            path, scopes=scope)\n",
    "        service = build('drive', 'v3', credentials=creds)\n",
    "        print(\"X√°c th·ª±c b·∫±ng Service Account th√†nh c√¥ng!\")\n",
    "        return service\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói x√°c th·ª±c: {e}\")\n",
    "        return None\n",
    "\n",
    "def ls(drive, folder_id):\n",
    "    if not drive:\n",
    "        return 'drive is none'\n",
    "    try: \n",
    "        results = drive.files().list(\n",
    "            q=f'\"{folder_id}\" in parents',\n",
    "        ).execute()\n",
    "        file_list = results.get('files', [])\n",
    "        minimum = []\n",
    "        for file in file_list:\n",
    "            minimum.append(f\"{'+' if file['mimeType'].split('.')[-1] == 'folder' else '-'} {file['name']} ({file['id']}) {file['mimeType'].split('.')[-1]}\")\n",
    "        return minimum\n",
    "    except:\n",
    "        return 'error'\n",
    "\n",
    "def download_file(drive, file_id, save_as):\n",
    "    if not drive:\n",
    "        return 'drive is none'\n",
    "    try: \n",
    "        request = drive.files().get_media(fileId=file_id)\n",
    "    \n",
    "        with open(save_as, \"wb\") as f: #mo file o che do ghi bin\n",
    "            downloader = MediaIoBaseDownload(f, request)\n",
    "            done = False\n",
    "            while not done:\n",
    "                status, done = downloader.next_chunk()\n",
    "                if status:\n",
    "                    print(f\"Da tai xuong {int(status.progress()*100)}%\")\n",
    "        print(f\"Tai tep thanh cong, luu tai: {save_as}\")\n",
    "        return True\n",
    "    except HttpError as error:\n",
    "        print(f\"Loi API: {error}\")\n",
    "        if error.resp.status == 404:\n",
    "            print(\"L·ªói 404: t·ªáp kh√¥ng t·ªìn t·∫°i ho·∫∑c kh√¥ng c√≥ quy·ªÅn truy c·∫≠p\")\n",
    "        return False\n",
    "    except IOError as e:\n",
    "        print(f\"L·ªói ghi t·ªáp: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}\")\n",
    "        return False\n",
    "# WARNING:googleapiclient.http:Encountered 403 Forbidden with reason \"storageQuotaExceeded\"\n",
    "\n",
    "# L·ªói API: <HttpError 403 when requesting None returned \"The user's Drive storage quota has been exceeded.\". Details: \"[{'message': \"The user's Drive storage quota has been exceeded.\", 'domain': 'usageLimits', 'reason': 'storageQuotaExceeded'}]\">\n",
    "def upload_file(drive, path, file_name, folder_id=None, overwrite=True):\n",
    "    full_local_path = os.path.join(path, file_name)\n",
    "    if not os.path.isfile(full_local_path):\n",
    "        print(f\"L·ªói t·ªáp kh√¥ng t·ªìn t·∫°i\")\n",
    "        return None\n",
    "    ex = file_exists(drive, file_name, folder_id)\n",
    "    mime_type, _ = mimetypes.guess_type(full_local_path)\n",
    "    if mime_type is None:\n",
    "        mime_type = 'application/octet-stream'\n",
    "    metadata = {\n",
    "        \"name\": file_name,\n",
    "        \"parents\": [folder_id if folder_id else 'root']\n",
    "    }\n",
    "    media = MediaFileUpload(full_local_path, mimetype=mime_type, resumable=True)\n",
    "\n",
    "    try:\n",
    "        if ex == None or overwrite == False:\n",
    "            file = drive.files().create(\n",
    "                body = metadata,\n",
    "                media_body=media,\n",
    "                fields='id, name' # yeu cau tra ve id va name\n",
    "            ).execute()\n",
    "            print(f\"T·∫£i l√™n th√†nh c√¥ng, Id t·ªáp: '{file.get('id')}'\")\n",
    "            return file.get('id')\n",
    "        else:\n",
    "            file = drive.files().update(\n",
    "                fileId=ex['id'],\n",
    "                media_body=media,\n",
    "                fields='id, name'\n",
    "            ).execute()\n",
    "            print(f\"C·∫≠p nh·∫≠t th√†nh c√¥ng, Id t·ªáp: '{file.get('id')}'\")\n",
    "            return file.get('id')\n",
    "    except HttpError as e:\n",
    "        print(f\"L·ªói API: {e}\")\n",
    "        decoded_string = e.content.decode('utf-8')\n",
    "        reason = json.loads(decoded_string)['error']['errors'][0]['reason']\n",
    "        if reason == 'storageQuotaExceeded':\n",
    "            drive = authenticate_service_account('service-account-key-2.json', SCOPES)\n",
    "            try:\n",
    "                if ex == None or overwrite == False:\n",
    "                    file = drive.files().create(\n",
    "                        body = metadata,\n",
    "                        media_body=media,\n",
    "                        fields='id, name' # yeu cau tra ve id va name\n",
    "                    ).execute()\n",
    "                    print(f\"T·∫£i l√™n th√†nh c√¥ng, Id t·ªáp: '{file.get('id')}'\")\n",
    "                    return file.get('id')\n",
    "                else:\n",
    "                    file = drive.files().update(\n",
    "                        fileId=ex['id'],\n",
    "                        media_body=media,\n",
    "                        fields='id, name'\n",
    "                    ).execute()\n",
    "                    print(f\"C·∫≠p nh·∫≠t th√†nh c√¥ng, Id t·ªáp: '{file.get('id')}'\")\n",
    "                    return file.get('id')\n",
    "            except Exception as e:\n",
    "                print(f\"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}\")\n",
    "                return None\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_id_of_title(drive, title, parent_directory_id='root'):\n",
    "    try:\n",
    "        res = drive.files().list(\n",
    "            q = f\"name='{title}' and '{parent_directory_id}' in parents and trashed=false\",\n",
    "            spaces='drive',\n",
    "            fields='files(id,name)',\n",
    "            pageSize=1\n",
    "        ).execute()\n",
    "        res = res.get('files', [])\n",
    "        if not res:\n",
    "            return None\n",
    "        else:\n",
    "            return res[0]['id']\n",
    "    except HttpError as error:\n",
    "        print(f\"L·ªói API: {error}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}\")\n",
    "        return None\n",
    "\n",
    "def file_exists(drive, file_name, parent_id):\n",
    "    try:\n",
    "        res = drive.files().list(\n",
    "            q = f\"name='{file_name}' and '{parent_id}' in parents and trashed=false\",\n",
    "            spaces='drive',\n",
    "            fields='files(id,name,mimeType)',\n",
    "            pageSize=1\n",
    "        ).execute()\n",
    "        res = res.get('files', [])\n",
    "        if not res:\n",
    "            return None\n",
    "        else:\n",
    "            return res[0]\n",
    "    except HttpError as error:\n",
    "        print(f\"L·ªói API: {error}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_folder(drive, name, parent_id=None):\n",
    "    if not drive:\n",
    "        return None\n",
    "    if parent_id == None:\n",
    "        parent_id = 'root'\n",
    "    try:\n",
    "        res = drive.files().list(\n",
    "            q=f\"name='{name}' and '{parent_id}' in parents and trashed =false\",\n",
    "            fields='files(id)'\n",
    "        ).execute()\n",
    "        res = res.get('files', [])\n",
    "        if res:\n",
    "            print(\"Th∆∞ m·ª•c ƒë√£ t·ªìn t·∫°i\");\n",
    "            return None\n",
    "        else:\n",
    "            metadata = {\n",
    "                'name': name,\n",
    "                'mimeType': 'application/vnd.google-apps.folder',\n",
    "                'parents': [parent_id]\n",
    "            }\n",
    "            folder = drive.files().create(\n",
    "                body=metadata,\n",
    "                fields='id, name'\n",
    "            ).execute()\n",
    "            print(f'T·∫°o th∆∞ m·ª•c th√†nh c√¥ng: {folder.get(\"id\")}')\n",
    "            return folder.get('id')\n",
    "    except HttpError as error:\n",
    "        print(f\"L·ªói API: {error}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "def delete_file(drive, file_name, parent_id=None):\n",
    "    if not drive:\n",
    "        return None\n",
    "    try:\n",
    "        file = drive.files().list(\n",
    "            q = f\"name='{file_name}' and '{parent_id}' in parents and trashed=false\",\n",
    "            pageSize=1,\n",
    "        ).execute()\n",
    "        file = file.get('files',[])\n",
    "    except HttpError as error:\n",
    "        print(f\"L·ªói API khong get duoc id: {error}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        if file == []:\n",
    "            print(\"File kh√¥ng t·ªìn t·∫°i\")\n",
    "            return None\n",
    "        file = drive.files().delete(\n",
    "            fileId=file[0]['id'],\n",
    "            fields='id'\n",
    "        ).execute()\n",
    "        print(\"ƒê√£ x√≥a file\")\n",
    "        return True\n",
    "    except HttpError as error:\n",
    "        print(f\"L·ªói API khong xoa duoc file: {error}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói kh√¥ng x√°c ƒë·ªãnh: {e}\")\n",
    "        return None\n",
    "\n",
    "def download_folder(drive, folder_id, save_as):\n",
    "    \"\"\"\n",
    "    T·∫£i to√†n b·ªô m·ªôt th∆∞ m·ª•c v√† n·ªôi dung c·ªßa n√≥ t·ª´ Google Drive.\n",
    "\n",
    "    Args:\n",
    "        drive: ƒê·ªëi t∆∞·ª£ng service ƒë√£ ƒë∆∞·ª£c x√°c th·ª±c c·ªßa Google Drive API.\n",
    "        folder_id (str): ID c·ªßa th∆∞ m·ª•c tr√™n Drive c·∫ßn t·∫£i v·ªÅ.\n",
    "        save_as (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c tr√™n m√°y t√≠nh\n",
    "                                      n∆°i s·∫Ω ch·ª©a th∆∞ m·ª•c t·∫£i v·ªÅ.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # L·∫•y t√™n c·ªßa th∆∞ m·ª•c g·ªëc ƒë·ªÉ t·∫°o th∆∞ m·ª•c t∆∞∆°ng ·ª©ng ·ªü local\n",
    "        folder_info = drive.files().get(fileId=folder_id, fields='name').execute()\n",
    "        folder_name = folder_info.get('name')\n",
    "        \n",
    "        # T·∫°o ƒë∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß cho th∆∞ m·ª•c s·∫Ω ƒë∆∞·ª£c t·∫£i v·ªÅ\n",
    "        local_folder_path = os.path.join(save_as, folder_name)\n",
    "        if os.path.exists(local_folder_path):\n",
    "            shutil.rmtree(local_folder_path)\n",
    "        else:\n",
    "            print(f\"üìÅ T·∫°o th∆∞ m·ª•c c·ª•c b·ªô: '{local_folder_path}'\")\n",
    "        os.makedirs(local_folder_path, exist_ok=True)\n",
    "\n",
    "        # Li·ªát k√™ t·∫•t c·∫£ c√°c m·ª•c trong th∆∞ m·ª•c tr√™n Drive\n",
    "        query = f\"'{folder_id}' in parents and trashed = false\"\n",
    "        results = drive.files().list(q=query,\n",
    "                                       pageSize=1000, # L·∫•y t·ªëi ƒëa 1000 m·ª•c m·ªói l·∫ßn\n",
    "                                       fields=\"nextPageToken, files(id, name, mimeType)\").execute()\n",
    "        items = results.get('files', [])\n",
    "\n",
    "        if not items:\n",
    "            print(f\"   Th∆∞ m·ª•c '{folder_name}' r·ªóng.\")\n",
    "            return\n",
    "\n",
    "        # D√πng tqdm ƒë·ªÉ hi·ªÉn th·ªã thanh ti·∫øn tr√¨nh\n",
    "        with tqdm(total=len(items), desc=f'üì• T·∫£i xu·ªëng {folder_name}', unit='item') as pbar:\n",
    "            for item in items:\n",
    "                item_name = item.get('name')\n",
    "                item_id = item.get('id')\n",
    "                item_mime_type = item.get('mimeType')\n",
    "                \n",
    "                if item_mime_type == 'application/vnd.google-apps.folder':\n",
    "                    # N·∫øu l√† th∆∞ m·ª•c, g·ªçi ƒë·ªá quy ƒë·ªÉ t·∫£i th∆∞ m·ª•c con\n",
    "                    pbar.set_description(f\"üìÇ ƒê·ªá quy v√†o {item_name}\")\n",
    "                    download_folder(drive, item_id, local_folder_path)\n",
    "                else:\n",
    "                    # N·∫øu l√† t·ªáp, g·ªçi h√†m download_file\n",
    "                    pbar.set_description(f\"üìÑ T·∫£i t·ªáp {item_name}\")\n",
    "                    local_file_path = os.path.join(local_folder_path, item_name)\n",
    "                    download_file(drive, item_id, local_file_path)\n",
    "                \n",
    "                pbar.update(1)\n",
    "\n",
    "    except HttpError as error:\n",
    "        print(f\"L·ªói API khi x·ª≠ l√Ω th∆∞ m·ª•c ID '{folder_id}': {error}\")\n",
    "\n",
    "# def upload_folder(drive, local_path, parent_folder_id=None, overwrite=False):  \n",
    "# def delete_folder_recursive(drive, folder_identifier, parent_id=None, include_folder = True):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:51.879271Z",
     "iopub.status.busy": "2025-06-13T15:18:51.879005Z",
     "iopub.status.idle": "2025-06-13T15:18:51.910915Z",
     "shell.execute_reply": "2025-06-13T15:18:51.910339Z",
     "shell.execute_reply.started": "2025-06-13T15:18:51.879244Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#train model\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [[pre.strip() for pre in pred] for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # ROUGE k√¨ v·ªçng d√≤ng m·ªõi sau m·ªói c√¢u\n",
    "    preds = [[\"\\n\".join(nltk.sent_tokenize(pre)) for pre in pred] for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "    \n",
    "def train_method( **kwargs):\n",
    "    total_train_data = kwargs.get('total_train_data', None)\n",
    "    #print(len(total_train_data))\n",
    "    assert total_train_data is not None  \n",
    "    result_log = kwargs['df_log']\n",
    "    log_write = kwargs.get('log_write', None)\n",
    "    model_name = kwargs.get('model_name',None)\n",
    "    select_strategy = kwargs.get('select_strategy', None)\n",
    "    encoder = kwargs.get('encoder', None)\n",
    "    state_config = kwargs.get('state_config', {})\n",
    "    alpha = kwargs.get('alpha', 0.0005*100)\n",
    "    last_test_score = -1\n",
    "    test_score = 0\n",
    "    ind = state_config['iter']\n",
    "    #train model\n",
    "    bertscore = bertscores\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    while abs(test_score - last_test_score)*100 > alpha :\n",
    "        upload_file(drive, WORKING_DIR, 'app.log', folder_id=BACKUP_DIR_ID)\n",
    "        upload_file(drive, WORKING_DIR, 'app_critical.log', folder_id=BACKUP_DIR_ID)\n",
    "        last_test_score = test_score\n",
    "        if ind > max(total_train_data[\"iter\"].unique().tolist()):\n",
    "            state_config['iter'] = 0\n",
    "            with open('state_config.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(state_config, f, ensure_ascii=False, indent=4)\n",
    "            upload_file(drive, WORKING_DIR, 'state_config.json', folder_id=BACKUP_DIR_ID)\n",
    "            break\n",
    "        \n",
    "        print(f\"\\n\\n================ITER:{ind}=======================\")\n",
    "        log_write.info(f\"ITER:{ind}\")\n",
    "        state_config['iter'] = ind\n",
    "        with open('state_config.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(state_config, f, ensure_ascii=False, indent=4)\n",
    "        upload_file(drive, WORKING_DIR, 'state_config.json', folder_id=BACKUP_DIR_ID)\n",
    "        #print(\"check 3\")\n",
    "        #print(Dataset.from_pandas(total_train_data[ind]))\n",
    "        kwargs['df_log'] = result_log\n",
    "        result_log = train_model(\n",
    "            train_detail_ds = Dataset.from_pandas(total_train_data[total_train_data[\"iter\"] <= ind]),\n",
    "            iter = ind,\n",
    "            num_of_samples = len(total_train_data[total_train_data[\"iter\"] <= ind]),\n",
    "            bert_score = bertscore,\n",
    "            #meteor_score = meteor,\n",
    "            bleu_score = bleu,\n",
    "            **kwargs\n",
    "        )\n",
    "        result_log.to_csv(\"result.csv\",index=False)\n",
    "        upload_file(drive, WORKING_DIR, 'result.csv', folder_id=BACKUP_DIR_ID)\n",
    "        log_write.info(f\"ITER:{ind} Done\")\n",
    "        ind += 1\n",
    "        \n",
    "        test_score = result_log.loc[len(result_log)-1, \"rouge_avg_score\"]\n",
    "        print(f\"different: {test_score} : {last_test_score} : {test_score - last_test_score}\")\n",
    "    if abs(test_score - last_test_score)*100 <= alpha:\n",
    "        state_config['iter'] = 0\n",
    "        with open('state_config.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(state_config, f, ensure_ascii=False, indent=4)\n",
    "        upload_file(drive, WORKING_DIR, 'state_config.json', folder_id=BACKUP_DIR_ID)\n",
    "        upload_file(drive, WORKING_DIR, 'app.log', folder_id=BACKUP_DIR_ID)\n",
    "        upload_file(drive, WORKING_DIR, 'app_critical.log', folder_id=BACKUP_DIR_ID)\n",
    "    return result_log\n",
    "\n",
    "def train_model( **kwargs ):\n",
    "    train_detail_ds = kwargs.get('train_detail_ds', None)\n",
    "    val_detail = kwargs.get('val_detail', None)\n",
    "    test_detail = kwargs.get('test_detail', None)\n",
    "    model_path = kwargs.get('model_path', None)\n",
    "    model_name = kwargs.get('model_name',None)\n",
    "    output_dir = kwargs.get('output_dir', None)\n",
    "    batch_size = kwargs.get('batch_size', 8)\n",
    "    num_train_epochs = kwargs.get('num_train_epochs', 3)\n",
    "    limit_new_tokens = kwargs.get('limit_new_tokens', 512)\n",
    "    select_strategy = kwargs.get('select_strategy', None)\n",
    "    encoder = kwargs.get('encoder', None)\n",
    "    loop = kwargs.get('loop', None)\n",
    "    device = kwargs.get('device', torch.device(\"cpu\"))\n",
    "    df_log = kwargs.get('df_log', pd.DataFrame(columns=result_log_template_col))\n",
    "    iter = kwargs.get('iter', None)\n",
    "    num_of_samples = kwargs.get('num_of_samples', None)\n",
    "    log_write = kwargs.get('log_write', None)\n",
    "    bert_score = kwargs.get('bert_score',bertscores)\n",
    "    #meteor_score = kwargs.get('meteor_score', evaluate.load('meteor'))\n",
    "    bleu_score = kwargs.get('bleu_score', evaluate.load('bleu'))\n",
    "    num_var = kwargs.get('num_var', 1)\n",
    "    var_certainty = kwargs.get('var_certainty', 'bleu')\n",
    "    test_score = 0\n",
    "    last_test_score = test_score\n",
    "    train_detail_ds.reset_format()\n",
    "    val_detail.reset_format()\n",
    "    test_detail.reset_format()\n",
    "\n",
    "    if \"T5\" in model_name:\n",
    "        model_iter = T5ForConditionalGeneration.from_pretrained(model_path)\n",
    "        model_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        print('Use model T5')\n",
    "    elif re.search(\"bart\", model_name, re.I) != None:  \n",
    "        model_iter = MBartForConditionalGeneration.from_pretrained(model_path)\n",
    "        if re.search(\"mbart\", model_name, re.I) != None:\n",
    "            model_tokenizer = AutoTokenizer.from_pretrained(model_path, src_lang=\"vi_VN\", tgt_lang=\"vi_VN\")\n",
    "            print('Use model mbart')\n",
    "        else:\n",
    "            model_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "            print('Use model bartpho')\n",
    "    model_iter.to(device)\n",
    "    \n",
    "    tokenized_train_datasets = train_detail_ds.map(\n",
    "        lambda x:  model_tokenizer(x[\"raw_text\"], text_target=x[\"anchor_text\"] , max_length=max_input_length, truncation=True), \n",
    "        remove_columns=['cluster_id', 'title', 'anchor_text', 'raw_text', 'embeddings', 'iter']\n",
    "    )\n",
    "    tokenized_val_datasets = val_detail.map(\n",
    "        lambda x: model_tokenizer(x[\"raw_text\"], text_target=x[\"anchor_text\"] , max_length=max_input_length, truncation=True),\n",
    "        remove_columns=['cluster_id', 'title', 'anchor_text', 'raw_text'] \n",
    "    )\n",
    "    #tokenized_test_datasets = test_detail.map(lambda x: preprocess_function(x,model_tokenizer), remove_columns=['cluster_id', 'title', 'anchor_text', 'raw_text'] )\n",
    "    tokenized_train_datasets.set_format(\"torch\")\n",
    "    tokenized_val_datasets.set_format(\"torch\")\n",
    "    #tokenized_test_datasets.set_format(\"torch\") \n",
    "    \n",
    "    #model_iter = MBartForConditionalGeneration.from_pretrained(\"/kaggle/working/origin_backup/bartpho\")\n",
    "    data_collator = DataCollatorForSeq2Seq(model_tokenizer, model=model_iter)\n",
    "    print(f\"train_detail_ds : {len(train_detail_ds)}\")\n",
    "    train_dataloader = DataLoader(\n",
    "        tokenized_train_datasets, shuffle=True, collate_fn=data_collator, batch_size=batch_size,\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        tokenized_val_datasets, shuffle=True, collate_fn=data_collator, batch_size=batch_size\n",
    "    )\n",
    "    # test_dataloader = DataLoader(\n",
    "    #     tokenized_test_datasets, shuffle=True, collate_fn=data_collator, batch_size=batch_size,\n",
    "    # )\n",
    "    optimizer = AdamW(model_iter.parameters(), lr=2e-5)\n",
    "    accelerator = Accelerator(cpu = 'cpu' in device.type)\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model_iter, optimizer, train_dataloader, eval_dataloader\n",
    "    )\n",
    "\n",
    "\n",
    "    num_update_steps_per_epoch = len(train_dataloader)\n",
    "    num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        \"linear\",\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "    rouge_score = evaluate.load(\"rouge\")\n",
    "    log_list = []\n",
    "    log_dict = {\n",
    "        \"Dataset\" : \"abmusu\",\n",
    "        \"Encoder\" : encoder,\n",
    "        \"Model\" : model_name,\n",
    "        \"Select strategy\" : select_strategy,\n",
    "        \"Loop\": loop,\n",
    "        \"Iter\" : iter,\n",
    "        \"Num of Samples\": num_of_samples,\n",
    "        \"Epoch\" : 0,\n",
    "        \"Type score\" : \"\",\n",
    "        \"rouge_avg_score\" : 0.0, \n",
    "        \"rouge1\" : 0.0, \n",
    "        \"rouge2\" : 0.0, \n",
    "        \"rougeL\" : 0.0, \n",
    "        \"rougeLsum\" : 0.0, \n",
    "        \"bleu\" : 0.0, \n",
    "        \"bleu_std\" : 0.0, \n",
    "        \"bleu_1\" : 0.0, \n",
    "        \"bleu_2\" : 0.0,\n",
    "        \"bleu_3\" : 0.0, \n",
    "        \"bleu_4\" : 0.0,\n",
    "        \"bleu_brevity_penalty\" : 0.0, \n",
    "        \"bleu_length_ratio\" : 0.0, \n",
    "        \"bleu_translation_length\" : 0, \n",
    "        \"bert_precision\" : 0.0, \n",
    "        \"bert_recall\" : 0.0, \n",
    "        \"bert_f1\" : 0.0,\n",
    "        \"bert_std\" : 0.0,\n",
    "        \"Time\" : 0,\n",
    "    }\n",
    "    \n",
    "    progress_bar = tqdm(range(num_training_steps), desc=\"Train\")\n",
    "    model.train()\n",
    "    for epoch in range(num_train_epochs):\n",
    "        time_start = time.time()\n",
    "        # Hu·∫•n luy·ªán\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batchs = {\n",
    "                'input_ids' : batch['input_ids'],#.to(cudadevice), \n",
    "                'attention_mask' : batch['attention_mask'],#.to(cudadevice), \n",
    "                'labels' : batch['labels'],#.to(cudadevice)\n",
    "            }\n",
    "            outputs = model(**batchs)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        # ƒê√°nh gi√° epoch\n",
    "        log_write.info(f\"train epoch {epoch} done\")\n",
    "        #print(\"check1\")\n",
    "    model.eval()\n",
    "    predictions_list = []\n",
    "    certain_list = []\n",
    "    labels_list = []\n",
    "    bert_std_list = []\n",
    "    bleu_std_list = []\n",
    "    for module in model.modules():\n",
    "        if module.__class__.__name__.startswith('Dropout'):\n",
    "            module.train()\n",
    "    eval_progress_bar = tqdm(range(len(eval_dataloader)), desc=\"Eval\")\n",
    "    if num_var < 1:\n",
    "        num_var = 1\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            if num_var == 1:\n",
    "                generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                    batch[\"input_ids\"],#.to(cudadevice),\n",
    "                    attention_mask=batch[\"attention_mask\"],#.to(cudadevice),\n",
    "                    max_new_tokens = limit_new_tokens,\n",
    "                )\n",
    "            else:\n",
    "                num_var = int(num_var)\n",
    "                generated_tokens = accelerator.unwrap_model(model).generate(\n",
    "                    batch[\"input_ids\"],\n",
    "                    attention_mask=batch[\"attention_mask\"],\n",
    "                    max_new_tokens = limit_new_tokens,\n",
    "                    num_return_sequences=num_var,\n",
    "                    do_sample=True,\n",
    "                    top_k=50,\n",
    "                    top_p=0.95\n",
    "                )\n",
    "            generated_tokens = accelerator.pad_across_processes(\n",
    "                generated_tokens, dim=1, pad_index=model_tokenizer.pad_token_id\n",
    "            )\n",
    "            labels = batch[\"labels\"]\n",
    "            labels = accelerator.pad_across_processes(\n",
    "                batch[\"labels\"], dim=1, pad_index=model_tokenizer.pad_token_id\n",
    "            )\n",
    "            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
    "            generated_tokens = generated_tokens.reshape(len(batch[\"input_ids\"]), num_var, -1)\n",
    "            labels = accelerator.gather(labels).cpu().numpy()\n",
    "            labels = np.where(labels != -100, labels, model_tokenizer.pad_token_id)\n",
    "            if isinstance(generated_tokens, tuple):\n",
    "                generated_tokens = generated_tokens[0]\n",
    "            decoded_preds = [model_tokenizer.batch_decode(generated_token, skip_special_tokens=True) for generated_token in generated_tokens]\n",
    "            decoded_labels = model_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "            predictions_list.extend(decoded_preds)\n",
    "            labels_list.extend(decoded_labels)\n",
    "            eval_progress_bar.update(1)\n",
    "            del generated_tokens\n",
    "            del labels\n",
    "    if num_var > 1:\n",
    "        for l in range(len(predictions_list)):\n",
    "            f = 0\n",
    "            while f < len(predictions_list[l]):\n",
    "                if predictions_list[l][f] == '':\n",
    "                    #decoded_preds[l].pop(f)\n",
    "                    predictions_list[l][f] = 'a'\n",
    "                else:\n",
    "                    f += 1\n",
    "        for l in range(len(predictions_list)):\n",
    "            bleu_matrix = [[bleu_score.compute(predictions=[i], references=[j])['bleu'] for j in predictions_list[l]] for i in predictions_list[l]]\n",
    "            bleu_std_list.append(np.std([bleu_matrix[i][j] for i in range(0, len(predictions_list[l])-1) for j in range(i+1, len(predictions_list[l]))]))\n",
    "            if var_certainty == 'bleu':\n",
    "                certain_list.append(predictions_list[l][np.argmax(np.sum(bleu_matrix, axis = 1))])\n",
    "        for l in range(len(predictions_list)):\n",
    "            pred_for_bert = [predictions_list[l][i] for i in range(0, len(predictions_list[l])-1) for j in range(i+1, len(predictions_list[l]))]\n",
    "            ref_for_bert = [predictions_list[l][j] for i in range(0, len(predictions_list[l])-1) for j in range(i+1, len(predictions_list[l]))]\n",
    "            bert_f1 = bert_score(pred_for_bert, ref_for_bert, model_type=\"bert-base-multilingual-cased\")[2]\n",
    "            bert_matrix = [[0.0] * len(predictions_list[l])] * len(predictions_list[l])\n",
    "            for i in range(0, len(predictions_list[l])):\n",
    "                for j in range(0, len(predictions_list[l])):\n",
    "                    if i < j:\n",
    "                        bert_matrix[i][j] = bert_f1[i*len(predictions_list[l]) + j - 1 - (i+3)*i//2]\n",
    "                    elif i > j:\n",
    "                        bert_matrix[i][j] = bert_f1[j*len(predictions_list[l]) + i - 1 - (j+3)*j//2]\n",
    "                    else:\n",
    "                        bert_matrix[i][j] = torch.tensor(1.0)\n",
    "            bert_std_list.append(np.std(bert_f1.tolist()))\n",
    "            #last list to cal rouge, bleu, bert_f1\n",
    "            if var_certainty == 'bert':\n",
    "                certain_list.append(predictions_list[l][np.argmax(np.sum(bert_matrix, axis = 1))])\n",
    "            # certain_list.append(decoded_preds[l][0])\n",
    "    else:\n",
    "        for l in range(len(predictions_list)):\n",
    "            if predictions_list[l][0] == '':\n",
    "                predictions_list[l][0] = 'a'\n",
    "            bleu_std_list.append(1)\n",
    "            bert_std_list.append(1)\n",
    "            certain_list.append(predictions_list[l][0])\n",
    "    # T√≠nh to√°n c√°c ch·ªâ s·ªë\n",
    "\n",
    "    result_rouge_score = rouge_score.compute(predictions=certain_list, references=labels_list)\n",
    "    result_rouge_score = {key: round(value * 100,4) for key, value in result_rouge_score.items()}\n",
    "    #result_meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "    try:\n",
    "        result_bleu_score = bleu_score.compute(predictions=certain_list, references=labels_list)\n",
    "    except:\n",
    "        result_bleu_score = -1\n",
    "    bert_precision, bert_recall, bert_f1 = bert_score(certain_list, labels_list, model_type=\"working/origin_backup/bert-base-multilingual-cased\", num_layers=9,  lang=\"vi\", verbose=True)\n",
    "    bert_precision = round(float(bert_precision.mean())*100,4)\n",
    "    bert_recall = round(float(bert_recall.mean())*100,4)\n",
    "    bert_f1 = round(float(bert_f1.mean())*100,4)\n",
    "    bert_std = round(np.average(bert_std_list)*100,4)\n",
    "    bleu_std = np.average(bleu_std_list)\n",
    "    test_score = sum(result_rouge_score.values()) / 4\n",
    "    te = log_dict.copy()\n",
    "    te['Type score'] = 'Val'\n",
    "    te['Time'] = time.time() - time_start\n",
    "    te['rouge_avg_score'] = test_score\n",
    "    te['Epoch'] = epoch\n",
    "    te['rouge1'] = result_rouge_score['rouge1']\n",
    "    te['rouge2'] = result_rouge_score['rouge2']\n",
    "    te['rougeL'] = result_rouge_score['rougeL']\n",
    "    te['rougeLsum'] = result_rouge_score['rougeLsum']\n",
    "    # te['meteor'] = round(result_meteor_score['meteor'] * 100,4)\n",
    "    te['bleu'] = result_bleu_score['bleu']\n",
    "    te['bleu_std'] = bleu_std\n",
    "    te['bleu_1'] = result_bleu_score['precisions'][0]\n",
    "    te['bleu_2'] = result_bleu_score['precisions'][1]\n",
    "    te['bleu_3'] = result_bleu_score['precisions'][2]\n",
    "    te['bleu_4'] = result_bleu_score['precisions'][3]\n",
    "    te['bleu_brevity_penalty'] = result_bleu_score['brevity_penalty']\n",
    "    te['bleu_length_ratio'] = result_bleu_score['length_ratio']\n",
    "    te['bleu_translation_length'] = result_bleu_score['translation_length']\n",
    "    te['bert_precision'] = bert_precision\n",
    "    te['bert_recall'] = bert_recall\n",
    "    te['bert_f1'] = bert_f1\n",
    "    te['bert_std'] = bert_std\n",
    "    \n",
    "    log_list.append(te)\n",
    "\n",
    "    \n",
    "    # print(f\"Epoch {epoch}:\", result_rouge_score)\n",
    "    # print(f\"different: {test_score} : {last_test_score} : {test_score - last_test_score}\")\n",
    "    # last_test_score = test_score\n",
    "    log_write.info(f\"test done\")\n",
    "    # L∆∞u v√† t·∫£i\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        model_tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    #test iter\n",
    "    # print(f\"last_test_score: {last_test_score}\")\n",
    "    print(f\"test_score: {test_score}\")\n",
    "\n",
    "    new_log_df = pd.DataFrame(log_list)\n",
    "    del model_iter\n",
    "    del model_tokenizer\n",
    "    del train_dataloader\n",
    "    del eval_dataloader\n",
    "    # del test_dataloader\n",
    "    torch.cuda.empty_cache()\n",
    "    if len(df_log) == 0:\n",
    "        return new_log_df\n",
    "    else:\n",
    "        return pd.concat([df_log, new_log_df], axis=0,ignore_index=True)\n",
    "    #Run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:51.932550Z",
     "iopub.status.busy": "2025-06-13T15:18:51.932361Z",
     "iopub.status.idle": "2025-06-13T15:18:51.947710Z",
     "shell.execute_reply": "2025-06-13T15:18:51.946971Z",
     "shell.execute_reply.started": "2025-06-13T15:18:51.932536Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# GPU function\n",
    "def memory_stats():\n",
    "    print(torch.cuda.memory_allocated()/1024**2)\n",
    "    print(torch.cuda.memory_reserved()/1024**2)\n",
    "def memory_cache_clear():\n",
    "    torch.cuda.empty_cache()\n",
    "    memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:51.809347Z",
     "iopub.status.busy": "2025-06-13T15:18:51.808133Z",
     "iopub.status.idle": "2025-06-13T15:18:51.813643Z",
     "shell.execute_reply": "2025-06-13T15:18:51.812880Z",
     "shell.execute_reply.started": "2025-06-13T15:18:51.809324Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 1024\n",
    "\n",
    "MODE = \"RUN\"\n",
    "\n",
    "WORKING_DIR = '.'\n",
    "if MODE == \"TEST\":\n",
    "    BACKUP_DIR_ID = '1rhuFAQnmGBoVuARyhpcX8f_LDjcEHCGB'\n",
    "else:\n",
    "    BACKUP_DIR_ID = '15i-as_VUYJBRXXie01FgUZo9AqDiq-Cn'\n",
    "\n",
    "SERVICE_ACCOUNT_FILE = 'service-account-key-4.json' # ƒê∆∞·ªùng d·∫´n t·ªõi t·ªáp key c·ªßa service account\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive'] # Ph·∫°m vi truy c·∫≠p. V·∫´n c·∫ßn thi·∫øt ƒë·ªÉ ƒë·ªãnh nghƒ©a quy·ªÅn h·∫°n.\n",
    "\n",
    "CUDA_INDEX = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup template state config\n",
    "template_state_config = {}\n",
    "if MODE == \"TEST\":\n",
    "    #just for test\n",
    "    template_state_config['loop'] = 1 # measure stability\n",
    "    template_state_config['iter_init'] = 8\n",
    "    template_state_config['iter_num'] = 5\n",
    "    template_state_config['select_lamb'] = 0.67\n",
    "    template_state_config['num_train_epochs'] = 2\n",
    "    template_state_config['alpha'] = 0.005*100\n",
    "    template_state_config['limit_new_tokens'] =512\n",
    "    template_state_config['batch_size'] = 2 #bartpho dung 16, mbart chi duoc 2, viT5:4 ok v·ªõi 6iter, mT5:ko xai duoc\n",
    "    template_state_config['num_var'] = 2\n",
    "    template_state_config['var_certainty'] = 'bert' # ho·∫∑c 'bleu'\n",
    "  \n",
    "else:\n",
    "    #for real\n",
    "    template_state_config['loop'] = 3\n",
    "    template_state_config['iter_init'] = 100\n",
    "    template_state_config['iter_num'] = 50\n",
    "    template_state_config['select_lamb'] = 0.67\n",
    "    template_state_config['num_train_epochs'] = 10 # 10\n",
    "    template_state_config['alpha'] = 0.005*100\n",
    "    template_state_config['limit_new_tokens'] = 512\n",
    "    template_state_config['batch_size'] = 8\n",
    "    template_state_config['num_var'] = 1\n",
    "    template_state_config['var_certainty'] = 'bert' # ho·∫∑c 'bleu'\n",
    "    # vit5 24 t·∫ßm 68.33GB it6 -> 78.54GB+ it8(h·∫πo), \n",
    "    # vit5 20 t·∫ßm 61GB\n",
    "    # vit5 16 t·∫ßm 46GB -> 47.44 ·ªü iter 5, \n",
    "    # vit5 12 t·∫ßm 37.73GB(it8)->41.69(it13)\n",
    "    # vit5 8  t·∫ßm 27.8GB, \n",
    "    # vit5 4  t·∫ßm \n",
    "\n",
    "    # bartpho 24 t·∫ßm 21.15GB\n",
    "    # bartpho 32 t·∫ßm 31.18GB -> 36.23GB(it12)\n",
    "    # bartpho 48 t·∫ßm 50.15GB(i6L0) -> 52.36GB(i7L1)\n",
    "\n",
    "    # mT5 16 crash\n",
    "    # mT5 12 67.62GB(i0) -> 77.67GB(i8) -> crash(it13L1) ~ 83GB\n",
    "    # mT5 10 74.15GB(i13) -> 78GB(it16)\n",
    "\n",
    "    # mbart 36 71.99GB(i0L0) -> 76.30(i4L0) -> 78.31GB(i6L0) -> crash(it10) ~ 85GB\n",
    "    # mbart 32 74.86GB(i10L0) -> 78.15GB(i0L1) -> 83.15GB(i14L1)\n",
    "    # mbart 24 76.66GB(i14L1)\n",
    "template_state_config['name'] = 'train model mt5 budget=full loop=3 numvar=1 certainty=bert'\n",
    "template_state_config['description'] = 'train model v·ªõi quy tr√¨nh c·∫≠p nh·∫≠t c√≥ l·∫•y c·∫£ certainty'\n",
    "template_state_config['budget'] = 1200\n",
    "template_state_config['current_loop'] = 0\n",
    "template_state_config['iter'] = 0\n",
    "template_state_config['output_dir'] = WORKING_DIR\n",
    "template_state_config['encoders'] = ['phobert', 'bert-base-multilingual-cased']\n",
    "template_state_config['current_encoder'] = template_state_config['encoders'][0]\n",
    "template_state_config['models'] = ['mT5']\n",
    "template_state_config['current_models'] = template_state_config['models'][0]\n",
    "template_state_config['select_strategies'] = [\"IDDS\", \"random\"]\n",
    "template_state_config['current_select_strategies'] = template_state_config['select_strategies'][0]\n",
    "#config for embedding\n",
    "template_state_config['graph_split'] = None               # truncate - None / sentence split to sentences\n",
    "template_state_config['graph_merge'] = None              # mean/max/None pooling\n",
    "template_state_config['sentence_split'] = None            # truncate - None / split to 2 sentence, default truncate\n",
    "template_state_config['sentence_merge'] = None            # mean/max/None pooling\n",
    "template_state_config['sentence_vec'] = None             # cls - None/mean/max pooling\n",
    "\n",
    "\n",
    "result_log_template_col = [\"Dataset\", \"Encoder\", \"Model\", \"Select strategy\", \"Loop\", \"Iter\", \"Num of Samples\", \"Epoch\", \"Type score\", \n",
    "                            \"rouge_avg_score\", \"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\", \n",
    "                            \"bleu\", \"bleu_std\", \"bleu_1\", \"bleu_2\", \"bleu_3\", \"bleu_4\", \"bleu_brevity_penalty\", \"bleu_length_ratio\", \"bleu_translation_length\", \n",
    "                            \"bert_precision\", \"bert_recall\", \"bert_f1\", \"bert_std\", \"Time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:51.948814Z",
     "iopub.status.busy": "2025-06-13T15:18:51.948492Z",
     "iopub.status.idle": "2025-06-13T15:18:52.019692Z",
     "shell.execute_reply": "2025-06-13T15:18:52.019138Z",
     "shell.execute_reply.started": "2025-06-13T15:18:51.948792Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X√°c th·ª±c b·∫±ng Service Account th√†nh c√¥ng!\n"
     ]
    }
   ],
   "source": [
    "drive = authenticate_service_account(SERVICE_ACCOUNT_FILE, SCOPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:52.022318Z",
     "iopub.status.busy": "2025-06-13T15:18:52.021814Z",
     "iopub.status.idle": "2025-06-13T15:18:54.372780Z",
     "shell.execute_reply": "2025-06-13T15:18:54.371992Z",
     "shell.execute_reply.started": "2025-06-13T15:18:52.022303Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Da tai xuong 100%\n",
      "Tai tep thanh cong, luu tai: ./state_config.json\n",
      "Da tai xuong 100%\n",
      "Tai tep thanh cong, luu tai: ./result.csv\n",
      "Da tai xuong 100%\n",
      "Tai tep thanh cong, luu tai: ./working/data/data_backup_model-mT5_strategies-IDDS_encoder-bert-base-multilingual-cased_loop-1.jsonl\n",
      "Da tai xuong 100%\n",
      "Tai tep thanh cong, luu tai: ./app.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-07-06 18:03:47.995 - DEBUG]  initialized\n",
      "[2025-07-06 18:03:47.996 - INFO]  authenticate service account successful\n",
      "[2025-07-06 18:03:47.998 - INFO]  RUN mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Da tai xuong 0%\n",
      "Tai tep thanh cong, luu tai: ./app_critical.log\n"
     ]
    }
   ],
   "source": [
    "state_exist = file_exists(drive, 'state_config.json', BACKUP_DIR_ID)\n",
    "if state_exist != None:\n",
    "    download_file(drive, state_exist['id'], WORKING_DIR+'/state_config.json')\n",
    "    with open(WORKING_DIR+'/state_config.json', 'r') as f:\n",
    "        state_config = json.load(f)\n",
    "else: \n",
    "    state_config = template_state_config\n",
    "\n",
    "result_log_exist = file_exists(drive, 'result.csv', BACKUP_DIR_ID)\n",
    "if result_log_exist != None:\n",
    "    download_file(drive, result_log_exist['id'], WORKING_DIR+'/result.csv')\n",
    "    result_log = pd.read_csv(WORKING_DIR+'/result.csv')\n",
    "else:\n",
    "    result_log = pd.DataFrame(columns=result_log_template_col)\n",
    "\n",
    "data_backup_exist = file_exists(drive, f'data_backup_model-{state_config[\"current_models\"]}_strategies-{state_config[\"current_select_strategies\"]}_encoder-{state_config[\"current_encoder\"]}_loop-{state_config[\"current_loop\"]}.jsonl', BACKUP_DIR_ID)\n",
    "if data_backup_exist != None:\n",
    "    download_file(drive, data_backup_exist['id'], WORKING_DIR+f'/working/data/data_backup_model-{state_config[\"current_models\"]}_strategies-{state_config[\"current_select_strategies\"]}_encoder-{state_config[\"current_encoder\"]}_loop-{state_config[\"current_loop\"]}.jsonl')\n",
    "\n",
    "app_exist = file_exists(drive, 'app.log', BACKUP_DIR_ID)\n",
    "if app_exist != None:\n",
    "    download_file(drive, app_exist['id'], WORKING_DIR+'/app.log')\n",
    "\n",
    "app_critical_exist = file_exists(drive, 'app_critical.log', BACKUP_DIR_ID)\n",
    "if app_critical_exist != None:\n",
    "    download_file(drive, app_critical_exist['id'], WORKING_DIR+'/app_critical.log')\n",
    "    \n",
    "logger = initialize_logging(__name__)\n",
    "logger.info('authenticate service account successful')\n",
    "logger.info(f\"{MODE} mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:54.373711Z",
     "iopub.status.busy": "2025-06-13T15:18:54.373506Z",
     "iopub.status.idle": "2025-06-13T15:19:58.237040Z",
     "shell.execute_reply": "2025-06-13T15:19:58.236360Z",
     "shell.execute_reply.started": "2025-06-13T15:18:54.373695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#config device\n",
    "torch.cuda.set_device(CUDA_INDEX)\n",
    "cudadevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "cpudevice = torch.device(\"cpu\")\n",
    "\n",
    "# if not os.path.exists('./working/origin_backup/phobert'):\n",
    "#     download_folder(drive, '1YXkURCQp7aUUe0pskKzMufidofCnwV-Z', './working/origin_backup/phobert')\n",
    "# if not os.path.exists('./working/origin_backup/viT5'):\n",
    "#     download_folder(drive, '19o4wxqTTeP8owtV6Kthbxw69k20FPEWq', './working/origin_backup/viT5')\n",
    "#Load model encoder\n",
    "# checkpoint_encoder = \"google-bert/bert-base-multilingual-cased\"\n",
    "# checkpoint_encoder = 'keepitreal/vietnamese-sbert'\n",
    "# checkpoint_encoder = 'vinai/phobert-base-v2'\n",
    "# local_checkpoint_encoder = f'./working/origin_backup/{state_config[\"current_encoder\"]}'\n",
    "# tokenizer_encoder = AutoTokenizer.from_pretrained(local_checkpoint_encoder, local_files_only=True)\n",
    "# model_encoder = AutoModel.from_pretrained(local_checkpoint_encoder, local_files_only=True)\n",
    "# encoder_seq_length = 512\n",
    "# model_encoder.to(cudadevice)\n",
    "# logger.info(f\"{state_config['current_encoder']} cloned\")\n",
    "\n",
    "#Load model main\n",
    "# checkpoint_bartpho = \"vinai/bartpho-word-base\"\n",
    "# bartpho_tokenizer = AutoTokenizer.from_pretrained(checkpoint_bartpho)\n",
    "# bartpho_model = BartForConditionalGeneration.from_pretrained(checkpoint_bartpho)\n",
    "# bartpho_model.save_pretrained(\"/kaggle/working/origin_backup/bartpho\")\n",
    "# bartpho_tokenizer.save_pretrained(\"/kaggle/working/origin_backup/bartpho\")\n",
    "# #bartpho_model.to(cudadevice);\n",
    "# logger.info(\"bartpho-word-base cloned\")\n",
    "\n",
    "# checkpoint_mbart = \"facebook/mbart-large-50\" # 680M params\n",
    "# mbart_tokenizer = AutoTokenizer.from_pretrained(checkpoint_mbart)\n",
    "# mbart_model = MBartForConditionalGeneration.from_pretrained(checkpoint_mbart)\n",
    "# mbart_model.save_pretrained(\"./working/origin_backup/mbart\")\n",
    "# mbart_tokenizer.save_pretrained(\"./working/origin_backup/mbart\")\n",
    "# #mbart_model.to(cudadevice);\n",
    "# logger.info(\"mbart-large-50 cloned\")\n",
    "\n",
    "# checkpoint_viT5 = \"VietAI/vit5-base\" # 310M params and 866M params for large version\n",
    "# viT5_tokenizer = AutoTokenizer.from_pretrained(checkpoint_viT5, force_download=True)\n",
    "# viT5_model = T5ForConditionalGeneration.from_pretrained(checkpoint_viT5, force_download=True)\n",
    "# viT5_model.save_pretrained(\"./working/origin_backup/viT5\")\n",
    "# viT5_tokenizer.save_pretrained(\"./working/origin_backup/viT5\")\n",
    "# #mbart_model.to(cudadevice);\n",
    "# logger.info(\"viT5-base cloned\")\n",
    "\n",
    "\n",
    "#mbart_model.to(cudadevice);\n",
    "# logger.info(\"viT5-base cloned\")\n",
    "\n",
    "# checkpoint_mT5 = \"google/mt5-base\" # 580 million parameters\n",
    "# mT5_tokenizer = AutoTokenizer.from_pretrained(checkpoint_mT5)\n",
    "# mT5_model = T5ForConditionalGeneration.from_pretrained(checkpoint_mT5)\n",
    "# mT5_model.save_pretrained(\"./working/origin_backup/mT5\")\n",
    "# mT5_tokenizer.save_pretrained(\"./working/origin_backup/mT5\")\n",
    "# #mbart_model.to(cudadevice);\n",
    "# logger.info(\"mT5-base cloned\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:19:58.243117Z",
     "iopub.status.busy": "2025-06-13T15:19:58.242892Z",
     "iopub.status.idle": "2025-06-13T15:20:03.466390Z",
     "shell.execute_reply": "2025-06-13T15:20:03.464593Z",
     "shell.execute_reply.started": "2025-06-13T15:19:58.243096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#preprocess abmusu\n",
    "#limit data train to test\n",
    "if MODE == \"TEST\":\n",
    "    num_sample_train = 16\n",
    "    num_sample_val = 16\n",
    "    num_sample_test = 8\n",
    "else:\n",
    "    num_sample_train = -1\n",
    "    num_sample_val = -1\n",
    "    num_sample_test = -1\n",
    "\n",
    "\n",
    "#Load data abmusu\n",
    "# data_path = {\"train\":\"abmusu/vlsp_2022_abmusu_train_data_new.jsonl\",\n",
    "#              \"val\":\"abmusu/vlsp_2022_abmusu_validation_data_new.jsonl\",\n",
    "#              \"test\":\"abmusu/vlsp_abmusu_test_data.jsonl\"\n",
    "#             }\n",
    "# data = load_dataset(\"json\", data_files=data_path)\n",
    "# train_df_detail_ex = pd.read_csv('abmusu/abmusu_1200.csv')\n",
    "# display(data)\n",
    "\n",
    "# if MODE == \"TEST\":\n",
    "#     #just for test\n",
    "#     train_df_detail, train_df_cluster_info = preprocess_dataset(data[\"train\"].select(range(num_sample_train)))\n",
    "#     train_df_detail = train_df_detail[:num_sample_train]\n",
    "#     val_df_detail, val_df_cluster_info = preprocess_dataset(data[\"val\"].select(range(num_sample_val)))\n",
    "#     val_df_detail = val_df_detail[:num_sample_val]\n",
    "#     test_df_detail, test_df_cluster_info = preprocess_dataset(data[\"test\"].select(range(num_sample_val)))\n",
    "#     test_df_detail = test_df_detail[:num_sample_val]\n",
    "\n",
    "#     train_df_detail_ex = train_df_detail_ex[:num_sample_train]\n",
    "# else:\n",
    "#     #for real\n",
    "#     #split to paragraph\n",
    "#     train_df_detail, train_df_cluster_info = preprocess_dataset(data[\"train\"])\n",
    "#     val_df_detail, val_df_cluster_info = preprocess_dataset(data[\"val\"])\n",
    "#     test_df_detail, test_df_cluster_info = preprocess_dataset(data[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:20:03.794997Z",
     "iopub.status.busy": "2025-06-13T15:20:03.794795Z",
     "iopub.status.idle": "2025-06-13T15:20:20.404318Z",
     "shell.execute_reply": "2025-06-13T15:20:20.403541Z",
     "shell.execute_reply.started": "2025-06-13T15:20:03.794981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_detail = Dataset.from_pandas(train_df_detail) # train_df_detail_ex\n",
    "# val_detail = Dataset.from_pandas(val_df_detail)\n",
    "# test_detail = Dataset.from_pandas(test_df_detail)\n",
    "\n",
    "# logger.info('abmusu processed')\n",
    "# # tach theo cau, theo doan\n",
    "# # khongg xu ly(mac dinh truncate), truncate\n",
    "# # sau embedding max,mean pooling, cls theo cau, doan\n",
    "# train_detail = get_embeddings(\n",
    "#     train_detail,\n",
    "#     'raw_text',\n",
    "#     tokenizer_encoder,\n",
    "#     model_encoder,\n",
    "#     encoder_seq_length,\n",
    "#     device = cudadevice,\n",
    "#     return_device = cpudevice,\n",
    "#     graph_split = state_config['graph_split'],                  # truncate - None / sentence split to sentences\n",
    "#     graph_merge = state_config['graph_merge'],                  # mean/max/None pooling\n",
    "#     sentence_split = state_config['sentence_split'],            # truncate - None / split to 2 sentence, default truncate\n",
    "#     sentence_merge = state_config['sentence_merge'],            # mean/max/None pooling\n",
    "#     sentence_vec = state_config['sentence_vec'],                # cls - None/mean/max pooling\n",
    "#     return_type = 'datasets',\n",
    "#     batch_size = 256\n",
    "# )\n",
    "\n",
    "# train_detail.add_faiss_index(column=\"embeddings\")\n",
    "# t = model_encoder.to(cpudevice)\n",
    "# del model_encoder\n",
    "# model_encoder = t\n",
    "# torch.cuda.empty_cache()\n",
    "# train_detail.features\n",
    "# logger.info(\"abmusu embedded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-13T15:23:17.669Z",
     "iopub.execute_input": "2025-06-13T15:20:28.913361Z",
     "iopub.status.busy": "2025-06-13T15:20:28.913121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "encoders = state_config['encoders'][state_config['encoders'].index(state_config['current_encoder']):]\n",
    "for encoder in encoders:\n",
    "    models = state_config['models'][state_config['models'].index(state_config['current_models']):]\n",
    "    for j in models:\n",
    "        strategies = state_config['select_strategies'][state_config['select_strategies'].index(state_config['current_select_strategies']):]\n",
    "        for i in strategies:\n",
    "            current_loop = state_config['current_loop']\n",
    "            for l in range(current_loop, state_config['loop']):\n",
    "                logger.info(f\"Train {j} - {i} - {encoder} - with th-loop: {l}\")\n",
    "                state_config['current_loop'] = l\n",
    "                state_config['current_models'] = j\n",
    "                state_config['current_select_strategies'] = i\n",
    "                state_config['current_encoder'] = encoder\n",
    "                with open('state_config.json', 'w', encoding='utf-8') as f:\n",
    "                    json.dump(state_config, f, ensure_ascii=False, indent=4)\n",
    "                upload_file(drive, WORKING_DIR, 'state_config.json', folder_id= BACKUP_DIR_ID)\n",
    "                if os.path.exists(WORKING_DIR+f'/working/data/data_backup_model-{j}_strategies-{i}_encoder-{encoder}_loop-{l}.jsonl') == False or state_config['iter'] == 0:\n",
    "                    # process data\n",
    "                    local_checkpoint_encoder = f'./working/origin_backup/{state_config[\"current_encoder\"]}'\n",
    "                    tokenizer_encoder = AutoTokenizer.from_pretrained(local_checkpoint_encoder, local_files_only=True)\n",
    "                    model_encoder = AutoModel.from_pretrained(local_checkpoint_encoder, local_files_only=True)\n",
    "                    encoder_seq_length = int(model_encoder.config.max_position_embeddings / 256) * 256\n",
    "                    model_encoder.to(cudadevice)\n",
    "                    logger.info(f\"{state_config['current_encoder']} loaded\")\n",
    "                    if MODE == \"TEST\":\n",
    "                        train_df_detail = pd.read_csv('../abmusu/abmusu_1200.csv').loc[:num_sample_train,]\n",
    "                        val_df_detail = pd.read_csv('../abmusu/abmusu_val.csv').loc[:num_sample_val, ['cluster_id', 'title', 'anchor_text', 'raw_text']]\n",
    "                        test_df_detail = pd.read_csv('../abmusu/abmusu_test.csv').loc[:num_sample_test, ['cluster_id', 'title', 'anchor_text', 'raw_text']]\n",
    "                    else:\n",
    "                        train_df_detail = pd.read_csv('../abmusu/abmusu_1200.csv')\n",
    "                        val_df_detail = pd.read_csv('../abmusu/abmusu_val.csv').loc[:, ['cluster_id', 'title', 'anchor_text', 'raw_text']]\n",
    "                        test_df_detail = pd.read_csv('../abmusu/abmusu_test.csv').loc[:, ['cluster_id', 'title', 'anchor_text', 'raw_text']]\n",
    "\n",
    "                    train_detail = Dataset.from_pandas(train_df_detail) # train_df_detail_ex\n",
    "                    val_detail = Dataset.from_pandas(val_df_detail)\n",
    "                    test_detail = Dataset.from_pandas(test_df_detail)\n",
    "                    logger.info('abmusu processed')\n",
    "                    \n",
    "                    train_detail = get_embeddings(\n",
    "                        train_detail,\n",
    "                        'raw_text',\n",
    "                        tokenizer_encoder,\n",
    "                        model_encoder,\n",
    "                        encoder_seq_length,\n",
    "                        device = cudadevice,\n",
    "                        return_device = cpudevice,\n",
    "                        graph_split = state_config['graph_split'],                  # truncate - None / sentence split to sentences\n",
    "                        graph_merge = state_config['graph_merge'],                  # mean/max/None pooling\n",
    "                        sentence_split = state_config['sentence_split'],            # truncate - None / split to 2 sentence, default truncate\n",
    "                        sentence_merge = state_config['sentence_merge'],            # mean/max/None pooling\n",
    "                        sentence_vec = state_config['sentence_vec'],                # cls - None/mean/max pooling\n",
    "                        return_type = 'datasets',\n",
    "                        batch_size = 256\n",
    "                    )\n",
    "                    t = model_encoder.to(cpudevice)\n",
    "                    del model_encoder\n",
    "                    model_encoder = t\n",
    "                    torch.cuda.empty_cache()\n",
    "                    logger.info(\"abmusu embedded\")\n",
    "                    \n",
    "                    train_detail.set_format(\"pandas\")\n",
    "                    unlabel_train= train_detail[:]\n",
    "\n",
    "                    total_train_data = data_select(\n",
    "                        unlabel_train.copy(), budget = state_config['budget'],\n",
    "                        init = state_config['iter_init'], k_init = -1, init_type = f\"{i}\",\n",
    "                        iter = state_config['iter_num'],  k_iter = -1, iter_type = f\"{i}\",\n",
    "                        lamb = state_config['select_lamb'], device=cudadevice\n",
    "                    )\n",
    "                    data_backup = Dataset.from_pandas(total_train_data)\n",
    "                    data_backup.to_json(f'working/data/data_backup_model-{j}_strategies-{i}_encoder-{encoder}_loop-{l}.jsonl')\n",
    "                    upload_file(drive, WORKING_DIR + f'/working/data/', f'data_backup_model-{j}_strategies-{i}_encoder-{encoder}_loop-{l}.jsonl', folder_id=BACKUP_DIR_ID)\n",
    "                    logger.info(f\"backup data DONE\")\n",
    "                else:\n",
    "                    data = load_dataset(\"json\", data_files=WORKING_DIR+f'/working/data/data_backup_model-{j}_strategies-{i}_encoder-{encoder}_loop-{l}.jsonl')\n",
    "                    data = data['train']\n",
    "                    data.set_format(\"pandas\")\n",
    "                    total_train_data= data[:]\n",
    "                    if MODE == \"TEST\":\n",
    "                        val_df_detail = pd.read_csv('../abmusu/abmusu_val.csv').loc[:num_sample_val, ['cluster_id', 'title', 'anchor_text', 'raw_text']]\n",
    "                        test_df_detail = pd.read_csv('../abmusu/abmusu_test.csv').loc[:num_sample_test, ['cluster_id', 'title', 'anchor_text', 'raw_text']]\n",
    "                    else:\n",
    "                        val_df_detail = pd.read_csv('../abmusu/abmusu_val.csv').loc[:, ['cluster_id', 'title', 'anchor_text', 'raw_text']]\n",
    "                        test_df_detail = pd.read_csv('../abmusu/abmusu_test.csv').loc[:, ['cluster_id', 'title', 'anchor_text', 'raw_text']]\n",
    "\n",
    "                    val_detail = Dataset.from_pandas(val_df_detail)\n",
    "                    test_detail = Dataset.from_pandas(test_df_detail)\n",
    "                result_log = train_method(\n",
    "                    total_train_data = total_train_data,\n",
    "                    val_detail = val_detail,\n",
    "                    test_detail = test_detail,\n",
    "                    encoder = encoder,\n",
    "                    alpha = state_config['alpha'],\n",
    "                    model_path = f'{WORKING_DIR}/working/origin_backup/{j}',\n",
    "                    output_dir = f'{WORKING_DIR}/working/{j}/{i}',\n",
    "                    batch_size = state_config['batch_size'],\n",
    "                    model_name = j,\n",
    "                    num_train_epochs = state_config['num_train_epochs'],\n",
    "                    limit_new_tokens = state_config['limit_new_tokens'],\n",
    "                    select_strategy = i,\n",
    "                    loop = l,\n",
    "                    state_config = state_config,\n",
    "                    df_log = result_log,\n",
    "                    log_write = logger,\n",
    "                    num_var = state_config['num_var'],\n",
    "                    var_certainty = state_config['var_certainty'],\n",
    "                    device = cudadevice\n",
    "                )\n",
    "                result_log.to_csv(\"result.csv\",index=False)\n",
    "                upload_file(drive, WORKING_DIR, 'result.csv', folder_id=BACKUP_DIR_ID)\n",
    "                model_id = create_folder(drive, f'backup-model-{j}_strategies-{i}_encoder-{encoder}_loop-{l}', parent_id=BACKUP_DIR_ID)\n",
    "                if model_id:\n",
    "                    for f in os.listdir(f'{WORKING_DIR}/working/{j}/{i}'):\n",
    "                        upload_file(drive, f'{WORKING_DIR}/working/{j}/{i}', f, folder_id=model_id)\n",
    "                else:\n",
    "                    print('Kh√¥ng th·ªÉ t·∫°o folder backup model')\n",
    "                    logger.info(f\"Kh√¥ng th·ªÉ t·∫°o backup-model-{j}_strategies-{i}_encoder-{encoder}_loop-{l}\")\n",
    "            state_config['current_loop'] = 0\n",
    "        state_config['current_select_strategies'] = state_config['select_strategies'][0]\n",
    "    state_config['current_models'] = state_config['models'][0]\n",
    "result_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Encoder</th>\n",
       "      <th>Model</th>\n",
       "      <th>Select strategy</th>\n",
       "      <th>Loop</th>\n",
       "      <th>Iter</th>\n",
       "      <th>Num of Samples</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Type score</th>\n",
       "      <th>rouge_avg_score</th>\n",
       "      <th>...</th>\n",
       "      <th>bleu_3</th>\n",
       "      <th>bleu_4</th>\n",
       "      <th>bleu_brevity_penalty</th>\n",
       "      <th>bleu_length_ratio</th>\n",
       "      <th>bleu_translation_length</th>\n",
       "      <th>bert_precision</th>\n",
       "      <th>bert_recall</th>\n",
       "      <th>bert_f1</th>\n",
       "      <th>bert_std</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>phobert</td>\n",
       "      <td>viT5</td>\n",
       "      <td>IDDS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>22.779075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006269</td>\n",
       "      <td>0.002331</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.888514</td>\n",
       "      <td>4341</td>\n",
       "      <td>57.2434</td>\n",
       "      <td>65.9305</td>\n",
       "      <td>61.2496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.527842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>phobert</td>\n",
       "      <td>viT5</td>\n",
       "      <td>IDDS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>21.027725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008356</td>\n",
       "      <td>0.003774</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.429054</td>\n",
       "      <td>4821</td>\n",
       "      <td>58.9836</td>\n",
       "      <td>67.5065</td>\n",
       "      <td>62.9328</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.575020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>phobert</td>\n",
       "      <td>viT5</td>\n",
       "      <td>IDDS</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>21.196200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010158</td>\n",
       "      <td>0.005097</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.581081</td>\n",
       "      <td>4956</td>\n",
       "      <td>58.1596</td>\n",
       "      <td>67.4309</td>\n",
       "      <td>62.4315</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.347652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>phobert</td>\n",
       "      <td>viT5</td>\n",
       "      <td>random</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>22.960800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011296</td>\n",
       "      <td>0.005323</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.923423</td>\n",
       "      <td>4372</td>\n",
       "      <td>58.8391</td>\n",
       "      <td>67.4589</td>\n",
       "      <td>62.8187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.235364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>phobert</td>\n",
       "      <td>viT5</td>\n",
       "      <td>random</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>22.957000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006676</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.099099</td>\n",
       "      <td>4528</td>\n",
       "      <td>58.5039</td>\n",
       "      <td>67.1173</td>\n",
       "      <td>62.4848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.989027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "      <td>viT5</td>\n",
       "      <td>IDDS</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>23.285150</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008155</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.733108</td>\n",
       "      <td>4203</td>\n",
       "      <td>58.2752</td>\n",
       "      <td>67.0827</td>\n",
       "      <td>62.3464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.417964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "      <td>viT5</td>\n",
       "      <td>IDDS</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>21.021650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006163</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.702703</td>\n",
       "      <td>5064</td>\n",
       "      <td>57.9655</td>\n",
       "      <td>67.4218</td>\n",
       "      <td>62.3087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.572415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "      <td>viT5</td>\n",
       "      <td>IDDS</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>20.917550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008347</td>\n",
       "      <td>0.003589</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.704955</td>\n",
       "      <td>5066</td>\n",
       "      <td>59.1320</td>\n",
       "      <td>67.2237</td>\n",
       "      <td>62.8820</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.054230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "      <td>viT5</td>\n",
       "      <td>random</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>21.718375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003377</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.040541</td>\n",
       "      <td>4476</td>\n",
       "      <td>57.6373</td>\n",
       "      <td>66.1630</td>\n",
       "      <td>61.5894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.067327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "      <td>viT5</td>\n",
       "      <td>random</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>20.065750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009709</td>\n",
       "      <td>0.004778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.185811</td>\n",
       "      <td>5493</td>\n",
       "      <td>58.7372</td>\n",
       "      <td>68.3151</td>\n",
       "      <td>63.1406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.041931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>abmusu</td>\n",
       "      <td>bert-base-multilingual-cased</td>\n",
       "      <td>viT5</td>\n",
       "      <td>random</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>Val</td>\n",
       "      <td>20.912925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011178</td>\n",
       "      <td>0.004807</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.680180</td>\n",
       "      <td>5044</td>\n",
       "      <td>59.0050</td>\n",
       "      <td>67.8272</td>\n",
       "      <td>63.0949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.315633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows √ó 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dataset                       Encoder Model Select strategy  Loop  Iter  \\\n",
       "0   abmusu                       phobert  viT5            IDDS     0     0   \n",
       "1   abmusu                       phobert  viT5            IDDS     0     1   \n",
       "2   abmusu                       phobert  viT5            IDDS     0     2   \n",
       "3   abmusu                       phobert  viT5          random     0     0   \n",
       "4   abmusu                       phobert  viT5          random     0     1   \n",
       "5   abmusu  bert-base-multilingual-cased  viT5            IDDS     0     0   \n",
       "6   abmusu  bert-base-multilingual-cased  viT5            IDDS     0     1   \n",
       "7   abmusu  bert-base-multilingual-cased  viT5            IDDS     0     2   \n",
       "8   abmusu  bert-base-multilingual-cased  viT5          random     0     0   \n",
       "9   abmusu  bert-base-multilingual-cased  viT5          random     0     1   \n",
       "10  abmusu  bert-base-multilingual-cased  viT5          random     0     2   \n",
       "\n",
       "    Num of Samples  Epoch Type score  rouge_avg_score  ...    bleu_3  \\\n",
       "0                8      1        Val        22.779075  ...  0.006269   \n",
       "1               13      1        Val        21.027725  ...  0.008356   \n",
       "2               17      1        Val        21.196200  ...  0.010158   \n",
       "3                8      1        Val        22.960800  ...  0.011296   \n",
       "4               13      1        Val        22.957000  ...  0.006676   \n",
       "5                8      1        Val        23.285150  ...  0.008155   \n",
       "6               13      1        Val        21.021650  ...  0.006163   \n",
       "7               17      1        Val        20.917550  ...  0.008347   \n",
       "8                8      1        Val        21.718375  ...  0.003377   \n",
       "9               13      1        Val        20.065750  ...  0.009709   \n",
       "10              17      1        Val        20.912925  ...  0.011178   \n",
       "\n",
       "      bleu_4  bleu_brevity_penalty  bleu_length_ratio  \\\n",
       "0   0.002331                   1.0           4.888514   \n",
       "1   0.003774                   1.0           5.429054   \n",
       "2   0.005097                   1.0           5.581081   \n",
       "3   0.005323                   1.0           4.923423   \n",
       "4   0.002234                   1.0           5.099099   \n",
       "5   0.002649                   1.0           4.733108   \n",
       "6   0.002394                   1.0           5.702703   \n",
       "7   0.003589                   1.0           5.704955   \n",
       "8   0.000904                   1.0           5.040541   \n",
       "9   0.004778                   1.0           6.185811   \n",
       "10  0.004807                   1.0           5.680180   \n",
       "\n",
       "    bleu_translation_length  bert_precision  bert_recall  bert_f1  bert_std  \\\n",
       "0                      4341         57.2434      65.9305  61.2496       0.0   \n",
       "1                      4821         58.9836      67.5065  62.9328       0.0   \n",
       "2                      4956         58.1596      67.4309  62.4315       0.0   \n",
       "3                      4372         58.8391      67.4589  62.8187       0.0   \n",
       "4                      4528         58.5039      67.1173  62.4848       0.0   \n",
       "5                      4203         58.2752      67.0827  62.3464       0.0   \n",
       "6                      5064         57.9655      67.4218  62.3087       0.0   \n",
       "7                      5066         59.1320      67.2237  62.8820       0.0   \n",
       "8                      4476         57.6373      66.1630  61.5894       0.0   \n",
       "9                      5493         58.7372      68.3151  63.1406       0.0   \n",
       "10                     5044         59.0050      67.8272  63.0949       0.0   \n",
       "\n",
       "         Time  \n",
       "0   95.527842  \n",
       "1   88.575020  \n",
       "2   83.347652  \n",
       "3   83.235364  \n",
       "4   82.989027  \n",
       "5   84.417964  \n",
       "6   88.572415  \n",
       "7   88.054230  \n",
       "8   82.067327  \n",
       "9   89.041931  \n",
       "10  84.315633  \n",
       "\n",
       "[11 rows x 28 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=g_j6ILT-X0k\n",
    "\n",
    "email_sender = 'minhtuanhth811@gmail.com'\n",
    "email_password = 'kdho gpnh qsbp gewf'\n",
    "email_receiver = 'minhtuanhth811@gmail.com'\n",
    "\n",
    "subject = 'Check Active Learning runtime'\n",
    "body = f\"\"\"\n",
    "Active Learning ƒë√£ ch·∫°y xong, vui l√≤ng v√†o check\n",
    "{state_config['name']}\n",
    "\"\"\"\n",
    "\n",
    "em = EmailMessage()\n",
    "em['From'] = email_sender\n",
    "em['To'] = email_receiver\n",
    "em['Subject'] = subject\n",
    "em.set_content(body)\n",
    "\n",
    "context = ssl.create_default_context()\n",
    "with smtplib.SMTP_SSL('smtp.gmail.com', 465, context=context) as smtp:\n",
    "    smtp.login(email_sender, email_password)\n",
    "    smtp.sendmail(email_sender, email_receiver, em.as_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-13T15:23:17.669Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def remove_folder_contents(folder):\n",
    "    for the_file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, the_file)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                remove_folder_contents(file_path)\n",
    "                os.rmdir(file_path)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "def add_embeddings_old(train_data):\n",
    "    remove_folder_contents('/kaggle/working')\n",
    "    for i in range(len(train_data)):\n",
    "        for j in range(len(train_data[\"single_documents\"][i])):\n",
    "            train_data[\"single_documents\"][i][j][\"embeddings\"] = get_embeddings(train_data[\"single_documents\"][i][j][\"raw_text\"]).detach().cpu().numpy()\n",
    "        train_data.to_csv(f\"train_embedding_{i}\")\n",
    "        if os.path.exists(f\"/kaggle/working/train_embedding_{i-1}\"):\n",
    "            os.remove(f\"/kaggle/working/train_embedding_{i-1}\")\n",
    "        print(f\"added embedding to cluster: {i} - category: {train_data['category'][i]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# c·∫ßn x·ª≠ l√Ω tr√πng title\n",
    "# x·ª≠ l√Ω thay title n·∫øu tr·ªëng anchor text\n",
    "# c√≥ ƒëo·∫°n check duplicate kh√° hay\n",
    "# train_df_detail_ex = pd.read_csv('abmusu_1200.csv')\n",
    "# for i in range(len(train_df_detail_ex)):\n",
    "#     if pd.isnull(train_df_detail_ex.loc[i,'anchor_text']):\n",
    "#         print(i)\n",
    "#         train_df_detail_ex.loc[i,'anchor_text'] = train_df_detail_ex.loc[i,'title']\n",
    "# c = train_df_detail_ex[train_df_detail_ex['title'].isin(train_df_detail_ex['title'][train_df_detail_ex['title'].duplicated()])].sort_values(\"title\").index.tolist()\n",
    "# i, ind, j, l = 0, 1, 1, len(c)\n",
    "# while j < l:\n",
    "#     if train_df_detail_ex.loc[c[i],'title'] == train_df_detail_ex.loc[c[j],'title']:\n",
    "#         train_df_detail_ex.loc[c[j],'title'] = train_df_detail_ex.loc[c[j],'title'] + f\" ({ind})\"\n",
    "#         ind += 1\n",
    "#     else:\n",
    "#         i = j\n",
    "#         ind = 1\n",
    "#     j += 1\n",
    "# train_df_detail_ex.to_csv('abmusu/abmusu_1200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-13T15:23:17.669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#os.remove('/kaggle/working/state_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-13T15:23:17.669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ls(drive, '1MLkRuZw53k862qGHr9JpPmHIFNd-G0cC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-13T15:23:17.669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# download_file(drive, '12FdkHcqVrdmu5Ae1W5ysPcq0OtulCEpX', '/kaggle/working/result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-06-13T15:23:17.669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# upload_file(drive, '/kaggle/working', 'result.csv', folder_id='1MLkRuZw53k862qGHr9JpPmHIFNd-G0cC', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3772748,
     "sourceId": 6582058,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4756229,
     "sourceId": 8062685,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7263398,
     "sourceId": 11584366,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7456260,
     "sourceId": 11865699,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "al",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
